=== FILE: stage1_classifier.py (first 200 lines) ===
from __future__ import annotations

"""
Stage 1 classifier.

Loads prompt templates, formats inputs, calls generate(), parses JSON output,
and validates predictions with Pydantic.

Implements safe_parse_stage1_response and surfaces all schema/parse issues as
contains_painpoint="u" plus error flags. The classify_post function is the
primary entry point used by the Stage 1 benchmark runner.
"""

import json
import re
from pathlib import Path

from pydantic import ValidationError

from wgu_reddit_analyzer.benchmark.stage1_types import (
    Stage1PredictionInput,
    Stage1PredictionOutput,
    LlmCallResult,
)
from wgu_reddit_analyzer.benchmark.model_client import generate
from wgu_reddit_analyzer.utils.logging_utils import get_logger

logger = get_logger("benchmark.stage1_classifier")


def load_prompt_template(path: str | Path) -> str:
    """Load a prompt template from disk."""
    p = Path(path)
    return p.read_text(encoding="utf-8")


def build_prompt(template: str, example: Stage1PredictionInput) -> str:
    """
    Render a prompt template for a single post.

    Uses simple replacement so JSON braces in the template are not
    treated as format fields.
    """
    return (
        template.replace("{post_id}", example.post_id)
        .replace("{course_code}", example.course_code)
        .replace("{post_text}", example.text)
    )


def _strip_code_fences(text: str) -> str:
    s = text.strip()
    if s.startswith("```"):
        lines = s.splitlines()
        lines = lines[1:]
        if lines and lines[-1].strip().startswith("```"):
            lines = lines[:-1]
        s = "\n".join(lines).strip()
    return s


def _extract_json_block(text: str) -> str:
    """
    Best-effort extraction of the main JSON object from the model output.

    Strips code fences and trims to the outermost {...} block. If no
    braces are found, returns the original text.
    """
    s = _strip_code_fences(text)

    start = s.find("{")
    end = s.rfind("}")
    if start != -1 and end != -1 and end > start:
        return s[start : end + 1]
    return s


def _regex_contains_painpoint(text: str) -> tuple[str | None, bool]:
    """
    Try to extract an unambiguous y/n/u from a contains_painpoint field.

    Returns (label, ambiguous_flag).
    """
    pattern = r'"contains_painpoint"\s*:\s*"([ynu])"'
    matches = re.findall(pattern, text, flags=re.IGNORECASE)

    if not matches:
        return None, False

    distinct = {m.lower() for m in matches}
    if len(distinct) == 1:
        return distinct.pop(), False

    return None, True


def safe_parse_stage1_response(
    raw_text: str,
) -> tuple[str, str, str, float, bool, bool, bool]:
    """
    Safe parsing for Stage 1 responses.

    Returns:
        contains_painpoint (str: y/n/u)
        root_cause_summary (str)
        pain_point_snippet (str)
        confidence (float)
        parse_error (bool)
        schema_error (bool)
        used_fallback (bool)
    """
    parse_error = False
    schema_error = False
    used_fallback = False

    json_text = _extract_json_block(raw_text)

    # Strict JSON parse path
    try:
        data = json.loads(json_text)

        cp_raw = str(data.get("contains_painpoint", "")).strip().lower()
        if cp_raw not in {"y", "n", "u"}:
            schema_error = True
            cp = "u"
        else:
            cp = cp_raw

        root_cause = ""
        snippet = ""

        pain_points = data.get("pain_points")
        if isinstance(pain_points, list) and pain_points:
            first = pain_points[0] or {}
            root_cause = (first.get("root_cause_summary") or "").strip()
            snippet = (first.get("pain_point_snippet") or "").strip()
        else:
            root_cause = (data.get("root_cause_summary") or "").strip()
            snippet = (data.get("pain_point_snippet") or "").strip()

        conf_val = data.get("confidence", None)
        try:
            confidence = float(conf_val)
        except (TypeError, ValueError):
            confidence = 0.0

        return cp, root_cause, snippet, confidence, parse_error, schema_error, used_fallback

    except json.JSONDecodeError:
        parse_error = True
        schema_error = True
    except Exception:
        schema_error = True

    # Fallback: regex on contains_painpoint field only
    used_fallback = True
    label, ambiguous = _regex_contains_painpoint(raw_text)
    if ambiguous:
        schema_error = True
        return "u", "", "", 0.0, parse_error, schema_error, used_fallback

    if label in {"y", "n", "u"}:
        schema_error = True
        return label, "", "", 0.0, parse_error, schema_error, used_fallback

    schema_error = True
    return "u", "", "", 0.0, parse_error, schema_error, used_fallback


def _clamp_confidence(value: float) -> float:
    """
    Clamp confidence into [0.0, 1.0]. If it's NaN or out-of-range, return 0.0.
    """
    try:
        v = float(value)
    except (TypeError, ValueError):
        return 0.0

    if v < 0.0 or v > 1.0:
        return 0.0
    return v


def classify_post(
    model_name: str,
    example: Stage1PredictionInput,
    prompt_template: str,
    debug: bool = False,
) -> tuple[Stage1PredictionOutput, LlmCallResult]:
    """
    Classify a single post using the Stage 1 schema.

    Parameters
    ----------
    model_name : str
        Name of the model to use.
    example : Stage1PredictionInput
        Input post fields.
    prompt_template : str
        Template string containing {post_id}, {course_code}, {post_text}.

=== FILE: stage1_types.py (first 200 lines) ===
from __future__ import annotations
"""
Typed data structures used by Stage 1 benchmarking.

Defines the input format sent to the LLM, the normalized prediction returned
from the LLM, and the metadata collected for each model call. These types
provide a stable interface for all Stage 1 code.
"""

from typing import Literal
from pydantic import BaseModel


class Stage1PredictionInput(BaseModel):
    """Single post input to the Stage 1 classifier."""
    post_id: str
    course_code: str
    text: str  # usually "title\n\nselftext"


class Stage1PredictionOutput(BaseModel):
    """Normalized Stage 1 prediction and parse flags."""
    post_id: str
    course_code: str

    # Core decision
    contains_painpoint: Literal["y", "n", "u"]

    # Only meaningful when contains_painpoint == "y"
    root_cause_summary: str = ""
    pain_point_snippet: str = ""

    # Confidence in [0.0, 1.0]
    confidence: float = 0.0

    # Raw model output
    raw_response: str

    # Error / parsing flags
    parse_error: bool = False
    schema_error: bool = False
    used_fallback: bool = False


class LlmCallResult(BaseModel):
    """
    Metadata for a single LLM call.

    Captures low-level details needed for cost, latency, and failure
    analysis. Everything inside here should be provider-agnostic.
    """
    model_name: str
    provider: str
    raw_text: str
    input_tokens: int
    output_tokens: int
    total_cost_usd: float
    elapsed_sec: float

    # Failure / retry metadata
    llm_failure: bool = False
    num_retries: int = 0
    error_message: str | None = None
    timeout_sec: float | None = None

    # Timing metadata
    started_at: float | None = None
    finished_at: float | None = None
=== FILE: preprocess_painpoints.py (first 200 lines) ===
#!/usr/bin/env python3
"""
Prepare a token-conscious painpoint table from full-corpus Stage-1 predictions.

Adds:
    - Sorting by number of posts per course (descending)
    - Stable tie-break on course_code, then post_id

Input:
    /Users/buddy/Desktop/WGU-Reddit/artifacts/stage1/full_corpus/.../predictions_FULL.csv

Output (example):
    /Users/buddy/Desktop/WGU-Reddit/artifacts/stage2/painpoints_full_for_clustering.csv

Keeps only:
    - pred_contains_painpoint == "y"
    - no parse/schema/fallback/llm failures
    - confidence_pred >= MIN_CONFIDENCE

Output columns:
    - post_id
    - course_code
    - root_cause_summary
    - pain_point_snippet
"""

import csv
from collections import defaultdict
from pathlib import Path

# --- CONFIG ---------------------------------------------------------

DEFAULT_INPUT = Path(
    "/Users/buddy/Desktop/WGU-Reddit/artifacts/stage1/full_corpus/"
    "gpt-5-mini_s1_optimal_fullcorpus_20251126_023336/"
    "predictions_FULL.csv"
)

DEFAULT_OUTPUT = Path(
    "/Users/buddy/Desktop/WGU-Reddit/artifacts/stage2/painpoints_llm_friendly.csv"
)

MIN_CONFIDENCE = 0.50


# --- SCRIPT ---------------------------------------------------------

def prepare_painpoints(
    input_csv: Path = DEFAULT_INPUT,
    output_csv: Path = DEFAULT_OUTPUT,
    min_conf: float = MIN_CONFIDENCE,
) -> None:

    output_csv.parent.mkdir(parents=True, exist_ok=True)

    painpoints = []
    total = 0

    with input_csv.open("r", newline="", encoding="utf-8") as f_in:
        reader = csv.DictReader(f_in)

        for row in reader:
            total += 1

            if row.get("pred_contains_painpoint") != "y":
                continue

            if any(
                row.get(flag, "False") == "True"
                for flag in ("parse_error", "schema_error", "used_fallback", "llm_failure")
            ):
                continue

            try:
                conf = float(row.get("confidence_pred", "0") or 0.0)
            except ValueError:
                conf = 0.0

            if conf < min_conf:
                continue

            post_id = row["post_id"]
            course_code = row["course_code"]
            root_cause = (row.get("root_cause_summary_pred") or "").strip()
            snippet = (row.get("pain_point_snippet_pred") or "").strip()

            painpoints.append(
                {
                    "post_id": post_id,
                    "course_code": course_code,
                    "root_cause_summary": root_cause,
                    "pain_point_snippet": snippet,
                }
            )

    # --- SORT: by number of posts per course (desc), then course_code, then post_id --

    course_post_ids = defaultdict(set)
    for p in painpoints:
        course_post_ids[p["course_code"]].add(p["post_id"])

    painpoints.sort(
        key=lambda r: (
            -len(course_post_ids[r["course_code"]]),  # more posts first
            r["course_code"],
            r["post_id"],
        )
    )

    # --- WRITE OUTPUT ----------------------------------------------
    with output_csv.open("w", newline="", encoding="utf-8") as f_out:
        fieldnames = [
            "post_id",
            "course_code",
            "root_cause_summary",
            "pain_point_snippet",
        ]
        writer = csv.DictWriter(f_out, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(painpoints)

    print(f"Done. Kept {len(painpoints)} painpoints out of {total} posts.")
    print(f"Written to: {output_csv}")


if __name__ == "__main__":
    prepare_painpoints()
=== FILE: build_analytics.py (first 200 lines) ===
"""
Build the report_data layer for WGU Reddit Analyzer.

This script merges Stage 0, Stage 2, and Stage 3 artifacts plus course metadata
into a small set of clean tables under artifacts/report_data/, which power
human-facing reports and any future site/GUI.

Outputs (all unfiltered, full data):

- artifacts/report_data/post_master.csv
- artifacts/report_data/course_summary.csv
- artifacts/report_data/course_cluster_detail.jsonl
- artifacts/report_data/global_issues.csv
- artifacts/report_data/issue_course_matrix.csv
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import List, Dict, Any

import pandas as pd


# ---------------------------------------------------------------------------
# Path helpers
# ---------------------------------------------------------------------------

def project_root() -> Path:
    """Return the repo root (one level above src/)."""
    return Path(__file__).resolve().parents[3]


def ensure_dir(path: Path) -> None:
    """Create directory if it does not already exist."""
    path.mkdir(parents=True, exist_ok=True)


# ---------------------------------------------------------------------------
# Loaders
# ---------------------------------------------------------------------------

def load_stage0_filtered(artifacts_dir: Path) -> pd.DataFrame:
    """Load the locked Stage 0 filtered posts."""
    path = artifacts_dir / "stage0_filtered_posts.jsonl"
    if not path.exists():
        raise FileNotFoundError(f"Missing Stage 0 file: {path}")
    df = pd.read_json(path, lines=True)
    if "post_id" not in df.columns:
        raise ValueError("stage0_filtered_posts.jsonl missing 'post_id'")
    return df


def load_painpoints_stage2(artifacts_dir: Path) -> pd.DataFrame:
    """Load Stage 2 pain point classifier outputs."""
    path = artifacts_dir / "stage2" / "painpoints_llm_friendly.csv"
    if not path.exists():
        raise FileNotFoundError(f"Missing Stage 2 painpoint file: {path}")
    df = pd.read_csv(path)
    required_cols = {"post_id", "course_code", "root_cause_summary", "pain_point_snippet"}
    missing = required_cols - set(df.columns)
    if missing:
        raise ValueError(f"Stage 2 painpoint file missing columns: {missing}")
    return df


def load_stage3_preprocessed(preprocessed_dir: Path) -> pd.DataFrame:
    """Load Stage 2/3 preprocessed clusters (clusters_llm.csv)."""
    path = preprocessed_dir / "clusters_llm.csv"
    if not path.exists():
        raise FileNotFoundError(f"Missing clusters_llm.csv in {preprocessed_dir}")
    df = pd.read_csv(path)
    required_cols = {"cluster_id", "issue_summary", "course_code", "course_title", "num_posts"}
    missing = required_cols - set(df.columns)
    if missing:
        raise ValueError(f"clusters_llm.csv missing columns: {missing}")
    return df


def load_stage3_runs(run_dir: Path) -> Dict[str, Any]:
    """
    Load Stage 3 global clustering outputs:
    - cluster_global_index.csv
    - post_global_index.csv
    - global_clusters.json
    """
    cluster_global_path = run_dir / "cluster_global_index.csv"
    post_global_path = run_dir / "post_global_index.csv"
    global_clusters_path = run_dir / "global_clusters.json"

    for p in [cluster_global_path, post_global_path, global_clusters_path]:
        if not p.exists():
            raise FileNotFoundError(f"Missing Stage 3 file: {p}")

    df_cluster_global = pd.read_csv(cluster_global_path)
    df_post_global = pd.read_csv(post_global_path)
    with global_clusters_path.open("r", encoding="utf-8") as f:
        global_clusters = json.load(f)

    return {
        "cluster_global": df_cluster_global,
        "post_global": df_post_global,
        "global_clusters_raw": global_clusters,
    }


def load_course_metadata(data_dir: Path) -> pd.DataFrame:
    """Load course metadata (code, title, college)."""
    path = data_dir / "course_list_with_college.csv"
    if not path.exists():
        raise FileNotFoundError(f"Missing course metadata file: {path}")
    df = pd.read_csv(path)
    required_cols = {"CourseCode", "Title", "Colleges"}
    missing = required_cols - set(df.columns)
    if missing:
        raise ValueError(f"course_list_with_college.csv missing columns: {missing}")
    return df


# ---------------------------------------------------------------------------
# Builders
# ---------------------------------------------------------------------------

def build_post_master(
    df_stage0: pd.DataFrame,
    df_pp: pd.DataFrame,
    df_post_global: pd.DataFrame,
    df_cluster_global: pd.DataFrame,
    df_courses: pd.DataFrame,
) -> pd.DataFrame:
    """
    Build the joined postÃ—cluster table.

    One row per (post_id, cluster_id) when a post is assigned to multiple clusters.
    This is the raw intermediate used for course/global aggregations.

    A separate collapse step will derive the true post-level post_master.csv.
    """
    df_pp = df_pp.copy()
    df_pp["is_pain_point"] = 1

    # Merge Stage 0 with pain point outputs (left join: keep all negative posts)
    df = df_stage0.merge(
        df_pp[["post_id", "root_cause_summary", "pain_point_snippet", "is_pain_point"]],
        on="post_id",
        how="left",
    )
    df["is_pain_point"] = df["is_pain_point"].fillna(0).astype(int)

    # Merge in post_global_index to attach cluster_id + global_cluster_id + course_title
    df = df.merge(
        df_post_global[["post_id", "cluster_id", "global_cluster_id", "course_code", "course_title"]],
        on="post_id",
        how="left",
        suffixes=("", "_from_post_index"),
    )

    # Merge cluster_global_index to attach normalized_issue_label and provisional_label
    df = df.merge(
        df_cluster_global[
            [
                "cluster_id",
                "global_cluster_id",
                "provisional_label",
                "normalized_issue_label",
            ]
        ],
        on=["cluster_id", "global_cluster_id"],
        how="left",
    )

    # Attach course metadata (college, canonical catalog title)
    df = df.merge(
        df_courses.rename(
            columns={
                "CourseCode": "course_code",
                "Title": "course_title_catalog",
                "Colleges": "college_list",
            }
        ),
        on="course_code",
        how="left",
    )

    # Choose a single course title field
    def choose_title(row):
        if pd.notna(row.get("course_title")):
            return row["course_title"]
        if pd.notna(row.get("course_title_catalog")):
            return row["course_title_catalog"]
        return None

    df["course_title_final"] = df.apply(choose_title, axis=1)

    return df


def collapse_to_post_level(df: pd.DataFrame) -> pd.DataFrame:
