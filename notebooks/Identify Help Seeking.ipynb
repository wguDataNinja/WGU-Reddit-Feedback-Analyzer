{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26463c91-c8ef-4e1e-93e4-10a7f1072f87",
   "metadata": {},
   "source": [
    "# Identify Help-Seeking Posts\n",
    "\n",
    "Detect posts where students ask for help using NLP tools and data-driven keyword discovery.\n",
    "\n",
    "## Goal\n",
    "Go beyond manual keywords to uncover real help-seeking language patterns from student discourse.\n",
    "\n",
    "## Data Source\n",
    "**WGU-Reddit Database**: `WGU-Reddit/db/WGU-Reddit.db`\n",
    "- Contains 18,000+ posts from ~50 WGU-related subreddits\n",
    "- Covers posts from December 28, 2014 to July 14, 2025\n",
    "- Includes post metadata (scores, comments, timestamps)\n",
    "- *All data is sourced from public Reddit posts. No usernames or personal identifiers are stored.*\n",
    "\n",
    "## Definitions\n",
    "\n",
    "- **Help-Seeking**: A post where the author is asking for guidance, support, or answers—typically involving a need, confusion, or problem to solve.\n",
    "\n",
    "    - **Explicit Help-Seeking**: Clearly stated requests, such as *\"Can someone explain this?\"*\n",
    "\n",
    "    - **Implicit Help-Seeking**: Indirect expressions of struggle or uncertainty that imply a need for help. In natural conversation, people often ask for help without directly saying so. This kind of intent is hard to detect algorithmically. For example:\n",
    "        - “This situation is getting out of hand.”\n",
    "        - “I feel totally stuck.”\n",
    "        - “Nothing is working.”\n",
    "\n",
    "        These cases are difficult to classify and are excluded from this phase. We will refine this example list as we encounter real posts.\n",
    "\n",
    "        We will narrow our scope to explicit asks due to the extreme challenge of modeling indirect emotional cues without tone or context—“even humans struggle to detect the emotion of user utterance solely on the basis of text” (Chatterjee et al., 2019).\n",
    "\n",
    "- **Non-Help-Seeking**: Posts that are informational, reflective, or conversational with no clear request for help.\n",
    "\n",
    "*Note: All posts come from WGU-related subreddits, so discussions are generally university-adjacent, though not always strictly academic.*\n",
    "\n",
    "## Methodology\n",
    "\n",
    "**Step 1: Load and Clean Data**\n",
    "- Load posts from database\n",
    "- Filter to last 24 hours for manual labeling sample\n",
    "- Combine & lowercase `title` and `selftext` into a single `post_text` field\n",
    "- Create baseline dataset for pattern discovery\n",
    "\n",
    "**Step 2: Manual Labeling**\n",
    "- Create ground truth dataset with explicit help-seeking labels\n",
    "- Focus on direct questions and requests only (exclude implicit help-seeking)\n",
    "- Establish single source of truth for model training\n",
    "\n",
    "**Step 3: Baseline Keyword Detection**\n",
    "- Start with strongest anchor: question mark (`?`)\n",
    "- Evaluate performance against manual labels\n",
    "- Analyze false positives/negatives for pattern insights\n",
    "- Build a robust institutional stopword list using `FreqDist` on WGU Catalog Sections.\n",
    "\n",
    "**Step 4: NLP Phrase Discovery**\n",
    "- Extract linguistic patterns from labeled help-seeking posts\n",
    "- Use NLTK tools:\n",
    "  - `FreqDist()` → identify top unigrams\n",
    "  - `bigrams()` / `trigrams()` → discover help phrases\n",
    "  - `common_contexts()` → explore usage of \"help\", \"stuck\", etc.\n",
    "\n",
    "**Step 5: Refine & Expand**\n",
    "- Build enhanced keyword list (`help_keywords_v2`)\n",
    "- Scale to full database for training/testing\n",
    "- (Optional) Train classifier using discovered features\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c0a5c5-7d41-40b2-8ef2-bcd5ce212381",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "be94ed41-8ca4-45e5-92b5-22bdf2334af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "\n",
    "\n",
    "# Set project root to one level above current notebook directory\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from utils.db_connection import get_db_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb60263-f4c2-43a3-9a2a-68f69efeadac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "572efcb3-92b1-45f4-82dd-2ece95d0064a",
   "metadata": {},
   "source": [
    "## Step 1: Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8c2da091-f219-403f-a8fa-be7a08716db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18829 posts from 51 subreddits (2014-12-28 to 2025-07-15)\n",
      "Filtered 83 posts from the last 24 hours\n"
     ]
    }
   ],
   "source": [
    "db = get_db_connection()\n",
    "df = pd.read_sql_query(\n",
    "    \"\"\"\n",
    "    SELECT p.post_id, p.subreddit_id, p.title, p.selftext, p.created_utc,\n",
    "           p.score, p.num_comments, p.permalink, s.name AS subreddit_name\n",
    "    FROM posts p\n",
    "    LEFT JOIN subreddits s ON p.subreddit_id = s.subreddit_id\n",
    "    \"\"\", db)\n",
    "db.close()\n",
    "\n",
    "df['created_at'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "\n",
    "# Summary metrics\n",
    "total_posts = len(df)\n",
    "unique_subs = df['subreddit_name'].nunique()\n",
    "min_date = df['created_at'].min().strftime('%Y-%m-%d')\n",
    "max_date = df['created_at'].max().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Loaded {total_posts} posts from {unique_subs} subreddits ({min_date} to {max_date})\")# Cell 2: Filter posts from the last 24 hours\n",
    "\n",
    "latest_timestamp = df['created_at'].max()\n",
    "df = df[df['created_at'] >= latest_timestamp - pd.Timedelta(hours=24)]\n",
    "df = df[['post_id', 'title', 'selftext']]\n",
    "\n",
    "print(f\"Filtered {len(df)} posts from the last 24 hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3836d528-30c6-48d4-bec7-c22eab5b1719",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Combine Title + Selftext\n",
    "Lowercase for keyword matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "110fa17f-1a35-4597-b5b0-2f716c224ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_clean_text(df):\n",
    "    \"\"\"\n",
    "    Returns a cleaned text Series combining 'title' and 'selftext'.\n",
    "    \"\"\"\n",
    "    post_text = (df['title'].fillna('') + ' ' + df['selftext'].fillna('')).str.strip()\n",
    "    post_text = post_text.str.lower()\n",
    "    return post_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ace415b3-376d-42ea-894e-23f72da40da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df columns: ['post_id', 'title', 'selftext']\n"
     ]
    }
   ],
   "source": [
    "print(\"df columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bac5d4af-74ed-42f5-9455-0d8dcf6c4e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df columns: ['post_id', 'title', 'selftext']\n",
      "df_clean columns: ['post_id', 'title', 'selftext', 'post_text']\n"
     ]
    }
   ],
   "source": [
    "# combine and clean with updated column name 'post_text'\n",
    "df_clean = df.copy()\n",
    "df_clean['post_text'] = combine_and_clean_text(df_clean)\n",
    "\n",
    "print(\"df columns:\", df.columns.tolist())\n",
    "print(\"df_clean columns:\", df_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b3d89d3a-3d68-4768-8dc6-b83a12da0522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>post_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1lzfct7</td>\n",
       "      <td>MSN Application</td>\n",
       "      <td>I am in the process of filling out the application for MSN and it won’t let me pass the employment page even though I ha</td>\n",
       "      <td>msn application i am in the process of filling out the application for msn and it won’t let me pass the employment page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1lzf7hc</td>\n",
       "      <td>It is done.</td>\n",
       "      <td>Honestly one of the hardest things I've ever done for myself. Did terrible in highschool (was diagnosed at 15 with MS),</td>\n",
       "      <td>it is done. honestly one of the hardest things i've ever done for myself. did terrible in highschool (was diagnosed at 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1lzfgof</td>\n",
       "      <td>Dse withdraw process mba admissions?</td>\n",
       "      <td>How we can withdraw admission form dse mba program</td>\n",
       "      <td>dse withdraw process mba admissions? how we can withdraw admission form dse mba program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1m07p15</td>\n",
       "      <td>D427 | Nervous, haven't coded in forever. Need tips/advice.</td>\n",
       "      <td>I'm trying to graduate and I have a few courses left to go.  I am really scared of D427 (I did not have to do 426, I gue</td>\n",
       "      <td>d427 | nervous, haven't coded in forever. need tips/advice. i'm trying to graduate and i have a few courses left to go.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1m07304</td>\n",
       "      <td>Payment</td>\n",
       "      <td>Looking to start wgu for a bachelors \\n\\n\\nDo I have to pay first before starting ?</td>\n",
       "      <td>payment looking to start wgu for a bachelors \\n\\n\\ndo i have to pay first before starting ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display changes\n",
    "preview = df_clean[['post_id','title', 'selftext', 'post_text']].copy()\n",
    "pd.set_option('display.max_colwidth', None)  # Don't let pandas truncate; we do it ourselves\n",
    "preview['selftext'] = preview['selftext'].str.slice(0, 120)\n",
    "preview['post_text'] = preview['post_text'].str.slice(0, 120)\n",
    "\n",
    "html = preview.head(5).to_html(index=False, escape=False)\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8913e6ba-71d6-4cd0-bec5-165ef1ce5bdc",
   "metadata": {},
   "source": [
    "## Step 2: Manual Labeling\n",
    "### Create labelled dataset\n",
    "- Export a template CSV\n",
    "`manual_help_truth.csv`:\n",
    "| **post_id** | **text**               | **help_truth** |\n",
    "|-------------|------------------------|----------------|\n",
    "| abc123      | is this a question?    | 0 → 1          |  \n",
    "\n",
    "- Manually tag and move file to /data\n",
    "- Merge `help_truth` tag into dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99416e-f817-48d4-9480-55275baf082d",
   "metadata": {},
   "source": [
    "### Export a template CSV to manually tag help-seeking posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5be08dd2-f185-407a-8208-707ce32c239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_clean columns: ['post_id', 'title', 'selftext', 'post_text']\n",
      "Exported manual_help_truth.csv with columns: post_id, post_text, help_truth\n"
     ]
    }
   ],
   "source": [
    "print(\"df_clean columns:\", df_clean.columns.tolist())\n",
    "\n",
    "df_clean[['post_id', 'post_text']].assign(help_truth=0).to_csv('outputs/manual_help_truth.csv', index=False)\n",
    "\n",
    "print(\"Exported manual_help_truth.csv with columns: post_id, post_text, help_truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb8773b-08a5-41dc-8706-4f161f88f9dd",
   "metadata": {},
   "source": [
    "### Merge 'help_truth' tag into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "758d39eb-b6eb-4fa6-9464-f266caa7df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_truth_flag_to_clean.py\n",
    "df_truth = pd.read_csv('data/manual_help_truth.csv')[['post_id', 'help_truth']]\n",
    "df_labeled = df_clean.merge(df_truth, on='post_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a1199d-bf64-435d-bac3-63d895da6017",
   "metadata": {},
   "source": [
    "### Display labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d2e5dac8-ca6f-4b03-8591-c82cdf2d927e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_labeled columns: ['post_id', 'title', 'selftext', 'post_text', 'help_truth']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>post_id</th>\n",
       "      <th>post_text</th>\n",
       "      <th>help_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1lzfct7</td>\n",
       "      <td>msn application i am in the process of filling out the application for msn and it won’t let me pass the employment page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1lzf7hc</td>\n",
       "      <td>it is done. honestly one of the hardest things i've ever done for myself. did terrible in highschool (was diagnosed at 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1lzfgof</td>\n",
       "      <td>dse withdraw process mba admissions? how we can withdraw admission form dse mba program</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1m07p15</td>\n",
       "      <td>d427 | nervous, haven't coded in forever. need tips/advice. i'm trying to graduate and i have a few courses left to go.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1m07304</td>\n",
       "      <td>payment looking to start wgu for a bachelors \\n\\n\\ndo i have to pay first before starting ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preview = df_labeled[['post_id', 'post_text', 'help_truth']].copy()\n",
    "preview['post_text'] = preview['post_text'].str.slice(0, 120)\n",
    "html = preview.head(5).to_html(index=False, escape=False)\n",
    "print(\"df_labeled columns:\", df_labeled.columns.tolist())\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57acfe-84bf-4b3b-a57b-be5d19625fef",
   "metadata": {},
   "source": [
    "## Step 3: Baseline Keyword Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "db378c9d-adb1-4494-9387-49c63dc7849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['?']  # initial list\n",
    "\n",
    "# Create keyword_match and help_flag\n",
    "def detect_keywords(post_text):\n",
    "    matches = [kw for kw in keywords if kw in post_text]\n",
    "    return ' | '.join(matches), int(bool(matches))\n",
    "\n",
    "df_labeled['keyword_match'], df_labeled['help_flag'] = zip(*df_labeled['post_text'].map(detect_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ea08e35f-b4ed-4dcc-8910-4a7ac977d520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword Match Results (keywords: ?)\n",
      "Total posts reviewed:             83\n",
      "Posts flagged by keyword:         54\n",
      "Help-seeking posts (ground truth):62\n",
      "Correctly classified posts:       73\n",
      "False positives (flagged, not truth): 1\n",
      "False negatives (missed, were truth): 9\n",
      "Accuracy:                         0.88\n"
     ]
    }
   ],
   "source": [
    "# Summary metrics\n",
    "total_posts = len(df_labeled)\n",
    "total_flagged = df_labeled['help_flag'].sum()\n",
    "total_truth = df_labeled['help_truth'].sum()\n",
    "correct_matches = (df_labeled['help_flag'] == df_labeled['help_truth']).sum()\n",
    "accuracy = correct_matches / total_posts\n",
    "false_positives = ((df_labeled['help_flag'] == 1) & (df_labeled['help_truth'] == 0)).sum()\n",
    "false_negatives = ((df_labeled['help_flag'] == 0) & (df_labeled['help_truth'] == 1)).sum()\n",
    "\n",
    "print(f\"Keyword Match Results (keywords: {', '.join(keywords)})\")\n",
    "print(f\"Total posts reviewed:             {total_posts}\")\n",
    "print(f\"Posts flagged by keyword:         {total_flagged}\")\n",
    "print(f\"Help-seeking posts (ground truth):{total_truth}\")\n",
    "print(f\"Correctly classified posts:       {correct_matches}\")\n",
    "print(f\"False positives (flagged, not truth): {false_positives}\")\n",
    "print(f\"False negatives (missed, were truth): {false_negatives}\")\n",
    "print(f\"Accuracy:                         {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e533a-8d5a-4a3e-9346-ee3dc6b5660e",
   "metadata": {},
   "source": [
    "### now test if it **ends** with a `?`. -- wrap this in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fc6e63e9-8e50-4c20-80b2-c6ed1f389a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_keyword_match.py\n",
    "\n",
    "def test_keywords(df, keywords):\n",
    "    def detect_keywords(post_text):\n",
    "        matches = [kw for kw in keywords if kw in post_text]\n",
    "        return ' | '.join(matches), int(bool(matches))\n",
    "\n",
    "    df = df.copy()\n",
    "    df['keyword_match'], df['help_flag'] = zip(*df['post_text'].map(detect_keywords))\n",
    "    accuracy = (df['help_flag'] == df['help_truth']).mean()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# keywords = ['?', 'help', 'advice', 'anyone', 'how do i', 'need', 'should i', 'what to do']\n",
    "# accuracy = test_keywords(df_labeled, keywords)\n",
    "# print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dee60c50-ee1f-41fd-a203-32efc08712e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.46\n",
      "Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "keywords = ['how']\n",
    "accuracy = test_keywords(df_labeled, keywords)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "keywords = ['?']\n",
    "accuracy = test_keywords(df_labeled, keywords)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5e1e0e93-b552-4052-b879-6b597bd7ed26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "keywords = ['?']\n",
    "accuracy = test_keywords(df_labeled, keywords)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18481f7b-bd25-4472-ba50-6b031708ab59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c5f8c84d-3538-42c3-ad64-f8e9186510f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword Match Results (sentence ends with '?'):\n",
      "Total posts reviewed:             83\n",
      "Posts flagged by keyword:         54\n",
      "Help-seeking posts (ground truth):62\n",
      "Correctly classified posts:       73\n",
      "False positives (flagged, not truth): 1\n",
      "False negatives (missed, were truth): 9\n",
      "Accuracy:                         0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/buddy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Updated keyword detector: sentence ends with '?'\n",
    "def ends_with_question(post_text):\n",
    "    try:\n",
    "        sentences = sent_tokenize(post_text)\n",
    "        return any(s.strip().endswith('?') for s in sentences)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Apply to labeled dataset\n",
    "df_labeled['help_flag'] = df_labeled['post_text'].apply(ends_with_question)\n",
    "df_labeled['help_flag'] = df_labeled['help_flag'].astype(int)\n",
    "df_labeled['keyword_match'] = df_labeled['help_flag'].map(lambda x: '?' if x else '')\n",
    "\n",
    "# Metrics\n",
    "total_posts = len(df_labeled)\n",
    "total_flagged = df_labeled['help_flag'].sum()\n",
    "total_truth = df_labeled['help_truth'].sum()\n",
    "correct_matches = (df_labeled['help_flag'] == df_labeled['help_truth']).sum()\n",
    "accuracy = correct_matches / total_posts\n",
    "false_positives = ((df_labeled['help_flag'] == 1) & (df_labeled['help_truth'] == 0)).sum()\n",
    "false_negatives = ((df_labeled['help_flag'] == 0) & (df_labeled['help_truth'] == 1)).sum()\n",
    "\n",
    "print(\"Keyword Match Results (sentence ends with '?'):\")\n",
    "print(f\"Total posts reviewed:             {total_posts}\")\n",
    "print(f\"Posts flagged by keyword:         {total_flagged}\")\n",
    "print(f\"Help-seeking posts (ground truth):{total_truth}\")\n",
    "print(f\"Correctly classified posts:       {correct_matches}\")\n",
    "print(f\"False positives (flagged, not truth): {false_positives}\")\n",
    "print(f\"False negatives (missed, were truth): {false_negatives}\")\n",
    "print(f\"Accuracy:                         {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ee8862-0703-450e-ae76-b9308c2c7895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b155a8-4aa0-4d7f-9cd3-9175aa94fd81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a384186f-4751-49ef-9556-29e5c687d9b9",
   "metadata": {},
   "source": [
    "### inspect false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0330c0cb-1a23-4103-909e-949212379edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <h4>False Positives (Flagged, but not Help-Seeking)</h4>\n",
       "    <div style=\"max-height:500px; overflow:auto; border:1px solid #ccc; padding:10px; font-family:monospace; font-size:12px\">\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>help_truth</th>\n",
       "      <th>help_flag</th>\n",
       "      <th>keyword_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1lzuqo9</td>\n",
       "      <td>D316/D317 IT Foundations/Applications 1101/1102 (Completed) in 2 months</td>\n",
       "      <td>they just purchased. What should you do **FIRST*<mark>*?\\n</mark>\\nA. Install the new RAM and power on the system</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filename: step3_false_positives.py\n",
    "\n",
    "# Apply highlight and filter false positives\n",
    "df_display = df_labeled.copy()\n",
    "df_display['title'] = df_display['title'].fillna('').apply(highlight_snippet)\n",
    "df_display['selftext'] = df_display['selftext'].fillna('').apply(highlight_snippet)\n",
    "\n",
    "fp_df = df_display[(df_labeled['help_flag'] == 1) & (df_labeled['help_truth'] == 0)]\n",
    "\n",
    "def render_table(df, title):\n",
    "    html = df[['post_id', 'title', 'selftext', 'help_truth', 'help_flag', 'keyword_match']].to_html(index=False, escape=False)\n",
    "    display(HTML(f\"\"\"\n",
    "    <h4>{title}</h4>\n",
    "    <div style=\"max-height:500px; overflow:auto; border:1px solid #ccc; padding:10px; font-family:monospace; font-size:12px\">\n",
    "    {html}\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "\n",
    "render_table(fp_df, \"False Positives (Flagged, but not Help-Seeking)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4adb2b-f511-4286-9443-39e1aaef6985",
   "metadata": {},
   "source": [
    "**Observation:**  \n",
    "The false positive shown above was triggered by a `?` character found in a **URL**, not in an actual help-seeking question. \n",
    "\n",
    "We should **remove URLs** during preprocessing to avoid misleading keyword matches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb990c62-e074-4e7a-9a82-3820ec23ab70",
   "metadata": {},
   "source": [
    "### inspect false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "01c1d016-cfdf-4466-8944-b3f95dc97424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <h4>False Negatives (Missed, but Help-Seeking)</h4>\n",
       "    <div style=\"max-height:500px; overflow:auto; border:1px solid #ccc; padding:10px; font-family:monospace; font-size:12px\">\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>help_truth</th>\n",
       "      <th>help_flag</th>\n",
       "      <th>keyword_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1lzxbgh</td>\n",
       "      <td>FASFA Issues</td>\n",
       "      <td>I Graduated 2022 High school diploma, and wanted to take a break for a semester or two. But my famil...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1lzvqd9</td>\n",
       "      <td>Anyone have an update on the Notarized Fasfa Verification.</td>\n",
       "      <td>My documents were uploaded and expidited on friday I haven't received any update since. Just wonderi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1lzq8vi</td>\n",
       "      <td>Capstone “write up and summary product is missing” please help</td>\n",
       "      <td>Please help. I'm on an extension and needing to wrap up my capstone. It has been rejected with this ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1lzpqre</td>\n",
       "      <td>Statement of Purpose</td>\n",
       "      <td>Hello, I wanted to make a thread for the people who had to fill out and submit the statement of purp...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1lzohxm</td>\n",
       "      <td>ITIL4 room requirements</td>\n",
       "      <td>Hey,\\n\\nCan a mirror be in the bathroom when I take my exam. That is the only room in my house with 1 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1lzvmzt</td>\n",
       "      <td>Goreact</td>\n",
       "      <td>I am struggling trying figure out how to use The goreact recording system. Help me</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1lzpgag</td>\n",
       "      <td>Capstone Trouble - I don't understand what I'm doing wrong</td>\n",
       "      <td>Please help. I'm on an extension and needing to wrap up my capstone. It has been rejected with this ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1m03meb</td>\n",
       "      <td>D280 Javascript Programming</td>\n",
       "      <td>I just can't seem to get the map to load / become interactive. Desperately need help</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1m04rem</td>\n",
       "      <td>Failed OA</td>\n",
       "      <td>Okay so I’m not understanding this is my 3rd retake for the Learners and learning science class I un...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filename: step3_false_negatives.py\n",
    "\n",
    "# Apply highlight and filter false negatives\n",
    "fn_df = df_display[(df_labeled['help_flag'] == 0) & (df_labeled['help_truth'] == 1)]\n",
    "\n",
    "render_table(fn_df, \"False Negatives (Missed, but Help-Seeking)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa2012-847d-449c-b578-952867b9d446",
   "metadata": {},
   "source": [
    "**Observation:** \n",
    "false negatives show obvious keywords like \"help\". We will analyze them methodically to update our keyword list. \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b7e714-5805-4826-9d63-a490c812696f",
   "metadata": {},
   "source": [
    "## Step 4 is to filter full dataset by `?` and emerge patterns. Continued on Help Seeking Step 4.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64702bc7-6aff-4036-9078-079adb429755",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The `?` proved 88% accurate identifying help-seeking posts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3c343cfe-b516-41e6-90fe-3e3a368ca1f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_posts_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[130]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mload_posts_dataframe\u001b[49m()\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Summary metrics\u001b[39;00m\n\u001b[32m      3\u001b[39m total_posts = \u001b[38;5;28mlen\u001b[39m(df)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_posts_dataframe' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ff0d461a-5a08-45cb-af56-ea4b4f4cb780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top unigrams:\n",
      "[('i', 247), ('.', 168), ('to', 139), ('and', 125), ('the', 117), ('a', 99), ('?', 95), (',', 95), ('in', 88), ('my', 83), ('of', 71), ('for', 69), ('’', 66), ('│', 60), ('is', 55), ('this', 51), ('have', 50), ('it', 47), (')', 40), ('*', 40)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Filter all posts containing a question mark\n",
    "df_question_posts = df[df['post_text'].str.contains(r'\\?', na=False)]\n",
    "\n",
    "# Combine into one text blob\n",
    "all_text = ' '.join(df_question_posts['post_text'].dropna().tolist())\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(all_text)\n",
    "\n",
    "# Unigrams\n",
    "fdist_unigram = FreqDist(tokens)\n",
    "print(\"Top unigrams:\")\n",
    "print(fdist_unigram.most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c887fd-79db-431f-8052-13a6f212ac8a",
   "metadata": {},
   "source": [
    "**Observation:**  \n",
    "Top unigrams are dominated by stopwords and punctuation.\n",
    "\n",
    "**Plan:**  \n",
    "- *Temporarily* use NLTK's stopword list to filter unigrams for clarity.\n",
    "- Retain stopwords for bigram/trigram discovery — since many help-seeking phrases rely on them (e.g., *\"how do I\"*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0fd47106-c8ed-44f2-b366-454b8fa91a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/buddy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e5a94b39-246c-4e75-bfa7-9392bcb3c6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top unigrams (stopwords & punctuation removed):\n",
      "[('├──', 36), ('accounting', 25), ('get', 24), ('classes', 24), ('degree', 22), (\"'m\", 20), ('take', 20), ('course', 17), ('one', 17), ('wgu', 15), ('time', 15), ('anyone', 15), ('business', 15), ('class', 13), ('school', 13), ('work', 13), ('trying', 12), ('first', 12), ('like', 11), ('test', 11)]\n"
     ]
    }
   ],
   "source": [
    "# Define stopwords and punctuation set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation) # '?' removal ok - already a keyword\n",
    "\n",
    "# Filter tokens\n",
    "filtered_tokens = [t for t in tokens if t.lower() not in stop_words and t not in punctuation and len(t) > 1]\n",
    "\n",
    "# Filtered unigram frequency\n",
    "fdist_unigram_filtered = FreqDist(filtered_tokens)\n",
    "print(\"Top unigrams (stopwords & punctuation removed):\")\n",
    "print(fdist_unigram_filtered.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a62af26-65bb-493f-a3eb-2ca380bf2c93",
   "metadata": {},
   "source": [
    "### Import the individual unigrams for each Catalog section and combine\n",
    "(see notebooks/Catalog Stop Words.ipynb for methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cfc3fc-cc0b-4cd7-bcca-a8f92a12ad1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e2c43ad-b51b-4372-ae51-e5a7872ff9d5",
   "metadata": {},
   "source": [
    "**Observation:**  \n",
    "After removing NLTK stopwords, the top unigrams include domain-specific terms that are frequent but do not indicate help-seeking intent.\n",
    "\n",
    "**Domain-specific stopwords identified:**\n",
    "- `accounting`\n",
    "- `classes`\n",
    "- `degree`\n",
    "- `course`\n",
    "- `wgu`\n",
    "- `business`\n",
    "- `class`\n",
    "- `school`\n",
    "- `test`\n",
    "  \n",
    "**Plan:**  \n",
    "Introduce a custom stopword list to filter these out and improve signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396db34a-de56-49d1-b279-e1dfa363aa32",
   "metadata": {},
   "source": [
    "## Work on the 2025_06 catalog to generate custom stopwords list is on notebooks/Catalog Stop Words.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21086f8-3009-4a62-831b-86dc0fe06e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "baa5d91d-deab-4c3b-ab59-f63a31bb474e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top unigrams (NLTK + domain stopwords removed):\n",
      "[('├──', 36), ('get', 24), (\"'m\", 20), ('take', 20), ('one', 17), ('time', 15), ('anyone', 15), ('work', 13), ('trying', 12), ('first', 12), ('like', 11), (\"'s\", 11), (\"n't\", 10), ('need', 10), ('advice', 10), ('capstone', 10), ('credits', 10), ('getting', 9), ('term', 9), ('transfer', 9), ('oa', 9), ('finish', 9), ('help', 9), ('questions', 8), ('complete', 8), ('know', 8), ('second', 8), ('foundations', 8), ('management', 8), ('└──', 8), ('study', 8), ('intermediate', 8), ('go', 7), ('really', 7), ('back', 7), ('tips', 7), ('new', 7), ('looking', 7), ('start', 7), ('taken', 7), ('applications', 7), ('im', 7), ('also', 7), ('long', 7), ('taking', 7), ('thanks', 7), ('project', 7), ('hello', 7), ('times', 7), ('every', 7)]\n"
     ]
    }
   ],
   "source": [
    "custom_stopwords = {\n",
    "    'wgu', 'class', 'classes', 'course', 'degree',\n",
    "    'school', 'business', 'accounting', 'test'\n",
    "}\n",
    "custom_stopwords = stop_words.union(custom_stopwords)\n",
    "\n",
    "# Re-filter tokens\n",
    "filtered_tokens_custom = [\n",
    "    t for t in tokens\n",
    "    if t.lower() not in custom_stopwords and t not in punctuation and len(t) > 1\n",
    "]\n",
    "\n",
    "# Updated unigram frequency with domain stopwords removed\n",
    "fdist_unigram_custom = FreqDist(filtered_tokens_custom)\n",
    "print(\"Top unigrams (NLTK + domain stopwords removed):\")\n",
    "print(fdist_unigram_custom.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "40d6de1d-3173-47cd-8a09-08c2e77e223c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top bigrams (NLTK + domain stopwords removed):\n",
      "[(('bachelor', \"'s\"), 5), (('feel', 'like'), 4), (('markdown', 'cell'), 4), (('health', 'human'), 3), (('human', 'services'), 3), (('commit', 'start'), 3), (('practice', 'tests'), 3), (('write', 'summary'), 3), (('summary', 'product'), 3), (('please', 'help'), 3), (('transferred', 'gpas'), 3), (('need', 'get'), 3), (('supply', 'chain'), 3), (('-discrete', 'math'), 3), (('discrete', 'math'), 3), (('student', 'teaching'), 3), (('full', 'time'), 3), ((\"'m\", 'trying'), 2), (('easier', 'c877'), 2), (('c877', 'c883'), 2), (('want', 'take'), 2), (('view', 'poll'), 2), (('poll', 'https'), 2), (('bit', 'time'), 2), (('long', 'take'), 2), (('hoping', 'complete'), 2), (('one', 'semester'), 2), (('traditional', 'college'), 2), (('people', 'look'), 2), (('much', 'time'), 2), (('tests', 'taken'), 2), (('taken', 'one'), 2), (('anyone', 'taken'), 2), (('transcript', 'evaluation'), 2), (('trying', 'get'), 2), (('take', 'break'), 2), (('semester', 'failed'), 2), (('ratio', 'gpa'), 2), (('chat', 'gpt'), 2), (('take', 'order'), 2), (('help', \"'m\"), 2), ((\"'m\", 'extension'), 2), (('extension', 'needing'), 2), (('needing', 'wrap'), 2), (('wrap', 'rejected'), 2), (('rejected', 'exact'), 2), (('exact', 'message'), 2), (('message', 'twice'), 2), (('twice', 'included'), 2), (('included', '``'), 2)]\n"
     ]
    }
   ],
   "source": [
    "# filename: step4_bigrams_filtered.py\n",
    "\n",
    "from nltk.util import bigrams\n",
    "\n",
    "# Extend custom stopwords\n",
    "# combined_keywords.py\n",
    "\n",
    "domain_stopwords = [\n",
    "    'academic', 'accounting', 'advanced', 'course', 'cus', \"cu's\", 'data', 'degree',\n",
    "    'doctorate', 'education', 'engineering', 'faculty', 'financial', 'foundations',\n",
    "    'governors', 'graduate', 'health', 'healthcare', 'information', 'learn',\n",
    "    'learners', 'learning', 'management', 'marketing', 'master', 'methods', 'mgmt',\n",
    "    'nurs', 'nursing', 'payment', 'phd', 'practice', 'program', 'project',\n",
    "    'requirements', 'science', 'security', 'skills', 'software', 'student',\n",
    "    'students', 'teaching', 'term', 'tuition', 'university', 'western', 'wgu',\n",
    "    'wgu', 'class', 'classes', 'course', 'degree', 'school', 'business',\n",
    "    'accounting', 'test', 'capstone', 'credits', 'term', 'transfer', 'oa',\n",
    "    'foundations', 'management', 'study', 'applications', 'project', 'times'\n",
    "]\n",
    "custom_stopwords = stop_words.union(domain_stopwords)\n",
    "\n",
    "# Re-filter tokens\n",
    "filtered_tokens_custom = [\n",
    "    t for t in tokens\n",
    "    if t.lower() not in custom_stopwords and t not in punctuation and len(t) > 1\n",
    "]\n",
    "\n",
    "# Generate and count bigrams\n",
    "bigram_tokens = list(bigrams(filtered_tokens_custom))\n",
    "fdist_bigrams_custom = FreqDist(bigram_tokens)\n",
    "\n",
    "print(\"Top bigrams (NLTK + domain stopwords removed):\")\n",
    "print(fdist_bigrams_custom.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5996e655-0a99-4252-ab14-cc49be4ebf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top trigrams (NLTK + domain stopwords removed):\n",
      "[(('health', 'human', 'services'), 3), (('write', 'summary', 'product'), 3), (('easier', 'c877', 'c883'), 2), (('view', 'poll', 'https'), 2), (('please', 'help', \"'m\"), 2), (('help', \"'m\", 'extension'), 2), ((\"'m\", 'extension', 'needing'), 2), (('extension', 'needing', 'wrap'), 2), (('needing', 'wrap', 'rejected'), 2), (('wrap', 'rejected', 'exact'), 2), (('rejected', 'exact', 'message'), 2), (('exact', 'message', 'twice'), 2), (('message', 'twice', 'included'), 2), (('twice', 'included', '``'), 2), (('included', '``', 'write'), 2), (('``', 'write', 'summary'), 2), (('summary', 'product', \"''\"), 2), (('product', \"''\", 'made'), 2), ((\"''\", 'made', 'jupyter'), 2), (('made', 'jupyter', 'notebook'), 2), (('jupyter', 'notebook', 'first'), 2), (('notebook', 'first', 'submission'), 2), (('first', 'submission', 'every'), 2), (('submission', 'every', 'piece'), 2), (('every', 'piece', 'write-up'), 2), (('piece', 'write-up', 'markdown'), 2), (('write-up', 'markdown', 'cell'), 2), (('markdown', 'cell', 'within'), 2), (('cell', 'within', 'notebook'), 2), (('within', 'notebook', 'second'), 2), (('notebook', 'second', 'submission'), 2), (('second', 'submission', 'removed'), 2), (('submission', 'removed', 'every'), 2), (('removed', 'every', 'markdown'), 2), (('every', 'markdown', 'cell'), 2), (('markdown', 'cell', 'turned'), 2), (('cell', 'turned', 'markdown'), 2), (('turned', 'markdown', 'files'), 2), (('markdown', 'files', 'everything'), 2), (('files', 'everything', 'submitted'), 2), (('everything', 'submitted', 'zip'), 2), (('submitted', 'zip', 'folder'), 2), (('zip', 'folder', 'structure'), 2), (('folder', 'structure', 'capstone/'), 2), (('structure', 'capstone/', '├──'), 2), (('capstone/', '├──', 'app/'), 2), (('├──', 'app/', '├──'), 2), (('app/', '├──', 'app.log'), 2), (('├──', 'app.log', '├──'), 2), (('├──', 'documentation/', '├──'), 2)]\n"
     ]
    }
   ],
   "source": [
    "# filename: step4_trigrams_filtered.py\n",
    "\n",
    "from nltk.util import trigrams\n",
    "\n",
    "# Generate and count trigrams\n",
    "trigram_tokens = list(trigrams(filtered_tokens_custom))\n",
    "fdist_trigrams_custom = FreqDist(trigram_tokens)\n",
    "\n",
    "print(\"Top trigrams (NLTK + domain stopwords removed):\")\n",
    "print(fdist_trigrams_custom.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e5aae7e8-7cba-4a14-95eb-d67e7095938a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2346319142.py, line 13)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[113]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mTop unigrams (NLTK + domain stopwords removed):\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Focus on help-seeking posts\n",
    "help_posts = df_labeled[df_labeled['help_truth'] == 1]['post_text'].dropna().tolist()\n",
    "all_text = ' '.join(help_posts)\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(all_text)\n",
    "\n",
    "# Unigrams\n",
    "fdist_unigram = FreqDist(tokens)\n",
    "print(\"Top unigrams:\")\n",
    "print(fdist_unigram.most_common(20))\n",
    "\n",
    "Top unigrams (NLTK + domain stopwords removed):\n",
    "[('├──', 36), ('get', 24), (\"'m\", 20), ('take', 20), ('one', 17), ('time', 15), ('anyone', 15), ('work', 13), ('trying', 12), ('first', 12), ('like', 11), (\"'s\", 11), (\"n't\", 10), ('need', 10), ('advice', 10), ('capstone', 10), ('credits', 10), ('getting', 9), ('term', 9), ('transfer', 9)]\n",
    "# Trigrams\n",
    "trigram_tokens = list(trigrams(tokens))\n",
    "fdist_trigram = FreqDist(trigram_tokens)\n",
    "print(\"\\nTop trigrams:\")\n",
    "print(fdist_trigram.most_common(20))\n",
    "\n",
    "# Common contexts for \"help\", \"stuck\", etc.\n",
    "text_obj = Text(tokens)\n",
    "print(\"\\nContexts for 'help':\")\n",
    "text_obj.common_contexts(['help'])\n",
    "\n",
    "print(\"\\nContexts for 'stuck':\")\n",
    "text_obj.common_contexts(['stuck'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf01633-b78e-4dfc-842d-6a511de73f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867524d9-4485-4603-93a8-3d4eef60d60e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307819ad-6fab-4aac-a9c3-0778fcc219de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ae873a8-8bc7-4107-a5dd-05b413161a2b",
   "metadata": {},
   "source": [
    "## Step 5: Refine & Expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec6bd13-b88f-419d-bcd4-08cc6e5ca71a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf21539-636c-4f98-8506-412bfef6207f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0fdc29-b16e-4050-b956-175d0316446d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0605308-3015-4fee-a513-4c3bcf234183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a279891-a91e-41c4-ac23-6767175b27c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178391ae-5a37-4a2b-be49-5c7ad35b0b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b9917-c68d-44f1-bfde-9340a5d8679b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (WGU)",
   "language": "python",
   "name": "wgu-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
