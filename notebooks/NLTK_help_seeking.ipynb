{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1906cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5aea39e7",
   "metadata": {},
   "source": [
    "### üìå Help-Seeking Detection (NLTK + Keyword Match)\n",
    "\n",
    "**1. Load & Prep**\n",
    "- Load posts, combine `title` + `selftext` ‚Üí `post_text`\n",
    "- Lowercase for matching\n",
    "\n",
    "**2. Baseline Detection**\n",
    "- Flag posts with `help_keywords` ‚Üí `has_help_phrase`\n",
    "\n",
    "**3. Phrase Snippets**\n",
    "- Extract & highlight ~30-char context around matched phrases\n",
    "\n",
    "**4. Token Exploration**\n",
    "- Run `FreqDist()` on help-seeking posts\n",
    "- Remove stopwords/punctuation\n",
    "- Export top tokens\n",
    "\n",
    "**5. Build `help_keywords_v2`**\n",
    "- Manual review of top tokens\n",
    "- Apply to flag `has_help_v2`\n",
    "\n",
    "**6. Compare v1 vs v2**\n",
    "- v1: 13.5k posts, v2: 15.2k posts (+3.3k new)\n",
    "- ~1.9k missed by both\n",
    "\n",
    "**7. Phrase Discovery**\n",
    "- On `v2_only`:\n",
    "  - Extract bigrams/trigrams\n",
    "  - Use `common_contexts()` for ‚Äúhelp‚Äù, ‚Äúadvice‚Äù, etc.\n",
    "\n",
    "**8. Sentiment Analysis**\n",
    "- VADER sentiment ‚Üí histogram of all / help / missed\n",
    "- Found sentiment alone insufficient for detection\n",
    "\n",
    "**9. Missed Post Analysis**\n",
    "- Focus: `VADER < -0.2`, no help phrases\n",
    "- Token + bigram `FreqDist()` to find implicit help signals\n",
    "\n",
    "**10. Course Mentions**\n",
    "- Count posts with Top 20 course codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2b36dc-570f-49a2-8eee-df2e3ee36405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Project Bootstrapping ===\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Locate and add project root so 'utils' is importable\n",
    "ROOT_DIR = Path().resolve()\n",
    "while not (ROOT_DIR / \"utils\" / \"db_connection_new.py\").exists():\n",
    "    ROOT_DIR = ROOT_DIR.parent\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "# Now imports will work\n",
    "from utils.db_connection_new import load_posts_dataframe\n",
    "from utils.new_paths import DATA_DIR, OUTPUT_DIR\n",
    "\n",
    "# Input / Output files\n",
    "course_list = DATA_DIR / \"courses_with_college_v10.csv\"\n",
    "courses_top20 = DATA_DIR /\"reddit_top_20_mentioned_courses.csv\"\n",
    "output_dir = OUTPUT_DIR \n",
    "\n",
    "\n",
    "df = load_posts_dataframe()\n",
    "\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "display(df.head(3))  # optional if in notebook\n",
    "\n",
    "def combine_and_clean_text(df):\n",
    "    \"\"\"\n",
    "    Returns a cleaned text Series combining 'title' and 'selftext'.\n",
    "    \"\"\"\n",
    "    post_text = (df['title'].fillna('') + ' ' + df['selftext'].fillna('')).str.strip()\n",
    "    post_text = post_text.str.lower()\n",
    "    return post_text\n",
    "\n",
    "df['post_text'] = combine_and_clean_text(df).copy()\n",
    "\n",
    "display(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a28241d-59af-46f6-86fe-5f89d37cb9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "help_keywords = [\n",
    "    \"need help\", \"help!\", \"help with\", \"any advice\", \"looking for advice\",  \n",
    "    \"advice on\", \"tips on\", \"looking for tips\", \"need suggestions\",\n",
    "    \"need recommendations\", \"how do i\", \"where do i\", \"where can i\",\n",
    "    \"what do i\", \"when should i\", \"which should i\", \"does anyone know\",\n",
    "    \"does anyone have\", \"anyone know how\", \"can someone help\",\n",
    "    \"can anyone help\", \"stuck on\", \"struggling with\", \"cannot figure out\",\n",
    "    \"can‚Äôt figure out\", \"having trouble with\", \"confused about\",\n",
    "    \"lost on\", \"don‚Äôt understand\", \"not sure how\", \"no idea how\",\n",
    "    \"trying to figure out\", \"help me understand\", \"explain how\",\n",
    "    \"can someone explain\", \"make sense of\", \"anyone dealt with\",\n",
    "    \"how did you handle\", \"how did you manage\", \"what worked for you\",\n",
    "    \"am i missing something\", \"doing something wrong\", \"what am i doing wrong\",\n",
    "    \"should i be\", \"am i supposed to\", \"can anyone explain\",\n",
    "    \"what's the best way to\", \"any pointers on\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4409cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(df_posts.head(1))\n",
    "\n",
    "# === Load CSVs ===\n",
    "df_top20 = pd.read_csv(TOP20_CSV)\n",
    "df_courses = pd.read_csv(COURSES_CSV)\n",
    "\n",
    "print(\"‚úÖ Loaded top 20 courses:\")\n",
    "display(df_top20.head(1))\n",
    "\n",
    "print(\"‚úÖ Loaded full course/college catalog:\")\n",
    "display(df_courses.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf1c5fe",
   "metadata": {},
   "source": [
    "## Identify Help-Seeking Posts\n",
    "\n",
    "Detect posts where students ask for help ‚Äî explicitly  ‚Äî using NLP tools.\n",
    "\n",
    "---\n",
    "\n",
    "### Goal\n",
    "Go beyond manual keywords to uncover real help-seeking language patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### Plan\n",
    "\n",
    "#### Step 1: Baseline Keyword Match\n",
    "- Use `help_keywords` (~40 phrases)\n",
    "- Flag posts with `has_help_phrase`\n",
    "\n",
    "#### Step 2: NLP Phrase Discovery\n",
    "- Focus on posts with `has_help_phrase` or low VADER sentiment\n",
    "- Use:\n",
    "  - `FreqDist()` ‚Üí top unigrams\n",
    "  - `bigrams()` / `trigrams()` ‚Üí discover help phrases\n",
    "  - `common_contexts()` ‚Üí explore usage of ‚Äúhelp‚Äù, ‚Äústuck‚Äù, etc.\n",
    "\n",
    "#### Step 3: Refine & Expand\n",
    "- Build `help_keywords_v2`\n",
    "- (Optional) Train a basic classifier using labeled examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6ae49d",
   "metadata": {},
   "source": [
    "## Step 1: Baseline Keyword Match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6f7c92",
   "metadata": {},
   "source": [
    "### Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66189091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 2: Apply baseline help phrase match ===\n",
    "\n",
    "print(\"Merging title + selftext into Post_Text...\")\n",
    "df_posts['Post_Text'] = df_posts.apply(\n",
    "    lambda r: f\"{r['title']}\\n{r['selftext']}\".strip() if pd.notnull(r['selftext']) else r['title'].strip(),\n",
    "    axis=1\n",
    ")\n",
    "print(\"Sample Post_Text:\")\n",
    "print(df_posts['Post_Text'].head(3))\n",
    "\n",
    "print(\"\\nLowercasing for help phrase matching...\")\n",
    "df_posts['Post_Text_LC'] = df_posts['Post_Text'].str.lower()\n",
    "print(\"Sample Post_Text_LC:\")\n",
    "print(df_posts['Post_Text_LC'].head(3))\n",
    "\n",
    "print(\"\\nChecking for help phrases...\")\n",
    "df_posts['has_help_phrase'] = df_posts['Post_Text_LC'].apply(\n",
    "    lambda text: any(phrase in text for phrase in help_keywords)\n",
    ")\n",
    "print(\"Sample has_help_phrase values:\")\n",
    "print(df_posts['has_help_phrase'].head(3))\n",
    "\n",
    "print(\"\\nFiltering matched help-seeking posts...\")\n",
    "df_help_labeled = df_posts[df_posts['has_help_phrase']].copy()\n",
    "print(f\"Baseline help-seeking matches: {len(df_help_labeled)} / {len(df_posts)}\")\n",
    "\n",
    "print(\"Sample matched posts:\")\n",
    "display(df_help_labeled[['post_id', 'Post_Text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59dc1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Combine title + selftext\n",
    "df_help_labeled['Post_Text'] = df_help_labeled.apply(\n",
    "    lambda r: f\"{r['title']}\\n{r['selftext']}\".strip() if pd.notnull(r['selftext']) else r['title'].strip(),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Lowercase version for matching\n",
    "df_help_labeled['Post_Text_LC'] = df_help_labeled['Post_Text'].str.lower()\n",
    "\n",
    "# Find matched phrases\n",
    "df_help_labeled['matched_phrases'] = df_help_labeled['Post_Text_LC'].apply(\n",
    "    lambda text: [p for p in help_keywords_v2 if p in text]\n",
    ")\n",
    "\n",
    "# Extract ~30-character window around each match\n",
    "def extract_context_snippets(text, phrases, window=30):\n",
    "    text_lc = text.lower()\n",
    "    matches = []\n",
    "    for phrase in sorted(set(phrases), key=len, reverse=True):\n",
    "        for match in re.finditer(re.escape(phrase), text_lc):\n",
    "            start, end = match.span()\n",
    "            snippet = text[max(0, start-window):min(len(text), end+window)]\n",
    "            matches.append((start, snippet))\n",
    "    # Remove overlapping snippets\n",
    "    matches = sorted(matches, key=lambda x: x[0])\n",
    "    final_snippets = []\n",
    "    last_end = -1\n",
    "    for start, snippet in matches:\n",
    "        if start > last_end:\n",
    "            final_snippets.append(snippet.strip())\n",
    "            last_end = start + len(snippet)\n",
    "    return final_snippets\n",
    "\n",
    "# Extract snippets\n",
    "df_help_labeled['snippets'] = df_help_labeled.apply(\n",
    "    lambda row: extract_context_snippets(row['Post_Text'], row['matched_phrases']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Highlight help phrases\n",
    "def highlight_in_snippet(snippet, phrases):\n",
    "    for p in sorted(phrases, key=len, reverse=True):\n",
    "        pattern = re.compile(re.escape(p), re.IGNORECASE)\n",
    "        snippet = pattern.sub(f\"<mark>{p}</mark>\", snippet)\n",
    "    return snippet\n",
    "\n",
    "# Filter + explode\n",
    "df_snippet = df_help_labeled[df_help_labeled['snippets'].str.len() > 0].copy()\n",
    "df_snippet = df_snippet.explode('snippets')\n",
    "\n",
    "# Highlight in each snippet\n",
    "df_snippet['Context'] = df_snippet.apply(\n",
    "    lambda row: highlight_in_snippet(row['snippets'], row['matched_phrases']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Format display DataFrame\n",
    "df_snippet['Title'] = df_snippet['title'].apply(lambda x: truncate(x, 40))\n",
    "df_display = df_snippet[['post_id', 'Title', 'Context']].copy()\n",
    "\n",
    "# Render HTML (wider table)\n",
    "html_code = df_display.to_html(index=False, escape=False)\n",
    "display(HTML(f\"\"\"\n",
    "<div style='max-height: 700px; overflow-y: auto; border: 1px solid #ccc; padding: 10px; font-family: sans-serif'>\n",
    "<table style='width:100%; table-layout: fixed'>\n",
    "{html_code}\n",
    "</table>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542a0235",
   "metadata": {},
   "source": [
    "### Observation: Current help-seeking phrase detection works!\n",
    "Every result matched by `help_keywords` is a true help-seeking post ‚Äî no false positives observed.\n",
    "\n",
    "---\n",
    "\n",
    "### Next: Improve and Validate the List Using NLTK\n",
    "\n",
    "1. **Token Exploration**\n",
    "   - Run `FreqDist()` on `has_help_phrase == True` posts\n",
    "   - Find common unigrams related to help-seeking\n",
    "\n",
    "2. **Missed Signal Discovery**\n",
    "   - Run `FreqDist()` on `has_help_phrase == False AND vader_score < -0.2`\n",
    "   - Identify help-related language that isn‚Äôt in the current list\n",
    "\n",
    "3. **Phrase Expansion**\n",
    "   - Use `bigrams()` and `trigrams()` to find multi-word patterns\n",
    "   - Use `common_contexts()` to explore key word usage\n",
    "\n",
    "4. **Refine Help Phrase List**\n",
    "   - Build `help_keywords_v2`\n",
    "   - Optional: flag more posts or prep training data for a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a132d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 5: Token Exploration ‚Äî FreqDist on help-seeking posts ===\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Ensure NLTK punkt tokenizer is available\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Get only the help-seeking text\n",
    "help_texts = df_help_labeled['Post_Text'].dropna().tolist()\n",
    "all_help_tokens = []\n",
    "\n",
    "print(\"‚û°Ô∏è Tokenizing help-seeking posts...\")\n",
    "for text in help_texts:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    all_help_tokens.extend(tokens)\n",
    "\n",
    "# Generate frequency distribution\n",
    "fdist = FreqDist(all_help_tokens)\n",
    "\n",
    "# Show most common words\n",
    "print(\"‚úÖ Top 30 tokens in help-seeking posts:\")\n",
    "for word, count in fdist.most_common():\n",
    "    print(f\"{word:>12} : {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8bd79",
   "metadata": {},
   "source": [
    "### Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda7c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 6: Cleaned Token Exploration ‚Äî Stopwords Removed + Export ===\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Download stopwords if needed\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Define base stopwords and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "# Tokenize and clean\n",
    "clean_tokens = []\n",
    "for text in help_texts:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered = [t for t in tokens if t not in stop_words and t not in punct and len(t) > 2]\n",
    "    clean_tokens.extend(filtered)\n",
    "\n",
    "# Frequency distribution\n",
    "clean_fdist = FreqDist(clean_tokens)\n",
    "\n",
    "# Show top 30\n",
    "print(\"Top 30 cleaned tokens in help-seeking posts:\")\n",
    "for word, count in clean_fdist.most_common(30):\n",
    "    print(f\"{word:>12} : {count}\")\n",
    "\n",
    "# Save full freq list to CSV\n",
    "output_path = Path(\"/Users/buddy/Desktop/WGU-Reddit/notebooks/outputs/help_token_freq.csv\")\n",
    "freq_df = pd.DataFrame(clean_fdist.items(), columns=[\"token\", \"count\"]).sort_values(by=\"count\", ascending=False)\n",
    "freq_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Full token frequency saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd54ec",
   "metadata": {},
   "source": [
    "### Help-Seeking Tokens Identified\n",
    "`FreqDist()` revealed strong signals like: `help`, `need`, `advice`, `anyone`, `question`, `tips`, `thanks`, `stuck`.\n",
    "\n",
    "---\n",
    "\n",
    "### Noise Observed\n",
    "Default stopword removal missed low-signal terms:\n",
    "- WGU-specific: `wgu`, `course`, `class`, `degree`\n",
    "- Fillers: `like`, `get`, `know`, `really`, `also`\n",
    "\n",
    "---\n",
    "\n",
    "### What We Did\n",
    "- Manually reviewed top tokens\n",
    "- Removed generic terms\n",
    "- Finalized a list of **62 high-signal help-seeking words**\n",
    "\n",
    "Saved to: `/outputs/help_keywords_v2.csv`\n",
    "\n",
    "---\n",
    "\n",
    "### Sample from `help_keywords_v2`:\n",
    "`help`, `advice`, `anyone`, `need`, `tips`, `questions`, `looking`, `struggling`, `please`, `appreciated`\n",
    "\n",
    "### Next Steps \n",
    "\n",
    "1. **Apply `help_keywords_v2`**\n",
    "   - Load CSV\n",
    "   - Flag posts with `has_help_v2`\n",
    "\n",
    "2. **Compare Old vs New**\n",
    "   - Count posts:\n",
    "     - `has_help_phrase`\n",
    "     - `has_help_v2`\n",
    "     - in `v2` only\n",
    "     - still undetected\n",
    "\n",
    "3. **Analyze Misses**\n",
    "   - Focus: `has_help_v2 == False` and `vader_score < -0.2`\n",
    "   - Run `FreqDist()` + `bigrams()` on this slice\n",
    "\n",
    "4. **Prepare for Classifier**\n",
    "   - Create labeled dataset with:\n",
    "     - `post_id`, `Post_Text`, `has_help_v2`, `vader_score`\n",
    "   - Save as `help_labeled_dataset.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca155be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 1: Apply help_keywords_v2 ===\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(v2_path, \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    help_keywords_v2 = [row[0].strip().lower() for row in reader if row]\n",
    "\n",
    "help_keywords_v2.append(\"?\")  # added manually for help-question detection\n",
    "# Load refined keyword list (lowercase)\n",
    "v2_path = PROJECT_ROOT / \"WGU_catalog\" / \"outputs\" / \"help_keywords_v2.csv\"\n",
    "with open(v2_path, \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    help_keywords_v2 = [row[0].strip().lower() for row in reader if row]\n",
    "\n",
    "# Flag posts containing any v2 keyword\n",
    "df_posts[\"has_help_v2\"] = df_posts[\"Post_Text_LC\"].apply(\n",
    "    lambda text: any(kw in text for kw in help_keywords_v2)\n",
    ")\n",
    "\n",
    "# Preview results\n",
    "print(f\"‚úÖ help_keywords_v2 applied. Matches found: {df_posts['has_help_v2'].sum()}\")\n",
    "display(df_posts[df_posts[\"has_help_v2\"]].head(10)[[\"post_id\", \"Post_Text\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d789c817",
   "metadata": {},
   "source": [
    "## Compare first and second list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b7d507",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e8165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 2: Compare has_help_phrase vs has_help_v2 ===\n",
    "\n",
    "# Basic counts\n",
    "count_phrase = df_posts[\"has_help_phrase\"].sum()\n",
    "count_v2 = df_posts[\"has_help_v2\"].sum()\n",
    "\n",
    "# Posts newly flagged by v2\n",
    "v2_only = df_posts[(df_posts[\"has_help_v2\"]) & (~df_posts[\"has_help_phrase\"])]\n",
    "missed_by_both = df_posts[(~df_posts[\"has_help_v2\"]) & (~df_posts[\"has_help_phrase\"])]\n",
    "\n",
    "print(\"Help Phrase Comparison:\")\n",
    "print(f\"- Old (has_help_phrase): {count_phrase}\")\n",
    "print(f\"- New (has_help_v2): {count_v2}\")\n",
    "print(f\"- Detected by v2 only: {len(v2_only)}\")\n",
    "print(f\"- Missed by both: {len(missed_by_both)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb234f",
   "metadata": {},
   "source": [
    "### Help Phrase Comparison ‚Äì Observation\n",
    "\n",
    "The refined keyword list (`help_keywords_v2`) flagged 15,244 posts, up from 13,551 using the original list ‚Äî a significant increase in detected help-seeking behavior.\n",
    "\n",
    "Over 3,300 posts were uniquely caught by v2, highlighting the expanded coverage.\n",
    "\n",
    "However, this assumes all matches are valid ‚Äî in reality, some may be false positives. A manual sample review is needed to verify precision.\n",
    "\n",
    "Meanwhile, ~1,900 posts with low sentiment were missed entirely, suggesting additional help signals may still be uncovered.\n",
    "\n",
    "**Help Phrase Comparison:**\n",
    "- Old (`has_help_phrase`): 13,551  \n",
    "- New (`has_help_v2`): 15,244  \n",
    "- Detected by v2 only: 3,302  \n",
    "- Missed by both: 1,896"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf912e",
   "metadata": {},
   "source": [
    "### Phrase Expansion: Bigrams, Trigrams, and Contexts from `v2_only` Posts\n",
    "\n",
    "To uncover additional help-seeking patterns beyond keywords, we analyze `v2_only` posts using NLTK tools:\n",
    "\n",
    "1. **Tokenize & Clean Text**  \n",
    "   Normalize case and remove punctuation (not '?') for consistent phrase extraction.\n",
    "\n",
    "2. **Extract Bigrams and Trigrams**  \n",
    "   Identify frequently occurring 2‚Äì3 word phrases that suggest help-seeking intent (e.g. \"need advice\", \"anyone know how\").\n",
    "\n",
    "3. **Explore Common Contexts**  \n",
    "   Use `common_contexts()` to see how key terms like \"help\", \"tips\", and \"advice\" are used in surrounding text.\n",
    "\n",
    "These steps help surface real-world language patterns to refine detection beyond manual phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb1b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 3: Phrase Expansion from v2_only posts ===\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import bigrams, trigrams\n",
    "from nltk.text import Text\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    # Keep '?', remove all other punctuation\n",
    "    punct = string.punctuation.replace('?', '')\n",
    "    text = text.lower().translate(str.maketrans('', '', punct))\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Tokenize v2-only posts\n",
    "v2_only_tokens = v2_only[\"Post_Text\"].dropna().apply(clean_and_tokenize)\n",
    "\n",
    "# Flatten tokens\n",
    "flat_tokens = [token for tokens in v2_only_tokens for token in tokens]\n",
    "\n",
    "# === Bigrams ===\n",
    "bi = FreqDist(bigram for tokens in v2_only_tokens for bigram in bigrams(tokens))\n",
    "print(\"\\nTop 30 bigrams in v2-only posts:\")\n",
    "for phrase, count in bi.most_common(30):\n",
    "    print(f\"{phrase[0]} {phrase[1]}: {count}\")\n",
    "\n",
    "# === Trigrams ===\n",
    "tri = FreqDist(trigram for tokens in v2_only_tokens for trigram in trigrams(tokens))\n",
    "print(\"\\nTop 30 trigrams in v2-only posts:\")\n",
    "for phrase, count in tri.most_common(30):\n",
    "    print(f\"{phrase[0]} {phrase[1]} {phrase[2]}: {count}\")\n",
    "\n",
    "# === Common Contexts ===\n",
    "# Use flat tokens to build a Text object\n",
    "text_obj = Text(flat_tokens)\n",
    "\n",
    "# Explore how \"help\", \"tips\", \"advice\" appear in context\n",
    "print(\"\\nCommon contexts for 'help':\")\n",
    "text_obj.common_contexts([\"help\"])\n",
    "\n",
    "print(\"\\nCommon contexts for 'tips':\")\n",
    "text_obj.common_contexts([\"tips\"])\n",
    "\n",
    "print(\"\\nCommon contexts for 'advice':\")\n",
    "text_obj.common_contexts([\"advice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6e943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# add a column for VADER\n",
    "if \"VADER\" not in df_posts.columns:\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    df_posts[\"VADER\"] = df_posts[\"Post_Text\"].apply(\n",
    "        lambda t: analyzer.polarity_scores(t)[\"compound\"]\n",
    "    )\n",
    "    print(\"‚úÖ VADER sentiment scores added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8364b7",
   "metadata": {},
   "source": [
    "## Identify Sentiment Ranges with Help-Seeking Signals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68da05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pd.set_option(\"display.max_colwidth\", 500)\n",
    "\n",
    "# === Histogram: All Posts ===\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(df_posts[\"VADER\"], bins=40, edgecolor='black')\n",
    "plt.title(\"VADER Sentiment ‚Äî All Posts\")\n",
    "plt.xlabel(\"VADER Score\")\n",
    "plt.ylabel(\"Post Count\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# === Histogram: Help-Seeking Posts Only ===\n",
    "help_mask = df_posts[\"has_help_phrase\"] | df_posts[\"has_help_v2\"]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(df_posts[help_mask][\"VADER\"], bins=40, edgecolor='black', color='green')\n",
    "plt.title(\"VADER Sentiment ‚Äî Help-Seeking Posts\")\n",
    "plt.xlabel(\"VADER Score\")\n",
    "plt.ylabel(\"Help-Seeking Post Count\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# === Histogram: Missed Posts ===\n",
    "missed_mask = (~df_posts[\"has_help_phrase\"]) & (~df_posts[\"has_help_v2\"])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(df_posts[missed_mask][\"VADER\"], bins=40, edgecolor='black', color='red')\n",
    "plt.title(\"VADER Sentiment ‚Äî Missed Posts (Unflagged)\")\n",
    "plt.xlabel(\"VADER Score\")\n",
    "plt.ylabel(\"Missed Post Count\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dc6c53",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "A sharp peak in the -0.5 to -0.4 range suggests many distress-based help posts. The large spike at 0.0 likely reflects noise. Consistent volume in the 0.3 to 0.9 range points to advice, gratitude, or resolved help ‚Äî still relevant for identifying help-seeking behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e3b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define 0.1 bins from -1.0 to +1.0\n",
    "bins = np.arange(-1.0, 1.1, 0.1)\n",
    "\n",
    "# === Histogram: All Posts ===\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(df_posts[\"VADER\"], bins=bins, edgecolor='black')\n",
    "plt.title(\"VADER Sentiment ‚Äî All Posts (0.1 Bins)\")\n",
    "plt.xlabel(\"VADER Score\")\n",
    "plt.ylabel(\"Post Count\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# === Histogram: Original Keyword Matches ===\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(df_posts[df_posts[\"has_help_phrase\"]][\"VADER\"], bins=bins, edgecolor='black', color='orange')\n",
    "plt.title(\"VADER Sentiment ‚Äî has_help_phrase (Original Keywords)\")\n",
    "plt.xlabel(\"VADER Score\")\n",
    "plt.ylabel(\"Help-Seeking Post Count\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# === Histogram: Expanded Keyword Matches ===\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(df_posts[df_posts[\"has_help_v2\"]][\"VADER\"], bins=bins, edgecolor='black', color='green')\n",
    "plt.title(\"VADER Sentiment ‚Äî has_help_v2 (Expanded Keywords)\")\n",
    "plt.xlabel(\"VADER Score\")\n",
    "plt.ylabel(\"Help-Seeking Post Count\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed3a1ca",
   "metadata": {},
   "source": [
    "### Sentiment Histogram Conclusion\n",
    "\n",
    "Sentiment distributions for all posts, original, and v2 help-seeking are similar. The v2 list captures more posts, especially with low sentiment, but doesn't shift the overall curve.\n",
    "\n",
    "**Conclusion:** No clear sentiment range isolates missed help-seeking. Future improvements should focus on language patterns, not sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8373a0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 3: Analyze Misses (VADER < -0.2 but no help keywords matched) ===\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import bigrams\n",
    "import string\n",
    "\n",
    "# Filter missed posts with negative sentiment\n",
    "NEG_THRESHOLD = -0.2\n",
    "missed_neg = df_posts[(~df_posts[\"has_help_v2\"]) & \n",
    "                      (~df_posts[\"has_help_phrase\"]) &\n",
    "                      (df_posts[\"VADER\"] < NEG_THRESHOLD)].copy()\n",
    "\n",
    "print(f\"Posts missed by both help detectors but VADER < {NEG_THRESHOLD}: {len(missed_neg)}\")\n",
    "\n",
    "# Lowercase + remove punctuation\n",
    "def clean_and_tokenize(text):\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Tokenize all missed posts\n",
    "missed_tokens = missed_neg[\"Post_Text\"].apply(clean_and_tokenize).explode()\n",
    "\n",
    "# FreqDist of individual tokens\n",
    "fdist = FreqDist(missed_tokens)\n",
    "print(\"\\nTop 30 tokens in missed posts:\")\n",
    "print(fdist.most_common(30))\n",
    "\n",
    "# Get bigrams\n",
    "missed_bigrams = missed_neg[\"Post_Text\"].apply(\n",
    "    lambda text: list(bigrams(clean_and_tokenize(text)))\n",
    ").explode()\n",
    "\n",
    "bigram_dist = FreqDist(missed_bigrams)\n",
    "print(\"\\nTop 30 bigrams in missed posts:\")\n",
    "for bigram, count in bigram_dist.most_common(30):\n",
    "    print(f\"{bigram[0]} {bigram[1]}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6e59b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35b47130",
   "metadata": {},
   "source": [
    "## Top 20 course code match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccda76e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many posts mention any Top 20 course code\n",
    "top20_list = df_top20['Course Code'].str.upper().unique().tolist()\n",
    "\n",
    "literal_counts = post_texts.str.upper().apply(\n",
    "    lambda t: any(code in t for code in top20_list)\n",
    ").sum()\n",
    "\n",
    "print(f\"‚úÖ Posts mentioning a Top 20 course: {literal_counts} / {len(df_posts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c7785f",
   "metadata": {},
   "source": [
    "## VADER on preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59670d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "sample_scores = post_texts.head(5).apply(lambda t: analyzer.polarity_scores(t)['compound'])\n",
    "\n",
    "print(\"‚úÖ VADER compound scores (sample):\")\n",
    "print(sample_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b7b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107892f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d675b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0764c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
