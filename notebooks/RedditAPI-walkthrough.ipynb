{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "887f0ce7-2937-43e1-bae9-19ef4de06b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to the database established.\n"
     ]
    }
   ],
   "source": [
    "# notebook_setup.py\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"/Users/buddy/Desktop/WGU-Reddit\")\n",
    "db_path = PROJECT_ROOT / \"db\" / \"WGU-Reddit.db\"\n",
    "\n",
    "try:\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    conn.execute(\"SELECT 1;\")\n",
    "    print(\"Connection to the database established.\")\n",
    "except sqlite3.Error as e:\n",
    "    print(f\"Failed to connect: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84598190-782b-4928-8630-fcfffdcbc488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db/WGU-Reddit.db\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "      <th>Row_Count</th>\n",
       "      <th>Columns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subreddits</td>\n",
       "      <td>51</td>\n",
       "      <td>[subreddit_id, name, description, is_nsfw, cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subreddit_stats</td>\n",
       "      <td>2637</td>\n",
       "      <td>[subreddit_id, captured_at, subscriber_count, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>posts</td>\n",
       "      <td>18907</td>\n",
       "      <td>[post_id, subreddit_id, username, title, selft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comments</td>\n",
       "      <td>84736</td>\n",
       "      <td>[comment_id, post_id, username, parent_comment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>users</td>\n",
       "      <td>19814</td>\n",
       "      <td>[username, karma_comment, karma_post, created_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>user_stats</td>\n",
       "      <td>14773</td>\n",
       "      <td>[username, captured_at, karma_post, karma_comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>posts_keyword</td>\n",
       "      <td>3952</td>\n",
       "      <td>[post_id, subreddit_id, username, title, selft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>comments_keyword</td>\n",
       "      <td>0</td>\n",
       "      <td>[comment_id, post_id, subreddit_id, username, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Table  Row_Count  \\\n",
       "0        subreddits         51   \n",
       "1   subreddit_stats       2637   \n",
       "2             posts      18907   \n",
       "3          comments      84736   \n",
       "4             users      19814   \n",
       "5        user_stats      14773   \n",
       "6     posts_keyword       3952   \n",
       "7  comments_keyword          0   \n",
       "\n",
       "                                             Columns  \n",
       "0  [subreddit_id, name, description, is_nsfw, cre...  \n",
       "1  [subreddit_id, captured_at, subscriber_count, ...  \n",
       "2  [post_id, subreddit_id, username, title, selft...  \n",
       "3  [comment_id, post_id, username, parent_comment...  \n",
       "4  [username, karma_comment, karma_post, created_...  \n",
       "5  [username, captured_at, karma_post, karma_comm...  \n",
       "6  [post_id, subreddit_id, username, title, selft...  \n",
       "7  [comment_id, post_id, subreddit_id, username, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TABLES = [\n",
    "    \"subreddits\",\n",
    "    \"subreddit_stats\",\n",
    "    \"posts\",\n",
    "    \"comments\",\n",
    "    \"users\",\n",
    "    \"user_stats\",\n",
    "    \"posts_keyword\",\n",
    "    \"comments_keyword\"\n",
    "]\n",
    "\n",
    "row_counts = []\n",
    "\n",
    "for table in TABLES:\n",
    "    count = pd.read_sql_query(f\"SELECT COUNT(*) as count FROM {table};\", conn)['count'].iloc[0]\n",
    "    columns = pd.read_sql_query(f\"SELECT * FROM {table} LIMIT 1;\", conn).columns.tolist()\n",
    "    row_counts.append({\n",
    "        \"Table\": table,\n",
    "        \"Row_Count\": count,\n",
    "        \"Columns\": columns\n",
    "    })\n",
    "\n",
    "table_counts_df = pd.DataFrame(row_counts)\n",
    "print(\"db/WGU-Reddit.db\")\n",
    "display(table_counts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1ee249-3549-4f53-98fd-3987d5cd6958",
   "metadata": {},
   "source": [
    "### Topic 1 — Database design: To-do - remove unused parts\n",
    "\n",
    "0. `subreddits` — 51 WGU-related subreddits. Static, not updated. \n",
    "1. `subreddit_stats` — Tracks subreddit popularity over time (subs, active users). **Likely DROP** \n",
    "2. `posts` — Main table for top-level submissions from daily subreddit fetch.   \n",
    "3. `comments` — fetcher gets top 3 comments per post + direct replies (max depth 2). Not currently analyzed. \n",
    "4. `users` — User metadata for anyone posting in WGU subs. Not used in current pipeline. **Possible DROP**.  \n",
    "5. `user_stats` — Historical user karma snapshots. Not used. **Likely DROP**.  \n",
    "6. `posts_keyword` — **_keyword** tables have extra posta found by explicit searches, segregated to not skew proportions\n",
    "7. `comments_keyword` — not yet used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bba4de1-bdf3-44d5-bdd9-e822ab24781e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subreddits: 51\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Subscribers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WGU</td>\n",
       "      <td>152,082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WGU_CompSci</td>\n",
       "      <td>23,788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WGUCyberSecurity</td>\n",
       "      <td>21,661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WGUIT</td>\n",
       "      <td>17,756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wguaccounting</td>\n",
       "      <td>10,392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wgu_devs</td>\n",
       "      <td>9,665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WGU_MBA</td>\n",
       "      <td>8,074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wgueducation</td>\n",
       "      <td>6,945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WGU_Military</td>\n",
       "      <td>5,802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WGU_Accelerators</td>\n",
       "      <td>4,228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Subreddit Subscribers\n",
       "0               WGU     152,082\n",
       "1       WGU_CompSci      23,788\n",
       "2  WGUCyberSecurity      21,661\n",
       "3             WGUIT      17,756\n",
       "4     wguaccounting      10,392\n",
       "5          wgu_devs       9,665\n",
       "6           WGU_MBA       8,074\n",
       "7      wgueducation       6,945\n",
       "8      WGU_Military       5,802\n",
       "9  WGU_Accelerators       4,228"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show Subreddits\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT sr.name AS Subreddit, MAX(ss.subscriber_count) AS Subscribers\n",
    "FROM subreddit_stats ss\n",
    "JOIN subreddits sr ON ss.subreddit_id = sr.subreddit_id\n",
    "GROUP BY sr.name\n",
    "ORDER BY Subscribers DESC;\n",
    "\"\"\"\n",
    "\n",
    "subs_df = pd.read_sql_query(query, conn)\n",
    "subs_df[\"Subscribers\"] = subs_df[\"Subscribers\"].map(\"{:,}\".format)\n",
    "\n",
    "print(f\"Total subreddits: {len(subs_df)}\")\n",
    "display(subs_df.head(10))\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738e7b33-afd1-4c1d-af69-b97c0812eab6",
   "metadata": {},
   "source": [
    "# Reddit Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7df420a-8716-44d8-9ea1-a14be230ecfb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd302596-70ca-4742-9d42-5e7b6455a48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 472 posts from the last 7 days.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Created_UTC</th>\n",
       "      <th>Post Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1m15sqq</td>\n",
       "      <td>D335</td>\n",
       "      <td>Has any been able to pass the OA by just pract...</td>\n",
       "      <td>WGU</td>\n",
       "      <td>1752649026</td>\n",
       "      <td>10 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1m155ub</td>\n",
       "      <td>One Term? BSITM</td>\n",
       "      <td>Hey there! I've spent the last few months crun...</td>\n",
       "      <td>WGU_Accelerators</td>\n",
       "      <td>1752646587</td>\n",
       "      <td>11 hours ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1m150xs</td>\n",
       "      <td>D281 Linux Foundations Question</td>\n",
       "      <td>Hi guys, please help. First time taking the ex...</td>\n",
       "      <td>WGU</td>\n",
       "      <td>1752646071</td>\n",
       "      <td>11 hours ago</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Post ID                            Title  \\\n",
       "0  1m15sqq                             D335   \n",
       "1  1m155ub                  One Term? BSITM   \n",
       "2  1m150xs  D281 Linux Foundations Question   \n",
       "\n",
       "                                                Body         Subreddit  \\\n",
       "0  Has any been able to pass the OA by just pract...               WGU   \n",
       "1  Hey there! I've spent the last few months crun...  WGU_Accelerators   \n",
       "2  Hi guys, please help. First time taking the ex...               WGU   \n",
       "\n",
       "   Created_UTC      Post Age  \n",
       "0   1752649026  10 hours ago  \n",
       "1   1752646587  11 hours ago  \n",
       "2   1752646071  11 hours ago  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# recent_posts.py\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def get_db_connection():\n",
    "    return sqlite3.connect(\"../db/WGU-Reddit.db\")\n",
    "\n",
    "# Connect\n",
    "conn = get_db_connection()\n",
    "\n",
    "# Query: recent posts (last 7 days)\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    p.post_id,\n",
    "    p.title,\n",
    "    p.selftext,\n",
    "    s.name AS subreddit_name,\n",
    "    p.created_utc\n",
    "FROM posts p\n",
    "LEFT JOIN subreddits s ON p.subreddit_id = s.subreddit_id\n",
    "WHERE p.created_utc >= strftime('%s', 'now', '-7 days')\n",
    "ORDER BY p.created_utc DESC\n",
    "\"\"\"\n",
    "\n",
    "df_posts_7d = pd.read_sql(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# Add human-readable age (but keep created_utc!)\n",
    "now = datetime.now(timezone.utc)\n",
    "df_posts_7d['post_age'] = df_posts_7d['created_utc'].apply(\n",
    "    lambda x: pd.Timedelta(now - datetime.fromtimestamp(x, tz=timezone.utc))\n",
    ").apply(\n",
    "    lambda td: f\"{td.days} days ago\" if td.days >= 1 else f\"{int(td.seconds / 3600)} hours ago\"\n",
    ")\n",
    "\n",
    "df_posts_7d = df_posts_7d.rename(columns={\n",
    "    'post_id': 'Post ID',\n",
    "    'title': 'Title',\n",
    "    'selftext': 'Body',\n",
    "    'subreddit_name': 'Subreddit',\n",
    "    'created_utc': 'Created_UTC',\n",
    "    'post_age': 'Post Age'\n",
    "})\n",
    "\n",
    "print(f\"Loaded {len(df_posts_7d)} posts from the last 7 days.\")\n",
    "display(df_posts_7d.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff949a3-55f2-4048-9008-22c8159721e7",
   "metadata": {},
   "source": [
    "## Search Posts for Course Mentions\n",
    "### Step 1: Load master course list:\n",
    "\"courses_with_college_v10.csv\" was created by scraping the WGU Institutional Catalogs 2017-1 thru 2025-6 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a77f82-de85-4978-81e3-bd8f99f75e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01_load_course_codes.py\n",
    "\n",
    "COURSE_CODES_PATH = PROJECT_ROOT / \"WGU_catalog\" / \"outputs\" / \"courses_with_college_v10.csv\"\n",
    "\n",
    "course_codes_df = pd.read_csv(COURSE_CODES_PATH)\n",
    "course_codes_df['CourseCode'] = course_codes_df['CourseCode'].str.upper()\n",
    "\n",
    "valid_course_codes = set(course_codes_df['CourseCode'].unique())\n",
    "\n",
    "print(f\"{COURSE_CODES_PATH} — Loaded {len(valid_course_codes)} unique course codes.\")\n",
    "display(course_codes_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae38bf79-e89d-4daf-a995-c83842863920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ae93c5c-cc8f-4f31-8941-7eb997aad627",
   "metadata": {},
   "source": [
    "## 🔍 Inspect Multi-Course Posts\n",
    "\n",
    "Some posts mention **multiple course codes** — these might be **degree planning**, **scheduling**, or **general path questions**, rather than detailed feedback on a single course.  \n",
    "This could dilute course-level sentiment signals.\n",
    "\n",
    "**Next step:**  \n",
    "- Add a `Num_Courses` column.  \n",
    "- Sort by number of courses mentioned.  \n",
    "- Inspect top examples to decide if they should be flagged or filtered for certain analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac574b-1add-4e96-959b-302b91461d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Count how many courses mentioned\n",
    "df_courses_7d['Num_Courses'] = df_courses_7d['Course Codes'].apply(len)\n",
    "df_courses_7d['Is_MultiCourse'] = df_courses_7d['Num_Courses'] > 1\n",
    "\n",
    "# ✅ Inspect multi-course posts sorted by number of courses\n",
    "df_multicourse_7d = df_courses_7d[df_courses_7d['Is_MultiCourse']].sort_values('Num_Courses', ascending=False)\n",
    "\n",
    "print(f\"Multi-course posts found: {len(df_multicourse_7d)}\")\n",
    "display(df_multicourse_7d[['Post ID', 'Title', 'Body', 'Course Codes', 'Num_Courses']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2b288e-ae8e-4e1f-848b-bf6e5c39c2ff",
   "metadata": {},
   "source": [
    "## Multi-Course Mentions — Conclusion & Plan\n",
    "\n",
    "- Posts that mention **many courses (4+)** are mostly **degree planning**, sequencing, or general workload questions.\n",
    "- These tend to be longer but do **not contain detailed feedback about each course**.\n",
    "- Keeping them in **course-level sentiment** can dilute the signal — they add noise when analyzing individual course experiences.\n",
    "\n",
    "**Plan**\n",
    "- **Tag** posts with `Num_Courses >= 4` as `Is_Planning = True`.\n",
    "- For **course-level sentiment**, filter out `Is_Planning = True`.\n",
    "- Use these planning posts separately to analyze:\n",
    "  - Overall workload stress\n",
    "  - Program pacing questions\n",
    "  - Common course sequences or bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3057a434-b706-41bc-be5a-0b12ae9b9d4b",
   "metadata": {},
   "source": [
    "## ✅ `combined_posts_top20_with_sentiment.csv`\n",
    "\n",
    "- Posts from last 90 days (`posts` + `posts_keyword`), deduped.\n",
    "- Only posts mentioning **Top 20** courses.\n",
    "- Posts with **4+ courses** skipped (planning noise).\n",
    "- **Exploded** → each row = 1 post × 1 course.\n",
    "- Includes: `post_id`, text, `source`, `CourseCode`, `Num_Courses`, `VADER_Compound`.\n",
    "\n",
    "Use for clean course-level sentiment, topic, and trend plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b4d5729-3e48-4575-a568-90af8e4f8b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/buddy/Desktop/WGU-Reddit/data/output/reddit_top_20_mentioned_courses.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m TOP20_COURSES_CSV = PROJECT_ROOT / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mreddit_top_20_mentioned_courses.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Load top 20 course codes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m df_top20 = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTOP20_COURSES_CSV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m top20_courses = \u001b[38;5;28mset\u001b[39m(df_top20[\u001b[33m'\u001b[39m\u001b[33mCourse Code\u001b[39m\u001b[33m'\u001b[39m].unique())\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Connect\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/WGU-Reddit/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/WGU-Reddit/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/WGU-Reddit/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/WGU-Reddit/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/WGU-Reddit/.venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/buddy/Desktop/WGU-Reddit/data/output/reddit_top_20_mentioned_courses.csv'"
     ]
    }
   ],
   "source": [
    "# === CELL 1: Combine posts and filter ===\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === CONFIG ===\n",
    "PROJECT_ROOT = Path(\"/Users/buddy/Desktop/WGU-Reddit\")\n",
    "DB_PATH = PROJECT_ROOT / \"db\" / \"WGU-Reddit.db\"\n",
    "TOP20_COURSES_CSV = PROJECT_ROOT / \"data\" / \"output\" / \"reddit_top_20_mentioned_courses.csv\"\n",
    "\n",
    "# Load top 20 course codes\n",
    "df_top20 = pd.read_csv(TOP20_COURSES_CSV)\n",
    "top20_courses = set(df_top20['Course Code'].unique())\n",
    "\n",
    "# Connect\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Organic posts\n",
    "df_organic = pd.read_sql_query(\"\"\"\n",
    "    SELECT post_id, title, selftext, created_utc\n",
    "    FROM posts\n",
    "    WHERE created_utc >= strftime('%s', 'now', '-90 days')\n",
    "\"\"\", conn)\n",
    "df_organic['source'] = 'organic'\n",
    "\n",
    "# Keyword posts\n",
    "df_keyword = pd.read_sql_query(\"\"\"\n",
    "    SELECT post_id, title, selftext, created_utc, search_terms\n",
    "    FROM posts_keyword\n",
    "    WHERE created_utc >= strftime('%s', 'now', '-90 days')\n",
    "\"\"\", conn)\n",
    "df_keyword['source'] = 'keyword'\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Combine & dedupe\n",
    "df_combined = pd.concat([df_organic, df_keyword], ignore_index=True)\n",
    "df_combined = df_combined.sort_values('source')  # keyword first\n",
    "df_combined = df_combined.drop_duplicates(subset=['post_id'], keep='first')\n",
    "\n",
    "# Extract courses\n",
    "def extract_top20(row):\n",
    "    combined_text = f\"{row['title']} {row['selftext']}\".upper().split()\n",
    "    return [word for word in combined_text if word in top20_courses]\n",
    "\n",
    "df_combined['Course Codes'] = df_combined.apply(extract_top20, axis=1)\n",
    "df_combined['Num_Courses'] = df_combined['Course Codes'].apply(len)\n",
    "\n",
    "# Filter: 1-3 top20 courses\n",
    "df_filtered = df_combined[\n",
    "    (df_combined['Num_Courses'] > 0) & (df_combined['Num_Courses'] < 4)\n",
    "].copy()\n",
    "\n",
    "# Explode\n",
    "df_filtered = df_filtered.explode('Course Codes')\n",
    "df_filtered = df_filtered.rename(columns={'Course Codes': 'CourseCode'})\n",
    "\n",
    "print(df_filtered.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "883af34a-5058-4742-98eb-0f44c466064e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ✅ SENTIMENT STEP DONE ===\n",
      "🔢 Total posts scored: 590\n",
      "\n",
      "📋 Sample rows with sentiment:\n",
      "      post_id CourseCode  VADER_Compound\n",
      "7275  1lvp63k       D278         -0.6249\n",
      "6910  1k6vz4e       C211          0.6745\n",
      "6910  1k6vz4e       C211          0.6745\n",
      "6912  1kkff8d       C211          0.0000\n",
      "6912  1kkff8d       C211          0.0000\n"
     ]
    }
   ],
   "source": [
    "# === CELL 2: Calculate sentiment ===\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Use df_filtered from previous cell\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_vader_compound(row):\n",
    "    text = f\"{row['title']} {row['selftext']}\"\n",
    "    return analyzer.polarity_scores(text)['compound']\n",
    "\n",
    "df_filtered['VADER_Compound'] = df_filtered.apply(get_vader_compound, axis=1)\n",
    "\n",
    "print(\"\\n=== ✅ SENTIMENT STEP DONE ===\")\n",
    "print(f\"🔢 Total posts scored: {len(df_filtered)}\")\n",
    "print(\"\\n📋 Sample rows with sentiment:\")\n",
    "print(df_filtered[['post_id', 'CourseCode', 'VADER_Compound']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "448afe7d-2123-4e56-a72f-36212a10aecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 🚩 COURSES BY % HIGHLY NEGATIVE POSTS (< -0.3) ===\n",
      "CourseCode  Num_Highly_Negative  Total_Posts  Pct_Highly_Negative\n",
      "      D197                    3            8                 37.5\n",
      "      D288                    7           20                 35.0\n",
      "      C777                   14           42                 33.3\n",
      "      C211                   11           40                 27.5\n",
      "      C949                    3           14                 21.4\n",
      "      C213                    5           24                 20.8\n",
      "      D287                    1            5                 20.0\n",
      "      D427                   16           80                 20.0\n",
      "      D335                   11           56                 19.6\n",
      "      C214                    6           32                 18.8\n",
      "      D336                    3           17                 17.6\n",
      "      D426                    7           41                 17.1\n",
      "      C215                    4           24                 16.7\n",
      "      D315                    2           14                 14.3\n",
      "      D487                    3           26                 11.5\n",
      "      C207                    3           27                 11.1\n",
      "      D316                    3           28                 10.7\n",
      "      D282                    1           10                 10.0\n",
      "      D333                    4           46                  8.7\n",
      "      D278                    3           36                  8.3\n"
     ]
    }
   ],
   "source": [
    "# === CELL 4: % highly negative posts by course ===\n",
    "\n",
    "# Use df_filtered with VADER_Compound\n",
    "NEG_THRESHOLD = -0.3  # your meaningful threshold\n",
    "\n",
    "# Flag highly negative posts\n",
    "df_filtered['Is_Highly_Negative'] = df_filtered['VADER_Compound'] < NEG_THRESHOLD\n",
    "\n",
    "# Count per course\n",
    "neg_stats = (\n",
    "    df_filtered\n",
    "    .groupby('CourseCode')['Is_Highly_Negative']\n",
    "    .agg(['sum', 'count'])\n",
    "    .reset_index()\n",
    "    .rename(columns={'sum': 'Num_Highly_Negative', 'count': 'Total_Posts'})\n",
    ")\n",
    "\n",
    "# % negative\n",
    "neg_stats['Pct_Highly_Negative'] = 100 * neg_stats['Num_Highly_Negative'] / neg_stats['Total_Posts']\n",
    "\n",
    "# Sort by %\n",
    "neg_stats = neg_stats.sort_values('Pct_Highly_Negative', ascending=False)\n",
    "\n",
    "print(\"\\n=== 🚩 COURSES BY % HIGHLY NEGATIVE POSTS (< -0.3) ===\")\n",
    "print(neg_stats.to_string(index=False, float_format=\"%.1f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e049d-e202-4814-8ee7-30145731ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch comments by course\n",
    "course_code = \"\"\n",
    "# show post IDs for the highly negative posts for the course\n",
    "#(later) fetch comments for post IDs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d5c888-8447-46a8-a9ac-218203191059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd9e33b-8fce-4352-952f-5ffa3afd5b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "from IPython.display import display\n",
    "\n",
    "# === 1️⃣ Filter highly negative ===\n",
    "neg_df = df_filtered[df_filtered['Is_Highly_Negative']].copy()\n",
    "\n",
    "# === 2️⃣ Add post age ===\n",
    "now = datetime.now(timezone.utc)\n",
    "neg_df['Post_Age'] = neg_df['created_utc'].apply(\n",
    "    lambda x: pd.Timedelta(now - datetime.fromtimestamp(x, tz=timezone.utc))\n",
    ").apply(\n",
    "    lambda td: f\"{td.days} days ago\" if td.days >= 1 else f\"{int(td.seconds / 3600)} hours ago\"\n",
    ")\n",
    "\n",
    "# === 3️⃣ Load course catalog for College info ===\n",
    "catalog_path = PROJECT_ROOT / \"WGU_catalog\" / \"outputs\" / \"courses_with_college_v10.csv\"\n",
    "course_catalog = pd.read_csv(catalog_path)\n",
    "course_catalog['CourseCode'] = course_catalog['CourseCode'].str.upper()\n",
    "\n",
    "course_college_map = (\n",
    "    course_catalog[['CourseCode', 'Colleges']]\n",
    "    .drop_duplicates()\n",
    "    .rename(columns={'Colleges': 'College'})\n",
    ")\n",
    "\n",
    "# Merge on CourseCode\n",
    "neg_df = neg_df.merge(\n",
    "    course_college_map,\n",
    "    on='CourseCode',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# === 4️⃣ Normalize College names ===\n",
    "def simplify_college(raw_college):\n",
    "    if pd.isna(raw_college):\n",
    "        return \"Other\"\n",
    "    raw = raw_college.lower()\n",
    "    if \"business\" in raw:\n",
    "        return \"Business\"\n",
    "    elif \"health\" in raw or \"leavitt\" in raw:\n",
    "        return \"Health\"\n",
    "    elif \"technology\" in raw:\n",
    "        return \"Technology\"\n",
    "    elif \"teachers\" in raw or \"education\" in raw:\n",
    "        return \"Education\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "neg_df['College_Normalized'] = neg_df['College'].apply(simplify_college)\n",
    "\n",
    "# === 5️⃣ Show final view ===\n",
    "cols = [\n",
    "    'post_id',\n",
    "    'CourseCode',\n",
    "    'College',\n",
    "    'College_Normalized',\n",
    "    'title',\n",
    "    'selftext',\n",
    "    'VADER_Compound',\n",
    "    'Post_Age'\n",
    "]\n",
    "\n",
    "print(f\"\\n=== 🚩 Highly Negative Posts (VADER < -0.3) — Top 15 ===\")\n",
    "print(f\"Total posts: {len(neg_df)}\\n\")\n",
    "\n",
    "display(neg_df[cols].reset_index(drop=True).head(5))\n",
    "from pathlib import Path\n",
    "\n",
    "# Make output dir\n",
    "output_dir = PROJECT_ROOT / \"outputs\" / \"top20_courses_Most_negative_posts\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Columns to save\n",
    "cols_to_save = [\n",
    "    'post_id', 'CourseCode', 'College_Normalized',\n",
    "    'title', 'selftext', 'VADER_Compound', 'Post_Age'\n",
    "]\n",
    "\n",
    "\n",
    "# === Make output dir ===\n",
    "output_dir = PROJECT_ROOT / \"outputs\" / \"top20_courses_Most_negative_posts\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === Columns to save ===\n",
    "cols_to_save = [\n",
    "    'post_id', 'CourseCode', 'College_Normalized',\n",
    "    'title', 'selftext', 'VADER_Compound', 'Post_Age'\n",
    "]\n",
    "\n",
    "# === Save each course ===\n",
    "files_saved = 0\n",
    "\n",
    "for code, group in neg_df.groupby('CourseCode'):\n",
    "    out_path = output_dir / f\"{code}_top20_most_negative.csv\"\n",
    "    group[cols_to_save].to_csv(out_path, index=False)\n",
    "    files_saved += 1\n",
    "\n",
    "# ✅ Just ONE summary line:\n",
    "print(f\"\\n✅ Saved {files_saved} Top 20 Most Negative files → {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa12a742-e3c6-445c-92ae-d691c5a69dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e707955-72ca-474e-9d8f-37d5267659d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4945428-524e-4de4-9d8b-514c07c1232d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34771ad1-8197-498c-8484-253c84e4446e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b1241-64d2-45f9-b656-c0a07620dc13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c496a414-a5b8-4cc7-8fbc-dec435154d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03c24a3-613d-472b-99ad-0ceb13e7da82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3c12087-624f-41cd-9ba2-47ad5d3fe7af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_course_codes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m chunk[\u001b[33m'\u001b[39m\u001b[33mwords\u001b[39m\u001b[33m'\u001b[39m] = chunk[\u001b[33m'\u001b[39m\u001b[33mcombined_text\u001b[39m\u001b[33m'\u001b[39m].str.split()\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m words \u001b[38;5;129;01min\u001b[39;00m chunk[\u001b[33m'\u001b[39m\u001b[33mwords\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     found = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvalid_course_codes\u001b[49m]\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m found:\n\u001b[32m     24\u001b[39m         posts_with_courses += \u001b[32m1\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'valid_course_codes' is not defined"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "conn = sqlite3.connect(\"../db/WGU-Reddit.db\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT title, selftext FROM posts\n",
    "\"\"\"\n",
    "# this chunk size allows searching all posts without loading all into memory, slowing the notebook.\n",
    "CHUNK_SIZE = 1000\n",
    "\n",
    "course_counts = Counter()\n",
    "total_posts = 0\n",
    "posts_with_courses = 0\n",
    "\n",
    "for chunk in pd.read_sql_query(query, conn, chunksize=CHUNK_SIZE):\n",
    "    total_posts += len(chunk)\n",
    "    chunk['combined_text'] = (chunk['title'].fillna('') + ' ' + chunk['selftext'].fillna('')).str.upper()\n",
    "    chunk['words'] = chunk['combined_text'].str.split()\n",
    "    for words in chunk['words']:\n",
    "        found = [word for word in words if word in valid_course_codes]\n",
    "        if found:\n",
    "            posts_with_courses += 1\n",
    "            course_counts.update(found)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "df_code_counts = pd.DataFrame(course_counts.items(), columns=['Course Code', 'Count'])\n",
    "df_code_counts = df_code_counts.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "df_catalog_renamed = course_codes_df.rename(columns={\n",
    "    'CourseCode': 'Course Code',\n",
    "    'CourseName': 'Course Name',\n",
    "    'Colleges': 'College'\n",
    "})\n",
    "\n",
    "df_code_counts = df_code_counts.merge(df_catalog_renamed, on='Course Code', how='left')\n",
    "\n",
    "def simplify_college_name(college_name):\n",
    "    if pd.isna(college_name):\n",
    "        return \"Other\"\n",
    "    name = college_name.lower()\n",
    "    if \"business\" in name:\n",
    "        return \"Business\"\n",
    "    elif \"health\" in name or \"leavitt\" in name:\n",
    "        return \"Health\"\n",
    "    elif \"technology\" in name:\n",
    "        return \"Technology\"\n",
    "    elif \"teachers\" in name or \"education\" in name:\n",
    "        return \"Education\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "df_code_counts['College'] = df_code_counts['College'].apply(simplify_college_name)\n",
    "\n",
    "df_code_counts = df_code_counts[['Course Code', 'Count', 'Course Name', 'College']]\n",
    "\n",
    "print(f\"Total posts processed: {total_posts}\")\n",
    "print(f\"Posts with course mentions: {posts_with_courses}\")\n",
    "print(f\"Unique course codes found: {len(df_code_counts)}\")\n",
    "\n",
    "display(df_code_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd209a-c0f7-4f1c-8b19-f6d0e11f5123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b00215-9c88-4f8d-9d4f-847e4ff2f7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c72cb4-bac4-4a75-a2d1-a2c75f177ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89d58bfc-4c0f-4b4c-a8fd-ca3f48eb8865",
   "metadata": {},
   "source": [
    "## Deep Dive: Analyze One Course in Detail\n",
    "\n",
    "We’ll focus on **one course** from the top mentions — here, **D335: Introduction to Programming in Python**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94193967-e94c-4f0d-af27-5675d86c70d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load catalog and rename columns\n",
    "catalog_path = \"../WGU_catalog/outputs/courses_with_college_v10.csv\"\n",
    "df_catalog = pd.read_csv(catalog_path)\n",
    "df_catalog = df_catalog.rename(columns={\n",
    "    'CourseCode': 'Course Code',\n",
    "    'CourseName': 'Course Name',\n",
    "    'Colleges': 'College'\n",
    "})\n",
    "\n",
    "# Show only essential columns\n",
    "display(df_catalog[['Course Code', 'Course Name', 'College']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cf0cfc-c377-4a1d-9da8-4b481545e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter posts mentioning this course code\n",
    "df_course_posts = df[df['Course Codes'].apply(lambda codes: course_mention in codes)]\n",
    "\n",
    "print(f\"Found {len(df_course_posts)} posts mentioning {course_mention}.\")\n",
    "display(df_course_posts[['Post ID', 'Title', 'Body', 'Comment Count', 'Post Age']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728c98c0-2a55-42b2-a2c9-a720920af75f",
   "metadata": {},
   "source": [
    "## Apply VADER Sentiment\n",
    "\n",
    "Next, we apply VADER sentiment analysis to these posts.\n",
    "VADER works well for social media text but may miss sarcasm or complex context.\n",
    "It returns positive, negative, neutral, and compound scores for each post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9062e05d-f179-4922-a593-1379c8f5d13d",
   "metadata": {},
   "source": [
    "## Should We Preprocess Text?\n",
    "\n",
    "**Sentiment (VADER):**  \n",
    "- **Do not heavily preprocess.**  \n",
    "- VADER relies on punctuation, casing, emojis, and slang for accuracy.  \n",
    "- We only combine **Title + Body** and lightly clean obvious junk if needed.  \n",
    "- *Source: Hutto & Gilbert, 2014 — VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text.*\n",
    "\n",
    "**Topics & Keywords:**  \n",
    "- **Yes, we preprocess.**  \n",
    "- Remove stopwords, lowercase, remove noise words (e.g., course codes).  \n",
    "- This makes keyword counts, LDA, and BERTopic results more meaningful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13bc7986-4385-4997-8647-47d951a08791",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m pd.set_option(\u001b[33m'\u001b[39m\u001b[33mdisplay.max_colwidth\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m300\u001b[39m)  \u001b[38;5;66;03m# adjust if needed\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Filter posts mentioning this course code\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m df_course_posts = \u001b[43mdf\u001b[49m[df[\u001b[33m'\u001b[39m\u001b[33mCourse Codes\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m codes: course_mention \u001b[38;5;129;01min\u001b[39;00m codes)].copy()\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_course_posts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m posts mentioning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcourse_mention\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Initialize VADER\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Show long text but not infinite\n",
    "pd.set_option('display.max_colwidth', 300)  # adjust if needed\n",
    "\n",
    "# Filter posts mentioning this course code\n",
    "df_course_posts = df[df['Course Codes'].apply(lambda codes: course_mention in codes)].copy()\n",
    "\n",
    "print(f\"Found {len(df_course_posts)} posts mentioning {course_mention}.\")\n",
    "\n",
    "# Initialize VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply VADER to Title + Body\n",
    "def get_compound_sentiment(row):\n",
    "    text = f\"{row['Title']} {row['Body']}\"\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return scores['compound']\n",
    "\n",
    "df_course_posts['Sentiment'] = df_course_posts.apply(get_compound_sentiment, axis=1)\n",
    "\n",
    "# Truncate long body text to 200 characters\n",
    "df_course_posts['Body'] = df_course_posts['Body'].apply(lambda x: x[:200] + '...' if len(x) > 200 else x)\n",
    "\n",
    "# Select columns\n",
    "df_course_posts_display = df_course_posts[['Post ID', 'Title', 'Body', 'Sentiment']].reset_index(drop=True)\n",
    "\n",
    "# Sort by sentiment descending\n",
    "df_sorted = df_course_posts_display.sort_values(by='Sentiment', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Top 5 most positive posts:\")\n",
    "display(df_sorted.head(5))\n",
    "\n",
    "print(\"Top 5 most negative posts:\")\n",
    "display(df_sorted.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c70a8-d6f0-4ae4-b8ce-23eb66a0c691",
   "metadata": {},
   "source": [
    "## Initial Sentiment Check\n",
    "\n",
    "Based on the top and bottom posts, VADER’s compound score seems reasonable for these examples:\n",
    "\n",
    "- **Most positive posts** are about students sharing success stories, passing a tough course, or giving helpful tips to others.\n",
    "- The compound scores are very close to +1, which aligns with encouraging, grateful, or proud tones.\n",
    "- **Most negative posts** show students expressing stress, frustration, self-doubt, or struggling with the course material.\n",
    "- The strongly negative scores (-0.7 to -0.9) match the visible frustration and discouragement in the text.\n",
    "\n",
    "This quick review suggests VADER gives a useful first-pass sentiment signal for identifying strongly positive or negative help-seeking posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a53e16-ded4-4b02-90fd-8e56eb3cb688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(df_course_posts_display['Sentiment'], bins=50, edgecolor='k')\n",
    "plt.title('D335 Sentiment Score Distribution (VADER Compound)')\n",
    "plt.xlabel('Compound Sentiment Score')\n",
    "plt.ylabel('Number of Posts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde8d99-fe18-4bed-9b01-0872a79acf65",
   "metadata": {},
   "source": [
    "## Most posts are positive, many near +1, but there’s a consistent spread across the whole sentiment range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8234ffa6-f04a-459a-8dbd-06b12c472ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"D335 summary statistics\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Mean sentiment: {df_sorted['Sentiment'].mean():.3f}\")\n",
    "print(f\"Median sentiment: {df_sorted['Sentiment'].median():.3f}\")\n",
    "print(f\"Std Dev: {df_sorted['Sentiment'].std():.3f}\")\n",
    "print(f\"Min sentiment: {df_sorted['Sentiment'].min():.3f}\")\n",
    "print(f\"Max sentiment: {df_sorted['Sentiment'].max():.3f}\")\n",
    "print(f\"Count: {df_sorted.shape[0]} posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2812c-548d-4250-8ae4-d6949472cf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get the sentiment column\n",
    "sentiment_scores = df_course_posts_display['Sentiment']\n",
    "\n",
    "# Create bins: from -1.0 to +1.0 in steps of 0.1\n",
    "bins = np.arange(-1.0, 1.1, 0.1)  # inclusive upper\n",
    "\n",
    "# Bin labels: midpoint of each bin for clarity\n",
    "bin_labels = [f\"{round(b,1)} to {round(b+0.1,1)}\" for b in bins[:-1]]\n",
    "\n",
    "# Bin the data\n",
    "sentiment_bins = pd.cut(sentiment_scores, bins=bins, labels=bin_labels, include_lowest=True)\n",
    "\n",
    "# Count posts in each bin\n",
    "bin_counts = sentiment_bins.value_counts().sort_index()\n",
    "\n",
    "# Show it as a table\n",
    "df_bins = bin_counts.reset_index()\n",
    "df_bins.columns = ['Sentiment Range', 'Post Count']\n",
    "print(df_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d355eb-825e-4d6c-af85-b2ec4d5d29a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter again with stricter positive cutoff\n",
    "strong_positive = df_sorted[df_sorted['Sentiment'] >= 0.9].copy()\n",
    "strong_negative = df_sorted[df_sorted['Sentiment'] <= -0.3].copy()\n",
    "\n",
    "# Save to CSVs\n",
    "strong_positive[['Title', 'Body', 'Sentiment']].to_csv('strong_positive_D335.csv', index=False)\n",
    "strong_negative[['Title', 'Body', 'Sentiment']].to_csv('strong_negative_D335.csv', index=False)\n",
    "\n",
    "# Get stats\n",
    "pos_count = strong_positive.shape[0]\n",
    "neg_count = strong_negative.shape[0]\n",
    "\n",
    "pos_min = strong_positive['Sentiment'].min()\n",
    "pos_max = strong_positive['Sentiment'].max()\n",
    "\n",
    "neg_min = strong_negative['Sentiment'].min()\n",
    "neg_max = strong_negative['Sentiment'].max()\n",
    "\n",
    "print(\"✅ Exported:\")\n",
    "print(f\"- strong_positive_D335.csv → {pos_count} posts | Sentiment Range: {pos_min:.3f} to {pos_max:.3f}\")\n",
    "print(f\"- strong_negative_D335.csv → {neg_count} posts | Sentiment Range: {neg_min:.3f} to {neg_max:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d4dd0-fc50-4a41-bf3c-559e2000b41e",
   "metadata": {},
   "source": [
    "## ✅ Final Sentiment Groups — D335\n",
    "\n",
    "**Updated thresholds:**  \n",
    "The strong positive cutoff was tightened from **≥ 0.7** to **≥ 0.9** to exclude mixed “barely passed” posts and keep only clear success stories.\n",
    "\n",
    "- **Strong Positive:** ≥ 0.9 → **[XX] posts** (0.900–0.997)\n",
    "- **Strong Negative:** ≤ -0.3 → **[YY] posts** (-0.923–-0.307)\n",
    "\n",
    "These groups cleanly separate success and struggle for the next keyword and topic modeling steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd527e-cf94-42f5-9e63-acb2d061d6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff4d6da-d821-48d0-a047-f1839b1bd44f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881a98f-6d3d-4c38-904c-37967c915f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02cd500-e50e-4af4-8270-552903663eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b7e7a40-54bf-4c23-b551-d4494be50742",
   "metadata": {},
   "source": [
    "## Begin NLP: Keywords & Topic Modeling\n",
    "\n",
    "In this section, we begin the **NLP (Natural Language Processing)** phase of the pipeline.  \n",
    "We start simple with **keyword frequency** to see what students discuss most often.  \n",
    "Next, we’ll expand to **topic modeling** (LDA and BERTopic) to uncover common themes and issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bdc0c0-e3dd-44b0-b153-bd047d82315e",
   "metadata": {},
   "source": [
    "## NLP Preprocessing for Keywords & Topics\n",
    "\n",
    "For keyword and topic modeling, we remove generic stopwords using **NLTK** and add custom domain-specific stopwords (e.g., course codes, words like *chapter*).  \n",
    "This follows best practice for technical language processing:\n",
    "> *“Removal of stopwords can increase the signal-to-noise ratio in unstructured text and improve topic modeling and classification.”*  \n",
    "(*Sarica & Luo, PLOS ONE, 2021, PMCID: PMC8341615*)\n",
    "\n",
    "**Plan:**  \n",
    "- Lowercase all text  \n",
    "- Remove generic stopwords (NLTK)  \n",
    "- Add custom stopwords as needed  \n",
    "- Filter short words, numbers, or irrelevant terms\n",
    "\n",
    "This ensures clearer keywords and more meaningful topic clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f490e2-94b5-468e-a59e-01717e2a6c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d588c-6e5f-46fd-951c-e2df0289d386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558e2f72-eff9-4697-b40a-ada0d4cff266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07b0cfb2-534c-4378-8cbe-fd1098c92e90",
   "metadata": {},
   "source": [
    "## Remove Generic Stopwords: (NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d478a-41a0-4028-9fe1-9332e9f67721",
   "metadata": {},
   "source": [
    "### Keyword Review — What We Keep vs. Remove\n",
    "\n",
    "**Keep (important):**  \n",
    "Words like `pass`, `passed`, `fail`, `failed`, `struggling`, `problem`, `help`, `issue`, `trouble`, `stuck` stay in — they directly signal **students needing help**.\n",
    "\n",
    "**Remove (low-value):**  \n",
    "Generic words or clutter like `course`, `courses`, `class`, `classes`, `question`, `questions`, specific course codes (`c949`, `d315`, `d427`, `d277`), and platform noise (`https`, `com`, `imgur`, `redd`, `preview`, `hey`, `hello`, `know`, `see`, `get`, `make`, `one`, `time`, `week`, `start`, `first`, `second`, `currently`) are removed — they add no useful signal on their own.\n",
    "\n",
    "This keeps the focus on **real struggles and requests for support**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c9b79-2e03-4ea9-b6b2-27fb2c14e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "\n",
    "# Combine generic and custom stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "custom_stopwords = {\n",
    "    'able', 'already', 'also', 'always',\n",
    "    'around', 'back', 'best', 'better', 'bit', 'business', 'cant',\n",
    "    'c949', 'com', 'complete', 'completed', 'could',\n",
    "    'create', 'currently', 'd277', 'd315', 'd335', 'd427',\n",
    "    'data', 'day', 'days', 'degree', 'different', 'didnt', 'difficult',\n",
    "    'dont', 'done', 'easy', 'easier', 'end', 'enough', 'etc',\n",
    "    'even', 'everyone', 'everything', 'experience', 'far', 'feel',\n",
    "    'felt', 'find', 'finished', 'first', 'focus', 'found',\n",
    "    'get', 'getting', 'give', 'going', 'good', 'got', 'hard',\n",
    "    'hello', 'hey', 'hours', 'https', 'ill', 'imgur', 'information',\n",
    "    'ive', 'keep', 'know', 'last', 'learn', 'learning', 'left',\n",
    "    'like', 'little', 'long', 'look', 'lot', 'luck', 'made',\n",
    "    'make', 'management', 'many', 'material', 'may', 'maybe',\n",
    "    'might', 'month', 'months', 'much', 'multiple', 'need',\n",
    "    'new', 'next', 'one', 'order', 'page', 'part', 'people',\n",
    "    'plan', 'point', 'post', 'preview', 'probably', 'reading',\n",
    "    'really', 'redd', 'right', 'run', 'said', 'say', 'see',\n",
    "    'set', 'similar', 'since', 'someone', 'something', 'start',\n",
    "    'started', 'starting', 'still', 'study', 'studying', 'stuff',\n",
    "    'sure', 'take', 'taking', 'taken', 'term', 'thing', 'things',\n",
    "    'think', 'thought', 'though', 'time', 'times', 'understand',\n",
    "    'understanding', 'used', 'using', 'version', 'watched',\n",
    "    'watch', 'way', 'week', 'weeks', 'well', 'went', 'wgu',\n",
    "    'work', 'working', 'wrong', 'would', 'x200b', 'years',\n",
    "    'youll', 'youre'\n",
    "}\n",
    "stop_words.update({w.lower() for w in custom_stopwords})\n",
    "\n",
    "def preprocess_text(text, course_code=None, course_name=None):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    if course_code:\n",
    "        text = re.sub(re.escape(course_code.lower()), '', text)\n",
    "    if course_name:\n",
    "        # Remove full string match\n",
    "        text = re.sub(re.escape(course_name.lower()), '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "    # Extra: break course name into unique words, drop them too\n",
    "    extra_stopwords = set()\n",
    "    if course_name:\n",
    "        for w in course_name.lower().split():\n",
    "            if len(w) > 2:\n",
    "                extra_stopwords.add(w)\n",
    "\n",
    "    final_tokens = [\n",
    "        w for w in tokens\n",
    "        if w not in stop_words and w not in extra_stopwords and len(w) > 2 and not w.isdigit()\n",
    "    ]\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ab5b9-34f3-4617-85d6-2c26b968b267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Make sure these are safe copies if they were slices before\n",
    "top_positive = top_positive.copy()\n",
    "top_negative = top_negative.copy()\n",
    "df_courses = df_courses.copy()\n",
    "\n",
    "# Add text column if needed\n",
    "df_courses['text'] = df_courses['Title'].fillna('') + ' ' + df_courses['Body'].fillna('')\n",
    "\n",
    "# Preprocess: remove course code and name\n",
    "top_positive['tokens'] = top_positive.apply(\n",
    "    lambda row: preprocess_text(f\"{row['Title']} {row['Body']}\", course_code, course_name), axis=1\n",
    ")\n",
    "top_negative['tokens'] = top_negative.apply(\n",
    "    lambda row: preprocess_text(f\"{row['Title']} {row['Body']}\", course_code, course_name), axis=1\n",
    ")\n",
    "df_courses['tokens'] = df_courses.apply(\n",
    "    lambda row: preprocess_text(row['text'], course_code, course_name), axis=1\n",
    ")\n",
    "\n",
    "print(\"Sample tokens (positive):\", top_positive['tokens'].head(1).values)\n",
    "print(\"Sample tokens (negative):\", top_negative['tokens'].head(1).values)\n",
    "\n",
    "# Combine tokens for top_positive and top_negative\n",
    "pos_tokens = sum(top_positive['tokens'], [])\n",
    "neg_tokens = sum(top_negative['tokens'], [])\n",
    "\n",
    "# Count frequencies\n",
    "pos_counts = Counter(pos_tokens)\n",
    "neg_counts = Counter(neg_tokens)\n",
    "\n",
    "# Also show top tokens for all posts if needed\n",
    "all_tokens = df_courses['tokens'].sum()\n",
    "common_words = Counter(all_tokens).most_common(20)\n",
    "print(\"\\nTop 20 words across all posts with course mentions:\\n\")\n",
    "print(\", \".join([f\"{w} ({c})\" for w, c in common_words]))\n",
    "\n",
    "# Plot helper\n",
    "def plot_keywords(counts, title, n=15):\n",
    "    if not counts:\n",
    "        print(f\"No keywords to plot for {title}\")\n",
    "        return\n",
    "    common = counts.most_common(n)\n",
    "    words, freqs = zip(*common)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(words[::-1], freqs[::-1])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "plot_keywords(pos_counts, 'Top Keywords — Strong Positive D335 Posts')\n",
    "plot_keywords(neg_counts, 'Top Keywords — Strong Negative D335 Posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6bec45-80b2-471d-8b92-643cd8db360e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129d74c7-c66d-4e05-ba42-fe690b1969b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top raw tokens and frequencies for both positive and negative\n",
    "\n",
    "print(\"=== Top 30 tokens in Positive D335 Posts ===\")\n",
    "print(pos_counts.most_common(30))\n",
    "\n",
    "print(\"\\n=== Top 30 tokens in Negative D335 Posts ===\")\n",
    "print(neg_counts.most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e0d80-e193-4c66-ae8a-879661fcd8cc",
   "metadata": {},
   "source": [
    "## Keyword Method — D335 Snapshot\n",
    "\n",
    "**Results:**  \n",
    "The single-word keyword counts show clear signals for D335 posts:\n",
    "\n",
    "- **Strong Positive Posts:**  \n",
    "  Top words highlight `passed`, `python`, `programming`, `codecademy`, `exam`, and `tips`.  \n",
    "  These match passing stories, resources used, and advice shared by students who succeeded.\n",
    "\n",
    "- **Strong Negative Posts:**  \n",
    "  Top words include `test`, `practice`, `chapter`, `second`, `attempt`, `questions`, `code`, `csv`, `file`, and `labs`.  \n",
    "  These show students discussing failed attempts, tricky chapters, file input/output issues, and repeated questions about practice tests and assessments.\n",
    "\n",
    "---\n",
    "\n",
    "## Observation\n",
    "\n",
    "Single-word counts clearly surface the main themes and repeated terms, but they do not capture the full context — like which chapters or which specific problems students face in their code.  \n",
    "This level of detail is essential to detect real help-seeking patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "To understand help requests and common struggles more deeply, we will:\n",
    "- Expand to **bigrams/trigrams** to capture phrases such as *“chapter 33”*, *“file input”*, or *“second attempt OA”*.\n",
    "- Run **BERTopic** to automatically cluster posts into small, related themes.\n",
    "- Optionally add simple pattern tagging for very specific recurring issues.\n",
    "\n",
    "This will provide more practical, course-specific insights than single words alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95060afd-fe85-4c0d-96f3-52eef6cc764e",
   "metadata": {},
   "source": [
    "## Determine length of posts, positive and negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0acebf-d029-40a3-8202-088b97efd558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Title + Body for raw text length\n",
    "strong_positive['text'] = strong_positive['Title'].fillna('') + ' ' + strong_positive['Body'].fillna('')\n",
    "strong_negative['text'] = strong_negative['Title'].fillna('') + ' ' + strong_negative['Body'].fillna('')\n",
    "\n",
    "# Add char + token length\n",
    "strong_positive['char_len'] = strong_positive['text'].apply(len)\n",
    "strong_positive['token_len'] = strong_positive['tokens'].apply(len)\n",
    "\n",
    "strong_negative['char_len'] = strong_negative['text'].apply(len)\n",
    "strong_negative['token_len'] = strong_negative['tokens'].apply(len)\n",
    "\n",
    "# Summary stats\n",
    "print(\"=== Strong Positive Posts ===\")\n",
    "print(\"Count:\", len(strong_positive))\n",
    "print(\"Char Length - Min:\", strong_positive['char_len'].min(), \n",
    "      \"| Max:\", strong_positive['char_len'].max(), \n",
    "      \"| Mean:\", round(strong_positive['char_len'].mean(), 1))\n",
    "print(\"Token Length - Min:\", strong_positive['token_len'].min(), \n",
    "      \"| Max:\", strong_positive['token_len'].max(), \n",
    "      \"| Mean:\", round(strong_positive['token_len'].mean(), 1))\n",
    "\n",
    "print(\"\\n=== Strong Negative Posts ===\")\n",
    "print(\"Count:\", len(strong_negative))\n",
    "print(\"Char Length - Min:\", strong_negative['char_len'].min(), \n",
    "      \"| Max:\", strong_negative['char_len'].max(), \n",
    "      \"| Mean:\", round(strong_negative['char_len'].mean(), 1))\n",
    "print(\"Token Length - Min:\", strong_negative['token_len'].min(), \n",
    "      \"| Max:\", strong_negative['token_len'].max(), \n",
    "      \"| Mean:\", round(strong_negative['token_len'].mean(), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3385a-1dac-4e3f-8cae-3ea2baba98b9",
   "metadata": {},
   "source": [
    "## Design Decision: Preprocessing vs. Course Name\n",
    "\n",
    "Reviewing real negative posts shows students repeat the full course name and code often — but the real signal is in their struggles:\n",
    "- Failing OA attempts\n",
    "- Chapter 33 vs. 34 confusion\n",
    "- CSV and file I/O\n",
    "- Practice test questions\n",
    "- Using Zybooks and other resources\n",
    "\n",
    "**Action:**  \n",
    "We remove the matched course name and code from the text to avoid redundant n-grams, but keep all other context words like *OA*, *PA*, *chapter*, and *test*.  \n",
    "This ensures keyword and topic extraction highlight real problems — not just the course label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a1db5e-5eb0-4660-a525-aa299748332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Updated Preprocessing: Remove Course Name & Code\n",
    "# -------------------------------------------\n",
    "import re\n",
    "\n",
    "# Example: use the first valid course match + name\n",
    "example_course_code = 'D335'\n",
    "example_course_name = df_code_counts[df_code_counts['Course Code'] == example_course_code]['Course Name'].iloc[0]\n",
    "\n",
    "print(f\"Removing code: {example_course_code}, name: {example_course_name}\")\n",
    "\n",
    "# Add raw text\n",
    "df_course_posts['text_raw'] = df_course_posts['Title'] + \" \" + df_course_posts['Body']\n",
    "\n",
    "def clean_text(text, course_code, course_name):\n",
    "    text = text.lower()\n",
    "    text = re.sub(re.escape(course_code.lower()), '', text)\n",
    "    text = re.sub(re.escape(course_name.lower()), '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n",
    "    return text\n",
    "\n",
    "df_course_posts['text_clean_filtered'] = df_course_posts['text_raw'].apply(\n",
    "    lambda text: clean_text(text, example_course_code, example_course_name)\n",
    ")\n",
    "\n",
    "print(df_course_posts[['text_raw', 'text_clean_filtered']].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843c8c30-37cb-44b1-b926-49869956ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "tokens = []\n",
    "for text in df_course_posts['text_clean']:\n",
    "    tokens += text.split()\n",
    "\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "\n",
    "print(Counter(bigrams).most_common(15))\n",
    "print(Counter(trigrams).most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082edf60-74fc-486b-a1e1-cce230ac7f45",
   "metadata": {},
   "source": [
    "## Design Decision: Preprocessing vs. Course Name\n",
    "\n",
    "Reviewing real negative posts shows students repeat the full course name and code often — but the real signal is in their struggles:\n",
    "- Failing OA attempts\n",
    "- Chapter 33 vs. 34 confusion\n",
    "- CSV and file I/O\n",
    "- Practice test questions\n",
    "- Using Zybooks and other resources\n",
    "\n",
    "**Action:**  \n",
    "We remove the matched course name and code from the text to avoid redundant n-grams, but keep all other context words like *OA*, *PA*, *chapter*, and *test*.  \n",
    "This ensures keyword and topic extraction highlight real problems — not just the course label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f58fe94-8291-4e24-b9bf-e1a7050728b3",
   "metadata": {},
   "source": [
    "## finalized Preprocessing for Keywords & Phrases\n",
    "\n",
    "We now:\n",
    "- Keep `text_raw` for BERTopic and human reading.\n",
    "- Use `preprocess_text()` to:\n",
    "  - Lowercase & clean.\n",
    "  - Remove the matched course code/name.\n",
    "  - Apply generic + custom stopwords.\n",
    "- Store the final token list → `tokens`.\n",
    "\n",
    "This guarantees the n-grams show real student context like *“chapter 33”* or *“second attempt”* instead of repeating the course name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ecd16-629f-437c-8f54-c3b243958841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Add Raw & Clean Columns with New Preprocessing\n",
    "# -------------------------------------------\n",
    "# Pick example course code & name\n",
    "example_course_code = 'D335'\n",
    "example_course_name = df_code_counts[df_code_counts['Course Code'] == example_course_code]['Course Name'].iloc[0]\n",
    "\n",
    "print(f\"Removing: {example_course_code} | {example_course_name}\")\n",
    "\n",
    "# Add raw text\n",
    "df_course_posts['text_raw'] = df_course_posts['Title'] + \" \" + df_course_posts['Body']\n",
    "\n",
    "# Add cleaned + tokenized text\n",
    "df_course_posts['tokens'] = df_course_posts.apply(\n",
    "    lambda row: preprocess_text(\n",
    "        row['text_raw'],\n",
    "        course_code=example_course_code,\n",
    "        course_name=example_course_name\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "print(df_course_posts[['text_raw', 'tokens']].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f6348-a730-4623-9515-df5da50c8eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Bigrams/Trigrams with Cleaned Tokens\n",
    "# -------------------------------------------\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "all_tokens = sum(df_course_posts['tokens'], [])\n",
    "\n",
    "bigrams = list(ngrams(all_tokens, 2))\n",
    "trigrams = list(ngrams(all_tokens, 3))\n",
    "\n",
    "print(\"Top 15 Bigrams:\", Counter(bigrams).most_common(15))\n",
    "print(\"Top 15 Trigrams:\", Counter(trigrams).most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af4dee9-0811-4a4d-a848-fa725cd01f0b",
   "metadata": {},
   "source": [
    "##  Observation: Bigrams & Trigrams\n",
    "\n",
    "The top bigrams and trigrams now highlight **real student context**, not just repeated course names.  \n",
    "Key phrases like **“practice test”**, **“second attempt”**, and **“zybooks course”** show where students struggle and what resources they use.  \n",
    "This confirms the preprocessing step works as intended — surfacing **help-seeking signals and study patterns**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf09da3-dd3e-4e93-847c-9257e7fec653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Bigrams and Trigrams on negative sentiment posts (<-.3 ) only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7768e8d-2fa7-427a-9b8c-de4cc1331c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply updated preprocessing on negative slice\n",
    "strong_negative['tokens'] = strong_negative.apply(\n",
    "    lambda row: preprocess_text(\n",
    "        row['Title'] + \" \" + row['Body'],\n",
    "        course_code=example_course_code,\n",
    "        course_name=example_course_name\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Negative-only bigrams/trigrams\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "neg_tokens = sum(strong_negative['tokens'], [])\n",
    "\n",
    "neg_bigrams = list(ngrams(neg_tokens, 2))\n",
    "neg_trigrams = list(ngrams(neg_tokens, 3))\n",
    "\n",
    "print(\"Top 15 Negative Bigrams:\", Counter(neg_bigrams).most_common(15))\n",
    "print(\"Top 15 Negative Trigrams:\", Counter(neg_trigrams).most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef87e3a-d1b8-4271-81da-5f18c86c8674",
   "metadata": {},
   "source": [
    "##  Negative-Only Bigrams & Trigrams\n",
    "\n",
    "The top bigrams and trigrams in **strong negative posts** confirm the pipeline captures **real help-seeking context**:\n",
    "- **Practice test**, **second attempt**, **Zybooks** → highlight where students struggle.\n",
    "- Phrases like **“anyone else fail”** and **“bombed emailed professor”** show direct peer help-seeking and escalation.\n",
    "- This validates that the preprocessing steps filter out redundant labels and reveal **true pain points** for specific courses.\n",
    "\n",
    "Next, we run **BERTopic** on the same slice to cluster these issues at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef8d4d8-7b26-409a-94e1-2ada13649187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4996f2c0-5ca0-41b9-a65c-5d570cb818f1",
   "metadata": {},
   "source": [
    "update:\n",
    "\n",
    "## Keywords, Bigrams & Trigrams — What We Did\n",
    "\n",
    "- **Filtered Posts:** We sliced the dataset into **strong positive** and **strong negative** sentiment groups using VADER.\n",
    "- **Focused on Negative:** We ran bigram/trigram counts specifically on **negative posts** (Sentiment <= -0.3) to highlight **help-seeking signals**.\n",
    "- **Preprocessed:** We removed the matched course code and course name to prevent redundant phrases from dominating counts.\n",
    "- **Tokens:** We used a combined stopword list (generic + custom) to drop filler words but keep domain context like *OA*, *chapter*, *test*.\n",
    "- **N-grams:** We extracted and counted bigrams/trigrams to surface real student context — for example:\n",
    "  - *“practice test”*\n",
    "  - *“second attempt”*\n",
    "  - *“zybooks course”*\n",
    "  - *“anyone else fail”*\n",
    "\n",
    "These phrases confirm the pipeline surfaces **real struggles**, not just generic chatter.\n",
    "\n",
    "---\n",
    "\n",
    "## BERTopic — Current Status\n",
    "\n",
    "- **Planned:** The pipeline is ready to run **BERTopic** next.\n",
    "- **Input:** We will use the full **text_raw** (Title + Body) to keep full context.\n",
    "- **Goal:** Cluster posts into themes, showing repeated issues (e.g., practice tests, OA retakes, file I/O errors).\n",
    "- **Not Yet Run:** The BERTopic clustering step will be added next to validate the same signals appear automatically at scale.\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Finalize BERTopic setup → cluster the same negative posts → export samples → link clusters back to help-seeking patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dfb1e5-435d-4d53-b394-dbcbbbdd6b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Use a lightweight embedding model for speed\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "topic_model = BERTopic(embedding_model=embedding_model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8c651d-ea6e-45ff-9850-f47bbe725ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_negative['text_raw'] = strong_negative['Title'] + \" \" + strong_negative['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ceb441-df71-4bb9-aa5e-21689d778c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(strong_negative.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb1b558-2880-4569-9f85-4476306ca735",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(strong_negative[['Title', 'Body', 'text_raw']].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0945ab9b-620e-441d-b6ef-091b9e7a1304",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = strong_negative['text_raw'].tolist()\n",
    "print(texts[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8512f83-d15c-4da6-873b-400b70f5d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7716545c-11b0-4321-a75f-4fff201bd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060de656-ca50-4b63-8d2a-792a6a328cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_info[topic_info.Topic != -1].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ddc4b2-874c-4ce8-9c3f-397f13d3bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_model.get_topic(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e0978b-86c2-4e28-9573-fd1ae0914510",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(texts))  # texts = strong_negative['text_raw'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806b62df-20d9-437e-998f-0c6fe45e11f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You need to lower the min_cluster_size parameter to let BERTopic form smaller clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4df389-23d1-4dc5-b80a-c4f023a72162",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(strong_negative.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b091236-4ed5-452c-b02f-31d400f4065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(strong_negative['text_raw'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ba19ed-1853-4481-a7ca-c551b88983e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import hdbscan\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize HDBSCAN clusterer with desired min_cluster_size\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=2, prediction_data=True)\n",
    "\n",
    "# Pass clusterer to BERTopic\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    hdbscan_model=clusterer,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "texts = strong_negative['text_raw'].tolist()\n",
    "topics, probs = topic_model.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6315de03-6040-4feb-848a-449c3a121680",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca32c57-bc6e-4791-8140-fa0e70eb6c6a",
   "metadata": {},
   "source": [
    "## The BERTopic clustering identified several key themes in the strongly negative D335 posts.\n",
    "These include discussions around practice tests and OA attempts, struggles with learning Python, programming certification topics, and student progress with ZyBooks.\n",
    "Nine posts were classified as outliers, which is expected given the small dataset size and the varied content.\n",
    "This thematic grouping will help target support resources and identify common pain points in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f32870-322d-4532-8d12-dc534649d06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b24717-70bc-43b3-8267-624807334a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create 'outputs' folder if it doesn't exist\n",
    "os.makedirs('outputs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfc068b-866c-47e7-bd13-133dcaa29e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Attach cluster labels back to the dataframe (if not done yet)\n",
    "strong_negative['BERTopic_Cluster'] = topics\n",
    "\n",
    "# 2. Save the dataframe with cluster info for reference and reporting\n",
    "strong_negative.to_csv(\"outputs/d335_negative_bertopic_clusters.csv\", index=False)\n",
    "\n",
    "# 3. Display topic summary\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info.head(10))\n",
    "\n",
    "# 4. Visualize topics interactively\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c1ec5-a1ee-4853-a539-aab3912212e7",
   "metadata": {},
   "source": [
    "per GPT: ### BERTopic Cluster Visualization\n",
    "\n",
    "The interactive cluster map shows:\n",
    "\n",
    "- **Clusters 0, 1, and 2 group closely** in the top-left region, reflecting shared themes around practice tests, OA attempts, and programming fundamentals in D335.\n",
    "- **Cluster 3 is visually distinct** at the bottom right, likely capturing posts about course navigation and ZyBooks usage.\n",
    "- The red circle highlighting topic 1 emphasizes this grouping and confirms the model's semantic separation of topics.\n",
    "  \n",
    "This visualization supports the idea that posts can be meaningfully grouped into recurring themes aligned with student challenges and course content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f1b3c-1d99-4333-b6b5-462136587d88",
   "metadata": {},
   "source": [
    "## What BERTopic Reveals About Course D335 — Confirmed by Raw Posts\n",
    "\n",
    "### Key Insights from BERTopic Clustering on Negative Sentiment Posts\n",
    "\n",
    "1. **Practice Tests and OA Retakes are a Major Student Concern**  \n",
    "   - BERTopic identified a cluster focused on *“practice test,” “OA attempt,”* and related terms, highlighting recurring struggles with assessments.  \n",
    "   - **Raw post example:**  \n",
    "     *“Second attempt at D335 OA... are the questions very similar? First attempt close to PA and Practice Test 2 questions...”*  \n",
    "     *“I bombed it and emailed the professor but all I received was a study plan...”*\n",
    "\n",
    "2. **Students Express Confusion Between Chapters 33 and 34**  \n",
    "   - Both the topic model and n-gram analysis surfaced terms like *“chapter 33”* and *“chapter 34,”* indicating uncertainty about the labs and their difficulty.  \n",
    "   - **Raw post example:**  \n",
    "     *“Chapter 33/34 labs are very similar to OA but I was so focused on memorization...”*  \n",
    "     *“Is the practice test from chapter 33 harder than 34 or is it just me?”*\n",
    "\n",
    "3. **ZyBooks Usage is Frequently Discussed, Often with Frustration**  \n",
    "   - A distinct cluster revolved around ZyBooks, reflecting its central role and associated challenges in the course.  \n",
    "   - **Raw post example:**  \n",
    "     *“For the ZyBooks, where did you stop before going to lab 33 and 34? Did you do all optional or only required reading?”*  \n",
    "     *“We all hate ZyBooks. I hate it. Is there any good place to learn how to practice coding in Python?”*\n",
    "\n",
    "4. **Active Help-Seeking Through Professor Contact After Failing**  \n",
    "   - BERTopic and n-grams highlighted phrases like *“bombed emailed professor,”* showing students reach out for help after poor results.  \n",
    "   - **Raw post example:**  \n",
    "     *“I bombed it and emailed the professor but all I received was a study plan to complete before testing again.”*\n",
    "\n",
    "---\n",
    "\n",
    "### Quality Assessment of BERTopic on This Dataset\n",
    "\n",
    "- Despite a small dataset (20 posts) and some noisy, brief posts, BERTopic was able to find **meaningful, coherent clusters** that reflect genuine student struggles.  \n",
    "- Initial runs with default clustering parameters grouped most posts as outliers, but tuning (lowering `min_cluster_size`) yielded clearer thematic groups.  \n",
    "- The clusters align closely with manual keyword analyses and the raw post content, validating the approach.  \n",
    "- Limitations include small sample size and some noise in posts, suggesting further improvements with larger data and filtering.  \n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "**BERTopic effectively surfaces authentic student pain points and discussion themes in course D335, which manual review of raw posts confirms.**  \n",
    "This confirms the value of combining sentiment filtering, course-specific preprocessing, and unsupervised topic modeling to monitor academic help-seeking behavior in Reddit data.\n",
    "\n",
    "---\n",
    "\n",
    "*Next Steps:* Scale this approach to more courses and larger datasets for robust academic support insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a0dc2-d5e8-4887-a639-d27085bf0e92",
   "metadata": {},
   "source": [
    "## Potential Improvements Based on BERTopic Best Practices\n",
    "\n",
    "- **Pre-calculate embeddings** to speed up iterative modeling and parameter tuning.  \n",
    "- **Set a fixed `random_state` in UMAP** to ensure reproducible and stable clustering results.  \n",
    "- **Experiment with HDBSCAN parameters** (e.g., `min_cluster_size`, `cluster_selection_method`) to better control topic granularity and reduce noise.  \n",
    "- **Customize the vectorizer model** by removing stopwords, setting `min_df` thresholds, and including n-grams (bigrams/trigrams) to enhance topic keyword quality.  \n",
    "- **Explore advanced topic representations** such as KeyBERT-inspired keywords or GPT-based labels for improved topic interpretability.  \n",
    "- **Reduce outliers post-clustering** using `reduce_outliers()` to assign previously unclustered posts, increasing topic coverage.  \n",
    "- **Use interactive visualizations** (`visualize_topics()`, `visualize_hierarchy()`) with custom labels for better insight and validation of topics.  \n",
    "- **Save and reload models with safetensors** to speed up inference and enable lightweight deployment.  \n",
    "\n",
    "Implementing these can improve cluster coherence, interpretability, and robustness—especially important given our dataset size and text variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb93a2e-86c2-4008-ab7f-990605e10606",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://medium.com/@karthikvellanki/dynamic-clustering-for-small-datasets-140458dfff1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a709dba5-f801-4514-ae1c-7d2016eed1fa",
   "metadata": {},
   "source": [
    "## Challenges and Solutions for Topic Modeling on Small Datasets\n",
    "\n",
    "Topic modeling algorithms like BERTopic work best with large, dense datasets. Small datasets—especially with short texts like Reddit posts—pose unique challenges:\n",
    "\n",
    "- **Sparse embeddings:** With fewer data points, vector representations become sparse, making density-based clustering (like HDBSCAN) less effective.\n",
    "- **Many outliers:** Clusters may be too small or fragmented, and many points get labeled as noise.\n",
    "- **Dynamic data issues:** Small datasets often require flexible clustering to handle new data points or manual adjustments.\n",
    "\n",
    "The article _“Dynamic Clustering for Small Datasets”_ by Karthik Vellanki highlights that traditional density-based clustering struggles with small, sparse data. He suggests:\n",
    "\n",
    "- Using **soft clustering**, where each data point has probabilities for membership in multiple clusters, rather than hard assignments. This yields richer, more flexible clusters.\n",
    "- Combining soft clustering with algorithms like agglomerative clustering to better handle dynamic updates and cluster shape irregularities.\n",
    "\n",
    "For our pipeline, this means:\n",
    "\n",
    "- We should carefully tune clustering parameters (e.g., lower `min_cluster_size` in HDBSCAN).\n",
    "- Consider approaches beyond strict density clustering to improve cluster quality on limited Reddit post sets.\n",
    "- Explore probabilistic or dynamic clustering methods to handle small, evolving datasets.\n",
    "\n",
    "This insight explains some of the difficulties we encountered and points toward advanced methods to improve topic modeling on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c94fef1-3bab-4c31-aa6a-10d59fec9906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (WGU)",
   "language": "python",
   "name": "wgu-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
