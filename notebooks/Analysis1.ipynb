{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2299de90-5aad-4a70-af92-47789d886059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/buddy/Desktop/WGU-Reddit\n",
      "Input dir: /Users/buddy/Desktop/WGU-Reddit/data\n",
      "Output dir: /Users/buddy/Desktop/WGU-Reddit/outputs\n",
      "Loaded 19001 rows.\n",
      "Index(['post_id', 'title', 'selftext', 'permalink'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Setup and database test\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Set project root for Jupyter\n",
    "project_root = Path(\"/Users/buddy/Desktop/WGU-Reddit\")\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from utils.paths import DATA_DIR, OUTPUT_DIR, DB_PATH, path\n",
    "from utils.db_connection import get_db_connection, load_posts_dataframe\n",
    "from utils.cleaning_functions import cleaning_vader, cleaning_nltk, cleaning_bertopic\n",
    "from utils.sentiment import calculate_vader_sentiment\n",
    "from utils.filters import apply_filters\n",
    "\n",
    "# Load course list\n",
    "course_list = pd.read_csv(DATA_DIR / \"courses_with_college_v10.csv\")\n",
    "output_dir = OUTPUT_DIR\n",
    "# Load posts from DB\n",
    "df = load_posts_dataframe()\n",
    "\n",
    "# Confirm setup\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Input dir: {DATA_DIR}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")\n",
    "print(f\"Loaded {len(df)} rows.\")\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10c9e8ca-881d-4a26-8954-3bdbdc74779c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19001 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1k6jeqd</td>\n",
       "      <td>Examity I‚Äôm curious as to how examity works. I...</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1k6j88n</td>\n",
       "      <td>Any Canadians here pursuing software developme...</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1k6iufu</td>\n",
       "      <td>ANYONE IN D277 I‚Äôm half way through Front End ...</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1k6hw8z</td>\n",
       "      <td>DING! Finally!! It's been a rough 2 years for ...</td>\n",
       "      <td>1742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1k6gjrk</td>\n",
       "      <td>Anyone ever have a capstone returned for revis...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                         text_clean  text_length\n",
       "0  1k6jeqd  Examity I‚Äôm curious as to how examity works. I...          495\n",
       "1  1k6j88n  Any Canadians here pursuing software developme...          572\n",
       "2  1k6iufu  ANYONE IN D277 I‚Äôm half way through Front End ...          228\n",
       "3  1k6hw8z  DING! Finally!! It's been a rough 2 years for ...         1742\n",
       "4  1k6gjrk  Anyone ever have a capstone returned for revis...           50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell: Run VADER cleaning (no NLTK)\n",
    "\n",
    "from utils.cleaning_functions import cleaning_vader\n",
    "\n",
    "df_vader = cleaning_vader(df)\n",
    "print(f\"{len(df)} rows.\")\n",
    "\n",
    "display(df_vader[['post_id','text_clean', 'text_length']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d7176bf-f98b-4480-bc59-c264eb559759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[filter_by_length] üìè Filtered to 717 posts (length 40-1000)\n",
      "[filter_by_course_codes] üéì Filtered to 717 posts with exactly 1 course match(es)\n",
      "[filter_sentiment] üéØ Filtered to 370 posts and ‚â§ -0.6.\n",
      "Filtered 370 posts with VADER ‚â§ -0.6\n",
      "Saved to: /Users/buddy/Desktop/WGU-Reddit/outputs/courses_filtered_scored_neg06.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell: Re-apply filters to existing VADER-scored file\n",
    "\n",
    "import pandas as pd\n",
    "from utils.filters import apply_filters\n",
    "\n",
    "# Load existing sentiment-scored posts\n",
    "input_path = OUTPUT_DIR / \"courses_filtered_scored.csv\"\n",
    "df_scored = pd.read_csv(input_path)\n",
    "\n",
    "# Load course codes\n",
    "course_codes = pd.read_csv(DATA_DIR / \"courses_with_college_v10.csv\")[\"CourseCode\"].dropna().astype(str).tolist()\n",
    "\n",
    "# Filter config ‚Äì update sentiment threshold here\n",
    "filters_config = {\n",
    "    \"length\": {\"enabled\": True, \"params\": {\"min_length\": 40, \"max_length\": 1000}},\n",
    "    \"course_codes\": {\"enabled\": True, \"params\": {\"course_codes\": course_codes, \"exact_match_count\": 1}},\n",
    "    \"sentiment\": {\"enabled\": True, \"params\": {\"max_score\": -0.6}}  # Adjust as needed\n",
    "}\n",
    "\n",
    "# Apply filters\n",
    "df_vader_filtered = apply_filters(df_scored, filters_config)\n",
    "\n",
    "# Save to new CSV\n",
    "output_path = OUTPUT_DIR / \"courses_filtered_scored_neg06.csv\"\n",
    "df_vader_filtered.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Filtered {len(df_vader_filtered)} posts with VADER ‚â§ -0.6\")\n",
    "print(f\"Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5ad3fd4-380e-4344-bcb2-5d5ec63c63c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK cleaning complete: 67 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1jkl3jc</td>\n",
       "      <td>failed oa couple minute ago took oa almost pas...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1b4ghw4</td>\n",
       "      <td>c first revision needed program edit passed hu...</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>18wi8zz</td>\n",
       "      <td>critical thinking yall omg passed oa much lawd...</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1jjyznx</td>\n",
       "      <td>failed nd oa end chapter quiz took pre assessm...</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1k0yej3</td>\n",
       "      <td>failed first oad feel discouraged hi guy faile...</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    post_id                                         text_clean  text_length\n",
       "10  1jkl3jc  failed oa couple minute ago took oa almost pas...           94\n",
       "32  1b4ghw4  c first revision needed program edit passed hu...          432\n",
       "35  18wi8zz  critical thinking yall omg passed oa much lawd...          230\n",
       "55  1jjyznx  failed nd oa end chapter quiz took pre assessm...          294\n",
       "92  1k0yej3  failed first oad feel discouraged hi guy faile...          221"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell: NLTK cleaning after VADER filtering\n",
    "\n",
    "# Apply NLTK-style cleaning\n",
    "df_nltk_cleaned = cleaning_nltk(df_vader_filtered)\n",
    "\n",
    "print(f\"NLTK cleaning complete: {len(df_nltk_cleaned)} rows.\")\n",
    "\n",
    "display(df_nltk_cleaned[['post_id', 'text_clean', 'text_length']].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dc633d-a979-4826-a535-9bc29b8bd6b1",
   "metadata": {},
   "source": [
    "## Tier2\n",
    "2. **Pattern Detection (Tier 2):**\n",
    "\n",
    "   * Employed NLP techniques to extract specific question sentences from filtered posts.\n",
    "   * Identified common question starters and frequent bigrams within questions.\n",
    "   * Developed a rule-based classifier using these linguistic patterns to categorize queries effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d091ebd-7165-4d6e-8ac3-5b507fd64427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Unigrams:\n",
      "[('im', 67), ('class', 63), ('failed', 41), ('oa', 40), ('question', 38), ('time', 37), ('ive', 32), ('pa', 31), ('first', 28), ('course', 28), ('passed', 26), ('c', 24), ('exam', 24), ('second', 22), ('anyone', 21), ('took', 20), ('dont', 20), ('one', 19), ('know', 19), ('get', 19)]\n",
      "\n",
      "Top 20 Bigrams:\n",
      "[(('failed', 'oa'), 6), (('anyone', 'else'), 6), (('first', 'time'), 6), (('dont', 'know'), 6), (('second', 'time'), 5), (('end', 'month'), 4), (('first', 'oa'), 4), (('second', 'attempt'), 4), (('im', 'sure'), 4), (('first', 'attempt'), 4), (('took', 'oa'), 3), (('study', 'plan'), 3), (('course', 'instructor'), 3), (('failed', 'first'), 3), (('feel', 'discouraged'), 3), (('feel', 'like'), 3), (('data', 'management'), 3), (('class', 'im'), 3), (('failed', 'second'), 3), (('waste', 'time'), 3)]\n",
      "\n",
      "Top 20 Trigrams:\n",
      "[(('going', 'back', 'school'), 3), (('anyone', 'else', 'encounter'), 2), (('cant', 'seem', 'get'), 2), (('work', 'full', 'time'), 2), (('two', 'week', 'im'), 2), (('im', 'beyond', 'frustrated'), 2), (('end', 'month', 'end'), 2), (('month', 'end', 'term'), 2), (('second', 'class', 'im'), 2), (('second', 'oa', 'fail'), 2), (('class', 'go', 'hell'), 2), (('failed', 'oa', 'couple'), 1), (('oa', 'couple', 'minute'), 1), (('couple', 'minute', 'ago'), 1), (('minute', 'ago', 'took'), 1), (('ago', 'took', 'oa'), 1), (('took', 'oa', 'almost'), 1), (('oa', 'almost', 'passed'), 1), (('almost', 'passed', 'missed'), 1), (('passed', 'missed', 'one'), 1)]\n"
     ]
    }
   ],
   "source": [
    "# Cell: N-gram frequency analysis\n",
    "\n",
    "from nltk import FreqDist, bigrams, trigrams\n",
    "from itertools import chain\n",
    "\n",
    "# Unigrams\n",
    "unigrams = list(chain.from_iterable(df_nltk_cleaned['tokens']))\n",
    "fdist_uni = FreqDist(unigrams)\n",
    "print(\"Top 20 Unigrams:\")\n",
    "print(fdist_uni.most_common(20))\n",
    "\n",
    "# Bigrams\n",
    "bigrams_list = list(chain.from_iterable(df_nltk_cleaned['tokens'].apply(bigrams)))\n",
    "fdist_bi = FreqDist(bigrams_list)\n",
    "print(\"\\nTop 20 Bigrams:\")\n",
    "print(fdist_bi.most_common(20))\n",
    "\n",
    "# Trigrams\n",
    "trigrams_list = list(chain.from_iterable(df_nltk_cleaned['tokens'].apply(trigrams)))\n",
    "fdist_tri = FreqDist(trigrams_list)\n",
    "print(\"\\nTop 20 Trigrams:\")\n",
    "print(fdist_tri.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "240f08a9-3f01-470d-94ad-eac1b4d52370",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment = -0.3\n",
    "help_seeking_trigrams_with_stop = [\n",
    "    (\"i\", \"need\", \"to\"),\n",
    "    (\"im\", \"not\", \"sure\"),\n",
    "    (\"i\", \"dont\", \"know\"),\n",
    "    (\"i\", \"have\", \"no\"),\n",
    "    (\"i\", \"have\", \"to\"),\n",
    "    (\"i\", \"want\", \"to\"),\n",
    "    (\"on\", \"how\", \"to\"),\n",
    "    (\"i\", \"feel\", \"like\"),\n",
    "    (\"doe\", \"anyone\", \"have\"),\n",
    "    (\"anyone\", \"have\", \"any\"),\n",
    "]\n",
    "\n",
    "help_seeking_unigrams_no_stop = [\n",
    "    \"failed\", \"struggling\", \"help\", \"question\", \"confused\", \"lost\", \"issue\", \"problem\", \"retry\", \"trouble\"\n",
    "]\n",
    "\n",
    "help_seeking_bigrams_no_stop = [\n",
    "    (\"failed\", \"oa\"),\n",
    "    (\"first\", \"attempt\"),\n",
    "    (\"second\", \"attempt\"),\n",
    "    (\"task\", \"returned\"),\n",
    "    (\"task\", \"rejected\"),\n",
    "    (\"study\", \"guide\"),\n",
    "    (\"sent\", \"back\"),\n",
    "    (\"exam\", \"retake\"),\n",
    "    (\"need\", \"help\"),\n",
    "    (\"cant\", \"pass\")\n",
    "]\n",
    "\n",
    "help_seeking_trigrams_no_stop = [\n",
    "    (\"failed\", \"first\", \"attempt\"),\n",
    "    (\"failed\", \"second\", \"attempt\"),\n",
    "    (\"task\", \"sent\", \"back\"),\n",
    "    (\"task\", \"got\", \"returned\"),\n",
    "    (\"need\", \"study\", \"guide\"),\n",
    "    (\"struggling\", \"pass\", \"oa\"),\n",
    "    (\"dont\", \"understand\", \"material\"),\n",
    "    (\"cant\", \"figure\", \"out\"),\n",
    "    (\"need\", \"help\", \"oa\"),\n",
    "    (\"repeated\", \"task\", \"submission\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5175080-1b6c-4cd5-a905-c22f5f17e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment -0.8\n",
    "help_seeking_unigrams_neg08 = [\n",
    "    \"failed\", \"frustrated\", \"dont\", \"question\", \"anyone\", \"passed\", \"took\", \"exam\", \"oa\", \"discouraged\"\n",
    "]\n",
    "\n",
    "help_seeking_bigrams_neg08 = [\n",
    "    (\"failed\", \"oa\"),\n",
    "    (\"second\", \"attempt\"),\n",
    "    (\"first\", \"attempt\"),\n",
    "    (\"dont\", \"know\"),\n",
    "    (\"feel\", \"discouraged\"),\n",
    "    (\"failed\", \"first\"),\n",
    "    (\"failed\", \"second\"),\n",
    "    (\"waste\", \"time\"),\n",
    "    (\"anyone\", \"else\"),\n",
    "    (\"study\", \"plan\")\n",
    "]\n",
    "\n",
    "help_seeking_trigrams_neg08 = [\n",
    "    (\"cant\", \"seem\", \"get\"),\n",
    "    (\"im\", \"beyond\", \"frustrated\"),\n",
    "    (\"second\", \"oa\", \"fail\"),\n",
    "    (\"class\", \"go\", \"hell\"),\n",
    "    (\"failed\", \"oa\", \"couple\"),\n",
    "    (\"took\", \"oa\", \"almost\"),\n",
    "    (\"oa\", \"almost\", \"passed\"),\n",
    "    (\"almost\", \"passed\", \"missed\"),\n",
    "    (\"passed\", \"missed\", \"one\"),\n",
    "    (\"anyone\", \"else\", \"encounter\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8724264-1401-4bd1-bea0-2cbf92e76b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f98a0-83b2-4284-8fd2-6839cc5a9f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cc5a76c-c0f0-468b-b222-321b16f45bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved NLTK-cleaned posts to: /Users/buddy/Desktop/WGU-Reddit/outputs/courses_nltk_cleaned_full.csv\n"
     ]
    }
   ],
   "source": [
    "# test lemmatize\n",
    "# Cell: Save full NLTK-cleaned posts to CSV\n",
    "\n",
    "from utils.cleaning_functions import cleaning_nltk\n",
    "import pandas as pd\n",
    "\n",
    "# Start from VADER-filtered posts\n",
    "input_path = \"/Users/buddy/Desktop/WGU-Reddit/outputs/courses_filtered_scored.csv\"\n",
    "df_vader = pd.read_csv(input_path)\n",
    "\n",
    "# Run NLTK cleaning (make sure stopword removal is disabled if needed)\n",
    "df_nltk_cleaned = cleaning_nltk(df_vader)\n",
    "\n",
    "# Save to CSV for inspection\n",
    "output_path = \"/Users/buddy/Desktop/WGU-Reddit/outputs/courses_nltk_cleaned_full.csv\"\n",
    "df_nltk_cleaned.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved NLTK-cleaned posts to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c665ed7-6292-4e96-8468-31072637acb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3160c42c-5aa1-42c4-bba9-be3343c176b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b7245-ad4e-4ee4-b966-98afc7bc6bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780d6f7-dbfe-47d8-8511-a214cf00ec6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502fe913-18fc-4a67-a84d-059ae402c10c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3183108-2344-46cd-bec5-76aaf4552e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b65a8f3-0ee9-4afe-8578-3ec4b3a14cba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6912382-b32f-421d-933b-9334f0a542b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3780cb45-bee6-45f2-b9a3-5dcb677f02b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c949fc63-4505-4f61-b67f-0a3abe4b0b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6abe85c5-61b5-4ca3-8917-063dda690786",
   "metadata": {},
   "source": [
    "# A.4 Data Analytics Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239feb83-0c30-4202-aee4-04d579d9b75b",
   "metadata": {},
   "source": [
    "## 1. Cleaning and Preprocessing:\n",
    "\n",
    "a) Merge `title` and `selftext`  \n",
    "‚ÄÉ‚ÄÉ*For: VADER, NLTK n-grams, BERTopic*\n",
    "\n",
    "b) Lowercase text  \n",
    "‚ÄÉ‚ÄÉ*For: Not VADER*\n",
    "\n",
    "c) Remove URLs, emojis, and extra punctuation  \n",
    "‚ÄÉ‚ÄÉ*For: BERTopic, NLTK n-grams ‚Äî NOT for VADER*\n",
    "\n",
    "d) Remove special characters and digits (optional)  \n",
    "‚ÄÉ‚ÄÉ*For: NLTK n-grams, BERTopic ‚Äî NOT for VADER*\n",
    "\n",
    "e) Tokenize text  \n",
    "‚ÄÉ‚ÄÉ*For: NLTK n-grams*\n",
    "\n",
    "f) Remove stopwords  \n",
    "‚ÄÉ‚ÄÉ*For: NLTK n-grams, BERTopic ‚Äî NOT for VADER*\n",
    "\n",
    "g) Lemmatize tokens  \n",
    "‚ÄÉ‚ÄÉ*For: NLTK n-grams, BERTopic ‚Äî NOT for VADER*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92744177-0fa7-4047-b185-0f8637d4507f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57627bc6-3871-4d2e-bdd1-27b2982c811d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c725e-674a-4305-8bc1-ea7275ec12af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c931220f-ff8d-454e-983f-a8c08a507372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34f7631e-6a65-4626-8c2e-dc19bba4b1d0",
   "metadata": {},
   "source": [
    "## **2. Pattern Detection (Tier 2):**\n",
    "\n",
    "   * Employed NLP techniques to extract specific question sentences from filtered posts.\n",
    "   * Identified common question starters and frequent bigrams within questions.\n",
    "   * Developed a rule-based classifier using these linguistic patterns to categorize queries effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8671cdf9-65d1-4ea9-baf4-0e7ce4d00411",
   "metadata": {},
   "source": [
    "## **3. AI Classification (Tier 3):**\n",
    "\n",
    " * Implemented advanced Large Language Model (LLM)-based classification to enhance accuracy in identifying help-seeking posts.\n",
    "   * Conducted comparative analyses between the LLM-based model and baseline/pattern-based classifiers to evaluate improvements in accuracy and precision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20e1812-e112-41e7-9252-37dee81ccd23",
   "metadata": {},
   "source": [
    "## **4. Statistical Validation:**\n",
    "\n",
    "   * Performed z-tests to statistically validate and compare the performance of classification models, ensuring robustness and reliability of results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c88f2f7-3b82-4f80-8616-529188d91a41",
   "metadata": {},
   "source": [
    "## **5. Interpretation and Application of Results:**\n",
    "   * Monitored academic issues systematically by course and course version, enabling precise identification of problematic areas.\n",
    "   * Developed actionable recommendations and scalable solutions to address identified academic issues effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a436dde-ede7-411f-b3d6-de7d0b1247ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (WGU)",
   "language": "python",
   "name": "wgu-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
