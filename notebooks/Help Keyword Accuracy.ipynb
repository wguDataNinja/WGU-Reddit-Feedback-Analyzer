{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "527c2fc7-00fa-4ac0-9c59-d41beb338db5",
   "metadata": {},
   "source": [
    "Go beyond manual keywords to uncover real help-seeking language patterns from student discourse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da385d7d-4090-41fc-bba9-a33f613e2a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "\n",
    "\n",
    "# Set project root to one level above current notebook directory\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from utils.db_connection import get_db_connection\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bc8d4fb-c2ca-4a51-8bdd-02cd95a57175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18907 posts from 51 subreddits\n"
     ]
    }
   ],
   "source": [
    "# notebook_setup.py\n",
    "\n",
    "# Imports\n",
    "\n",
    "\n",
    "# Load data\n",
    "db = get_db_connection()\n",
    "df = pd.read_sql_query(\n",
    "    \"\"\"\n",
    "    SELECT p.post_id, p.subreddit_id, p.title, p.selftext, p.created_utc,\n",
    "           p.score, p.num_comments, p.permalink, s.name AS subreddit_name\n",
    "    FROM posts p\n",
    "    LEFT JOIN subreddits s ON p.subreddit_id = s.subreddit_id\n",
    "    \"\"\", db)\n",
    "db.close()\n",
    "\n",
    "print(f\"Loaded {len(df)} posts from {df['subreddit_name'].nunique()} subreddits\")\n",
    "\n",
    "# Combine and clean text\n",
    "def combine_and_clean_text(df):\n",
    "    post_text = (df['title'].fillna('') + ' ' + df['selftext'].fillna('')).str.strip()\n",
    "    return post_text.str.lower()\n",
    "\n",
    "df_clean = df.copy()\n",
    "df_clean['post_text'] = combine_and_clean_text(df_clean)\n",
    "\n",
    "# Filter for short posts <60 words for manual labeling\n",
    "df_short = df_clean[df_clean['post_text'].str.split().str.len() < 60]\n",
    "\n",
    "# NLP setup paths (adjust to your project layout)\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = Path(\"/Users/buddy/Desktop/WGU-Reddit\")\n",
    "OPENAI_MODULE_PATH = PROJECT_ROOT / \"OpenAI\"\n",
    "CONFIG_PATH = OPENAI_MODULE_PATH / \"api_config.yaml\"\n",
    "\n",
    "if str(OPENAI_MODULE_PATH) not in sys.path:\n",
    "    sys.path.insert(0, str(OPENAI_MODULE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6afd6a74-15cc-4bb0-a778-d6e11c93c81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_clean_text(df):\n",
    "    \"\"\"\n",
    "    Returns a cleaned text Series combining 'title' and 'selftext'.\n",
    "    \"\"\"\n",
    "    post_text = (df['title'].fillna('') + ' ' + df['selftext'].fillna('')).str.strip()\n",
    "    post_text = post_text.str.lower()\n",
    "    return post_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce93616d-c29c-4908-83b9-3c8f93a249b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df columns: ['post_id', 'title', 'selftext', 'post_text', 'help_truth', 'help_flag']\n"
     ]
    }
   ],
   "source": [
    "# filename: step1_clean_columns.py\n",
    "\n",
    "# Keep only necessary columns\n",
    "df = df[['post_id', 'title', 'selftext']].copy()\n",
    "\n",
    "# Combine and clean text\n",
    "def combine_and_clean_text(df):\n",
    "    return (df['title'].fillna('') + ' ' + df['selftext'].fillna('')).str.lower().str.strip()\n",
    "\n",
    "df['post_text'] = combine_and_clean_text(df)\n",
    "\n",
    "# Add columns for manual label and keyword test\n",
    "df['help_truth'] = 0\n",
    "df['help_flag'] = 0\n",
    "\n",
    "print(\"df columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea280a82-a884-41cf-937b-d88fd3dfa2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path(\"/Users/buddy/Desktop/WGU-Reddit/outputs/manual_help_truth.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3027418f-4034-4d5a-992d-4e4372e8a5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported to: /Users/buddy/Desktop/WGU-Reddit/outputs/manual_help_truth.csv\n"
     ]
    }
   ],
   "source": [
    "# filename: step2_sample_for_labeling.py\n",
    "\n",
    "# Filter for short posts\n",
    "df['word_count'] = df['post_text'].str.split().str.len()\n",
    "sample_df = df[df['word_count'] < 50].sample(n=100, random_state=42)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Define absolute path manually\n",
    "csv_path = Path(\"/Users/buddy/Desktop/WGU-Reddit/outputs/manual_help_truth.csv\")\n",
    "\n",
    "# Export sample\n",
    "sample_df[['post_id', 'post_text']].assign(help_truth=0).to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Exported to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b44a5b9-6208-4a4d-8e78-f3dd4316b4b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (WGU)",
   "language": "python",
   "name": "wgu-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
