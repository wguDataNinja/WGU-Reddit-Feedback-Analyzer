{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0fe40a-cf9b-4f5e-82b2-71405a593e12",
   "metadata": {},
   "source": [
    "# Project Purpose\n",
    "We are preparing sectioned text files from the WGU 2025_06 catalog to support NLP tasks. The main goal is **help-seeking detection** in social media posts about the school.\n",
    "\n",
    "## Why Sectioned Text?\n",
    "Institutional catalog content (e.g. degree listings, policies, tuition) is **not help-seeking by nature**. By segmenting and analyzing these texts, we can build **custom stopword/phrase lists** to exclude non-help-seeking language during model training or inference.\n",
    "\n",
    "## Workflow Summary\n",
    "1. Extract raw text from catalog PDF via `pdfplumber`.\n",
    "2. Segment by top-level catalog sections.\n",
    "3. Save each section as a `.txt` file under versioned folder (`sections/2025_06/`).\n",
    "4. Use these files to identify **institutional language** for exclusion in downstream social media NLP.\n",
    "\n",
    "## Primary NLP Task\n",
    "- **Help-seeking detection** in student or prospect-generated content (e.g. Reddit posts).\n",
    "- Note: The Kneed algorithm (reference: Satopaa, V.) is used for elbow detection in frequency distributions.\n",
    "- Note: The elbow point is sometimes one off from the obvious correct point, not critical especially for our use case. It's actually just like the graphs are for if an increase in Y value is bad, but it's actually good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea86844e-d64d-46de-a35f-2ed62f88d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports claude\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from string import punctuation\n",
    "from calendar import month_name\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "from nltk import FreqDist, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51267f71-3998-4769-b3d0-8c2b8b38ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs claude\n",
    "# Set project root to one level above current notebook directory\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "wgu_catalog = Path(\"/Users/buddy/Desktop/WGU-Reddit/WGU_catalog\")\n",
    "\n",
    "# Input file\n",
    "input_file = wgu_catalog / \"sections\" / \"2025_06\" / \"01_about_western_governors_university.txt\"\n",
    "output_dir = Path(\"/Users/buddy/Desktop/WGU-Reddit/outputs\")\n",
    "\n",
    "# Fixed JSON structure\n",
    "catalog_sections = {\n",
    "    \"Catalog_Version\": \"2025_06\",\n",
    "    \"Sections\": {\n",
    "        \"Section01\": \"01_about_western_governors_university.txt\",\n",
    "        \"Section02\": \"02_admissions.txt\",\n",
    "        \"Section03\": \"03_state_regulatory_information.txt\",\n",
    "        \"Section04\": \"04_tuition_and_financial_aid.txt\",\n",
    "        \"Section05\": \"05_academic_policies.txt\",\n",
    "        \"Section06\": \"06_standalone_courses_and_certificates.txt\",\n",
    "        \"Section07\": \"07_academic_programs.txt\",\n",
    "        \"Section08\": \"08_school_of_business_programs.txt\",\n",
    "        \"Section09\": \"09_leavitt_school_of_health_programs.txt\",\n",
    "        \"Section10\": \"10_school_of_technology_programs.txt\",\n",
    "        \"Section11\": \"11_school_of_education_programs.txt\",\n",
    "        \"Section12\": \"12_program_outcomes.txt\",\n",
    "        \"Section13\": \"13_course_descriptions.txt\",\n",
    "        \"Section14\": \"14_instructor_directory.txt\",\n",
    "        \"Section15\": \"15_certificate_programs.txt\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Build metadata list from catalog_sections\n",
    "catalog_version = catalog_sections[\"Catalog_Version\"]\n",
    "catalog_dir = wgu_catalog / \"sections\" / catalog_version\n",
    "\n",
    "section_index = []\n",
    "# Build metadata list from catalog_sections\n",
    "catalog_version = catalog_sections[\"Catalog_Version\"]\n",
    "catalog_dir = wgu_catalog / \"sections\" / catalog_version\n",
    "\n",
    "section_index = []\n",
    "for section_key, filename in catalog_sections[\"Sections\"].items():\n",
    "    file_path = catalog_dir / filename\n",
    "    section_id = filename.split(\"_\")[0]\n",
    "    section_title = \" \".join(filename.split(\"_\")[1:]).replace(\".txt\", \"\").title()\n",
    "    \n",
    "    section_index.append({\n",
    "        \"filename\": filename,\n",
    "        \"path\": str(file_path),\n",
    "        \"section_id\": section_id,\n",
    "        \"section_title\": section_title,\n",
    "        \"catalog_version\": catalog_version,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5ec92c-6bb3-4db4-85c0-780d7af467f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42d0ee-f81b-400c-b2dd-564f72eda35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9068fc5c-d334-458c-a272-e1465ddf02ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd1999f1-aede-49fd-aea9-2635797f5c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 01_about_western_governors_university.txt...\n",
      "Saved unigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/01_about_unigram.txt\n",
      "Saved unigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/01_about_unigram.png\n",
      "Saved bigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/01_about_bigram.txt\n",
      "Saved bigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/01_about_bigram.png\n",
      "Saved trigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/01_about_trigram.txt\n",
      "Saved trigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/01_about_trigram.png\n",
      "\n",
      "Processing 02_admissions.txt...\n",
      "Saved unigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/02_admissions.txt_unigram.txt\n",
      "Saved unigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/02_admissions.txt_unigram.png\n",
      "Saved bigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/02_admissions.txt_bigram.txt\n",
      "Saved bigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/02_admissions.txt_bigram.png\n",
      "Saved trigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/02_admissions.txt_trigram.txt\n",
      "Saved trigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/02_admissions.txt_trigram.png\n",
      "\n",
      "Processing 03_state_regulatory_information.txt...\n",
      "Saved unigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/03_state_unigram.txt\n",
      "Saved unigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/03_state_unigram.png\n",
      "Saved bigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/03_state_bigram.txt\n",
      "Saved bigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/03_state_bigram.png\n",
      "Saved trigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/03_state_trigram.txt\n",
      "Saved trigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/03_state_trigram.png\n",
      "\n",
      "Processing 04_tuition_and_financial_aid.txt...\n",
      "Saved unigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/04_tuition_unigram.txt\n",
      "Saved unigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/04_tuition_unigram.png\n",
      "Saved bigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/04_tuition_bigram.txt\n",
      "Saved bigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/04_tuition_bigram.png\n",
      "Saved trigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/04_tuition_trigram.txt\n",
      "Saved trigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/04_tuition_trigram.png\n",
      "\n",
      "Processing 05_academic_policies.txt...\n",
      "Saved unigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/05_academic_unigram.txt\n",
      "Saved unigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/05_academic_unigram.png\n",
      "Saved bigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/05_academic_bigram.txt\n",
      "Saved bigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/05_academic_bigram.png\n",
      "Saved trigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/05_academic_trigram.txt\n",
      "Saved trigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/05_academic_trigram.png\n",
      "\n",
      "Processing 06_standalone_courses_and_certificates.txt...\n",
      "Saved unigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/06_standalone_unigram.txt\n",
      "Saved unigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/06_standalone_unigram.png\n",
      "Saved bigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/06_standalone_bigram.txt\n",
      "Saved bigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/06_standalone_bigram.png\n",
      "Saved trigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/06_standalone_trigram.txt\n",
      "Saved trigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/06_standalone_trigram.png\n",
      "\n",
      "Processing 07_academic_programs.txt...\n",
      "Saved unigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/07_academic_unigram.txt\n",
      "Saved unigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/07_academic_unigram.png\n",
      "Saved bigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/07_academic_bigram.txt\n",
      "Saved bigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/07_academic_bigram.png\n",
      "Saved trigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/07_academic_trigram.txt\n",
      "Saved trigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/07_academic_trigram.png\n",
      "\n",
      "Processing 08_school_of_business_programs.txt...\n",
      "Saved unigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/08_school_unigram.txt\n",
      "Saved unigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/08_school_unigram.png\n",
      "Saved bigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/08_school_bigram.txt\n",
      "Saved bigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/08_school_bigram.png\n",
      "Saved trigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/08_school_trigram.txt\n",
      "Saved trigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/08_school_trigram.png\n",
      "\n",
      "Processing 09_leavitt_school_of_health_programs.txt...\n",
      "Saved unigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/09_leavitt_unigram.txt\n",
      "Saved unigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/09_leavitt_unigram.png\n",
      "Saved bigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/09_leavitt_bigram.txt\n",
      "Saved bigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/09_leavitt_bigram.png\n",
      "Saved trigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/09_leavitt_trigram.txt\n",
      "Saved trigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/09_leavitt_trigram.png\n",
      "\n",
      "Processing 10_school_of_technology_programs.txt...\n",
      "Saved unigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/10_school_unigram.txt\n",
      "Saved unigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/10_school_unigram.png\n",
      "Saved bigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/10_school_bigram.txt\n",
      "Saved bigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/10_school_bigram.png\n",
      "Saved trigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/10_school_trigram.txt\n",
      "Saved trigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/10_school_trigram.png\n",
      "\n",
      "Processing 11_school_of_education_programs.txt...\n",
      "Saved unigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/11_school_unigram.txt\n",
      "Saved unigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/11_school_unigram.png\n",
      "Saved bigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/11_school_bigram.txt\n",
      "Saved bigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/11_school_bigram.png\n",
      "Saved trigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/11_school_trigram.txt\n",
      "Saved trigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/11_school_trigram.png\n",
      "\n",
      "Processing 12_program_outcomes.txt...\n",
      "Saved unigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/12_program_unigram.txt\n",
      "Saved unigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/12_program_unigram.png\n",
      "Saved bigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/12_program_bigram.txt\n",
      "Saved bigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/12_program_bigram.png\n",
      "Saved trigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/12_program_trigram.txt\n",
      "Saved trigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/12_program_trigram.png\n",
      "\n",
      "Processing 13_course_descriptions.txt...\n",
      "Saved unigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/13_course_unigram.txt\n",
      "Saved unigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/13_course_unigram.png\n",
      "Saved bigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/13_course_bigram.txt\n",
      "Saved bigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/13_course_bigram.png\n",
      "Saved trigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/13_course_trigram.txt\n",
      "Saved trigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/13_course_trigram.png\n",
      "\n",
      "Processing 14_instructor_directory.txt...\n",
      "Saved unigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/14_instructor_unigram.txt\n",
      "Saved unigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/14_instructor_unigram.png\n",
      "Saved bigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/14_instructor_bigram.txt\n",
      "Saved bigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/14_instructor_bigram.png\n",
      "Saved trigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/14_instructor_trigram.txt\n",
      "Saved trigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/14_instructor_trigram.png\n",
      "\n",
      "Processing 15_certificate_programs.txt...\n",
      "Saved unigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/15_certificate_unigram.txt\n",
      "Saved unigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/15_certificate_unigram.png\n",
      "Saved bigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/15_certificate_bigram.txt\n",
      "Saved bigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/15_certificate_bigram.png\n",
      "Saved trigram report: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/15_certificate_trigram.txt\n",
      "Saved trigram chart: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/15_certificate_trigram.png\n"
     ]
    }
   ],
   "source": [
    "# functions claude\n",
    "def get_top_unigrams(input_path: Path, top_k: int = 50):\n",
    "    \"\"\"\n",
    "    Extract top unigrams from a catalog section using NLTK after basic cleaning.\n",
    "\n",
    "    Args:\n",
    "        input_path (Path): Path to section .txt file.\n",
    "        top_k (int): Number of top unigrams to return.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[str, int]]: List of (term, frequency) tuples.\n",
    "    \"\"\"\n",
    "    with open(input_path) as f:\n",
    "        text = f.read().lower()\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    std_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "    filtered_tokens = [\n",
    "        t for t in tokens\n",
    "        if t.isalpha() and t not in std_stopwords and len(t) > 1\n",
    "    ]\n",
    "\n",
    "    fdist = FreqDist(filtered_tokens)\n",
    "    return fdist.most_common(top_k)\n",
    "\n",
    "def get_top_bigrams(input_path: Path, top_k: int = 50):\n",
    "    \"\"\"\n",
    "    Extract top bigrams from a catalog section using NLTK after basic cleaning.\n",
    "\n",
    "    Args:\n",
    "        input_path (Path): Path to section .txt file.\n",
    "        top_k (int): Number of top bigrams to return.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[str, int]]: List of (bigram_string, frequency) tuples.\n",
    "    \"\"\"\n",
    "    with open(input_path) as f:\n",
    "        text = f.read().lower()\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    std_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "    filtered_tokens = [\n",
    "        t for t in tokens\n",
    "        if t.isalpha() and t not in std_stopwords and len(t) > 1\n",
    "    ]\n",
    "\n",
    "    bigram_tokens = bigrams(filtered_tokens)\n",
    "    bigram_strings = [' '.join(pair) for pair in bigram_tokens]\n",
    "\n",
    "    fdist = FreqDist(bigram_strings)\n",
    "    return fdist.most_common(top_k)\n",
    "\n",
    "def get_top_trigrams(input_path: Path, top_k: int = 50):\n",
    "    \"\"\"\n",
    "    Extract top trigrams from a catalog section using NLTK after basic cleaning.\n",
    "\n",
    "    Args:\n",
    "        input_path (Path): Path to section .txt file.\n",
    "        top_k (int): Number of top trigrams to return.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[str, int]]: List of (trigram_string, frequency) tuples.\n",
    "    \"\"\"\n",
    "    with open(input_path) as f:\n",
    "        text = f.read().lower()\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    std_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "    filtered_tokens = [\n",
    "        t for t in tokens\n",
    "        if t.isalpha() and t not in std_stopwords and len(t) > 1\n",
    "    ]\n",
    "\n",
    "    trigram_tokens = trigrams(filtered_tokens)\n",
    "    trigram_strings = [' '.join(tg) for tg in trigram_tokens]\n",
    "\n",
    "    fdist = FreqDist(trigram_strings)\n",
    "    return fdist.most_common(top_k)\n",
    "\n",
    "def convert_chart_title(top_terms, section_name=\"01 About Section\", catalog_version=\"2025_06\"):\n",
    "    \"\"\"\n",
    "    Return a prettified chart title for a frequency rank plot.\n",
    "\n",
    "    Args:\n",
    "        top_terms (list): List of (term or phrase, frequency) tuples.\n",
    "        section_name (str): Catalog section name.\n",
    "        catalog_version (str): Catalog version in 'YYYY_MM' format.\n",
    "\n",
    "    Returns:\n",
    "        str: Prettified chart title.\n",
    "    \"\"\"\n",
    "    # Convert catalog version\n",
    "    year, month = catalog_version.split(\"_\")\n",
    "    month_str = month_name[int(month)]\n",
    "    pretty_version = f\"{month_str} {year}\"\n",
    "\n",
    "    # Detect n-gram type\n",
    "    term = top_terms[0][0] if top_terms else \"\"\n",
    "    n = len(term.split())\n",
    "    label = {1: \"Unigram\", 2: \"Bigram\", 3: \"Trigram\"}.get(n, f\"{n}-gram\")\n",
    "\n",
    "    return f\"WGU Catalog {pretty_version} {label} Frequency Rank with Elbow\"\n",
    "\n",
    "def plot_elbow_with_knee(top_terms: list[tuple[str, int]], section_name: str,\n",
    "                          catalog_version: str = \"2025_06\",\n",
    "                          curve: str = 'convex', direction: str = 'decreasing',\n",
    "                          highlight: bool = True):\n",
    "    \"\"\"\n",
    "    Plot unigram frequency rank with elbow detection using KneeLocator.\n",
    "\n",
    "    Args:\n",
    "        top_terms (list): List of (term, frequency) tuples.\n",
    "        section_name (str): Name of the section for labeling.\n",
    "        catalog_version (str): Catalog version in 'YYYY_MM' format.\n",
    "        curve (str): Shape of curve ('convex' or 'concave').\n",
    "        direction (str): 'increasing' or 'decreasing'.\n",
    "        highlight (bool): Whether to highlight the elbow.\n",
    "    \"\"\"\n",
    "    freqs = [freq for _, freq in top_terms]\n",
    "    ranks = list(range(1, len(freqs) + 1))\n",
    "\n",
    "    kneedle = KneeLocator(ranks, freqs, curve=curve, direction=direction)\n",
    "    elbow_rank = kneedle.knee\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ranks, freqs, marker='o')\n",
    "\n",
    "    if highlight and elbow_rank:\n",
    "        plt.scatter(elbow_rank, freqs[elbow_rank - 1], s=225,\n",
    "                    edgecolors='red', facecolors='none', linewidths=2, zorder=5)\n",
    "        plt.annotate('Elbow', xy=(elbow_rank, freqs[elbow_rank - 1]),\n",
    "                     xytext=(elbow_rank + 1, freqs[elbow_rank - 1] + 2),\n",
    "                     arrowprops=dict(arrowstyle='->'))\n",
    "\n",
    "    title = convert_chart_title(top_terms, section_name, catalog_version)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Rank')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(ranks, [term for term, _ in top_terms], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if elbow_rank:\n",
    "        term, freq = top_terms[elbow_rank - 1]\n",
    "        print(f\"Elbow at rank: {elbow_rank}, term: '{term}', frequency: {freq}\")\n",
    "    else:\n",
    "        print(\"No elbow detected.\")\n",
    "\n",
    "def save_elbow_chart(top_terms: list[tuple[str, int]], section_name: str,\n",
    "                     catalog_version: str, output_path: Path,\n",
    "                     curve: str = 'convex', direction: str = 'decreasing',\n",
    "                     highlight: bool = True):\n",
    "    \"\"\"\n",
    "    Save elbow chart to file instead of showing it.\n",
    "    \"\"\"\n",
    "    freqs = [freq for _, freq in top_terms]\n",
    "    ranks = list(range(1, len(freqs) + 1))\n",
    "\n",
    "    kneedle = KneeLocator(ranks, freqs, curve=curve, direction=direction)\n",
    "    elbow_rank = kneedle.knee\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ranks, freqs, marker='o')\n",
    "\n",
    "    if highlight and elbow_rank:\n",
    "        plt.scatter(elbow_rank, freqs[elbow_rank - 1], s=225,\n",
    "                    edgecolors='red', facecolors='none', linewidths=2, zorder=5)\n",
    "        plt.annotate('Elbow', xy=(elbow_rank, freqs[elbow_rank - 1]),\n",
    "                     xytext=(elbow_rank + 1, freqs[elbow_rank - 1] + 2),\n",
    "                     arrowprops=dict(arrowstyle='->'))\n",
    "\n",
    "    title = convert_chart_title(top_terms, section_name, catalog_version)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Rank')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(ranks, [term for term, _ in top_terms], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return elbow_rank\n",
    "\n",
    "def print_ngram_report(input_file: Path, catalog_version=\"2025_06\"):\n",
    "    from calendar import month_name\n",
    "\n",
    "    section_name = input_file.stem\n",
    "    year, month = catalog_version.split(\"_\")\n",
    "    month_str = month_name[int(month)]\n",
    "\n",
    "    # Get n-grams\n",
    "    top_unigrams = get_top_unigrams(input_file, top_k=50)\n",
    "    top_bigrams = get_top_bigrams(input_file, top_k=50)\n",
    "    top_trigrams = get_top_trigrams(input_file, top_k=50)\n",
    "\n",
    "    # Compute elbows\n",
    "    unigram_elbow = get_elbow_cutoff(top_unigrams)\n",
    "\n",
    "    # Header\n",
    "    print(f\"WGU Catalog {month_str} {year} Unigrams (Elbow: {unigram_elbow})\\n\")\n",
    "\n",
    "    # Print top N based on elbow\n",
    "    def print_top(title, ngrams):\n",
    "        elbow = get_elbow_cutoff(ngrams)\n",
    "        print(f\"{title} (Top {elbow}, based on elbow point)\")\n",
    "        for i, (term, freq) in enumerate(ngrams[:elbow], start=1):\n",
    "            print(f\"{i}. {term} ({freq})\")\n",
    "        print()\n",
    "\n",
    "    print_top(\"Unigrams\", top_unigrams)\n",
    "    print_top(\"Bigrams\", top_bigrams)\n",
    "    print_top(\"Trigrams\", top_trigrams)\n",
    "\n",
    "    # Plot charts\n",
    "    plot_elbow_with_knee(top_unigrams, section_name, catalog_version)\n",
    "    plot_elbow_with_knee(top_bigrams, section_name, catalog_version)\n",
    "    plot_elbow_with_knee(top_trigrams, section_name, catalog_version)\n",
    "\n",
    "def get_elbow_cutoff(top_terms: list[tuple[str, int]]) -> int:\n",
    "    freqs = [freq for _, freq in top_terms]\n",
    "    ranks = list(range(1, len(freqs) + 1))\n",
    "    kneedle = KneeLocator(ranks, freqs, curve='convex', direction='decreasing')\n",
    "    return kneedle.knee or len(top_terms)\n",
    "\n",
    "def save_ngram_report(entry: dict, output_root: Path = Path(\"/Users/buddy/Desktop/WGU-Reddit/outputs\")):\n",
    "    \"\"\"\n",
    "    Save n-gram reports and charts to files.\n",
    "    \n",
    "    Args:\n",
    "        entry (dict): Dictionary containing file info with keys: filename, path, section_id, catalog_version\n",
    "        output_root (Path): Root directory for outputs\n",
    "    \"\"\"\n",
    "    input_file = Path(entry[\"path\"])\n",
    "    section_name = input_file.stem\n",
    "    catalog_version = entry[\"catalog_version\"]\n",
    "    section_id = entry[\"section_id\"]\n",
    "    \n",
    "    # Create output directory structure\n",
    "    output_dir = output_root / catalog_version\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Extract first word after section number for filename\n",
    "    filename_parts = entry[\"filename\"].split(\"_\")\n",
    "    first_word = filename_parts[1] if len(filename_parts) > 1 else \"section\"\n",
    "    \n",
    "    # Get n-grams\n",
    "    top_unigrams = get_top_unigrams(input_file, top_k=50)\n",
    "    top_bigrams = get_top_bigrams(input_file, top_k=50)\n",
    "    top_trigrams = get_top_trigrams(input_file, top_k=50)\n",
    "    \n",
    "    # Generate reports and save\n",
    "    ngram_types = [\n",
    "        (\"unigram\", top_unigrams),\n",
    "        (\"bigram\", top_bigrams),\n",
    "        (\"trigram\", top_trigrams)\n",
    "    ]\n",
    "    \n",
    "    for ngram_type, ngram_data in ngram_types:\n",
    "        # Create filenames\n",
    "        report_filename = f\"{section_id}_{first_word}_{ngram_type}.txt\"\n",
    "        chart_filename = f\"{section_id}_{first_word}_{ngram_type}.png\"\n",
    "        \n",
    "        report_path = output_dir / report_filename\n",
    "        chart_path = output_dir / chart_filename\n",
    "        \n",
    "        # Save report\n",
    "        elbow = get_elbow_cutoff(ngram_data)\n",
    "        year, month = catalog_version.split(\"_\")\n",
    "        month_str = month_name[int(month)]\n",
    "        \n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(f\"WGU Catalog {month_str} {year} {ngram_type.title()}s (Elbow: {elbow})\\n\")\n",
    "            f.write(f\"Section: {section_name}\\n\\n\")\n",
    "            f.write(f\"{ngram_type.title()}s (Top {elbow}, based on elbow point)\\n\")\n",
    "            \n",
    "            for i, (term, freq) in enumerate(ngram_data[:elbow], start=1):\n",
    "                f.write(f\"{i}. {term} ({freq})\\n\")\n",
    "        \n",
    "        # Save chart\n",
    "        elbow_rank = save_elbow_chart(ngram_data, section_name, catalog_version, chart_path)\n",
    "        \n",
    "        print(f\"Saved {ngram_type} report: {report_path}\")\n",
    "        print(f\"Saved {ngram_type} chart: {chart_path}\")\n",
    "\n",
    "# Process all sections\n",
    "def process_all_sections():\n",
    "    \"\"\"Process all sections in the catalog\"\"\"\n",
    "    for entry in section_index:\n",
    "        print(f\"\\nProcessing {entry['filename']}...\")\n",
    "        save_ngram_report(entry)\n",
    "\n",
    "# Run for all sections\n",
    "process_all_sections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbf0c7c-a8f3-4e1a-9ea6-64160b66aebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1a3295-9a0b-4a4a-a1db-33fa31379bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab58866-8807-43a1-b738-a149d341d1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f5beeab-5c1c-4ec4-ab12-197da80f7493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NLTK Unigram Processor ===\n",
      "Input directory: /Users/buddy/Desktop/WGU-Reddit/outputs/2025_06\n",
      "Directory exists: True\n",
      "Found 15 unigram files:\n",
      "  - 01_about_unigram.txt\n",
      "  - 02_admissions.txt_unigram.txt\n",
      "  - 03_state_unigram.txt\n",
      "  - 04_tuition_unigram.txt\n",
      "  - 05_academic_unigram.txt\n",
      "  - 06_standalone_unigram.txt\n",
      "  - 07_academic_unigram.txt\n",
      "  - 08_school_unigram.txt\n",
      "  - 09_leavitt_unigram.txt\n",
      "  - 10_school_unigram.txt\n",
      "  - 11_school_unigram.txt\n",
      "  - 12_program_unigram.txt\n",
      "  - 13_course_unigram.txt\n",
      "  - 14_instructor_unigram.txt\n",
      "  - 15_certificate_unigram.txt\n",
      "\n",
      "Processing 15 files:\n",
      "\n",
      "Processing: 01_about_unigram.txt\n",
      "  - Extracted 9 unique words\n",
      "  - Sample words: ['program', 'academic', 'western', 'governors', 'student']\n",
      "\n",
      "Processing: 02_admissions.txt_unigram.txt\n",
      "  - Extracted 8 unique words\n",
      "  - Sample words: ['state', 'program', 'nursing', 'requirements', 'students']\n",
      "\n",
      "Processing: 03_state_unigram.txt\n",
      "  - Extracted 2 unique words\n",
      "  - Sample words: ['state', 'university']\n",
      "\n",
      "Processing: 04_tuition_unigram.txt\n",
      "  - Extracted 11 unique words\n",
      "  - Sample words: ['program', 'payment', 'term', 'aid', 'may']\n",
      "\n",
      "Processing: 05_academic_unigram.txt\n",
      "  - Extracted 10 unique words\n",
      "  - Sample words: ['board', 'program', 'wgu', 'may', 'term']\n",
      "\n",
      "Processing: 06_standalone_unigram.txt\n",
      "  - Extracted 11 unique words\n",
      "  - Sample words: ['management', 'learn', 'project', 'use', 'course']\n",
      "\n",
      "Processing: 07_academic_unigram.txt\n",
      "  - Extracted 7 unique words\n",
      "  - Sample words: ['information', 'management', 'nursing', 'science', 'education']\n",
      "\n",
      "Processing: 08_school_unigram.txt\n",
      "  - Extracted 12 unique words\n",
      "  - Sample words: ['acct', 'accounting', 'environment', 'management', 'bus']\n",
      "\n",
      "Processing: 09_leavitt_unigram.txt\n",
      "  - Extracted 8 unique words\n",
      "  - Sample words: ['healthcare', 'program', 'hlth', 'practice', 'nursing']\n",
      "\n",
      "Processing: 10_school_unigram.txt\n",
      "  - Extracted 10 unique words\n",
      "  - Sample words: ['management', 'cloud', 'science', 'security', 'data']\n",
      "\n",
      "Processing: 11_school_unigram.txt\n",
      "  - Extracted 8 unique words\n",
      "  - Sample words: ['program', 'teaching', 'educ', 'learning', 'methods']\n",
      "\n",
      "Processing: 12_program_unigram.txt\n",
      "  - Extracted 4 unique words\n",
      "  - Sample words: ['data', 'applies', 'learning', 'graduate']\n",
      "\n",
      "Processing: 13_course_unigram.txt\n",
      "  - Extracted 9 unique words\n",
      "  - Sample words: ['management', 'learners', 'learning', 'course', 'data']\n",
      "\n",
      "Processing: 14_instructor_unigram.txt\n",
      "  - Extracted 6 unique words\n",
      "  - Sample words: ['state', 'doctorate', 'phd', 'master', 'degree']\n",
      "\n",
      "Processing: 15_certificate_unigram.txt\n",
      "  - Extracted 1 unique words\n",
      "  - Sample words: ['course']\n",
      "\n",
      "=== RESULTS ===\n",
      "Files processed: 15\n",
      "Total words processed: 116\n",
      "Unique words in catalog_stopwords: 68\n",
      "\n",
      "First 10 words in catalog_stopwords:\n",
      "  1. academic\n",
      "  2. accounting\n",
      "  3. acct\n",
      "  4. advanced\n",
      "  5. aid\n",
      "  6. also\n",
      "  7. applies\n",
      "  8. board\n",
      "  9. bus\n",
      "  10. business\n",
      "\n",
      "Last 10 words in catalog_stopwords:\n",
      "  59. state\n",
      "  60. student\n",
      "  61. students\n",
      "  62. teaching\n",
      "  63. term\n",
      "  64. tuition\n",
      "  65. university\n",
      "  66. use\n",
      "  67. western\n",
      "  68. wgu\n",
      "\n",
      "catalog_stopwords set contains: 68 unique words\n",
      "Set ready for use!\n"
     ]
    }
   ],
   "source": [
    "# configs claude\n",
    "# Set project root to one level above current notebook directory\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))\n",
    "unigram_input_dir = Path(\"/Users/buddy/Desktop/WGU-Reddit/outputs/2025_06/\")\n",
    "\n",
    "def identify_unigram_files(directory):\n",
    "    \"\"\"\n",
    "    Identify files that:\n",
    "    1. Have .txt extension (handling duplicate .txt cases)\n",
    "    2. Contain 'unigram' in the filename\n",
    "    \"\"\"\n",
    "    unigram_files = []\n",
    "    \n",
    "    for file_path in directory.glob(\"*\"):\n",
    "        filename = file_path.name\n",
    "        \n",
    "        # Check if file has .txt extension (handle duplicate .txt)\n",
    "        if filename.endswith('.txt'):\n",
    "            # Check if filename contains 'unigram'\n",
    "            if 'unigram' in filename.lower():\n",
    "                unigram_files.append(file_path)\n",
    "    \n",
    "    return sorted(unigram_files)\n",
    "\n",
    "def extract_unigrams_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Extract unigrams from a single file using regex pattern matching\n",
    "    Returns a set of unique words and word count\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        # Regex pattern to match numbered lines with word and count\n",
    "        # Pattern: number. word (count)\n",
    "        pattern = r'^\\d+\\.\\s+([a-zA-Z]+)\\s+\\(\\d+\\)$'\n",
    "        \n",
    "        words = set()\n",
    "        for line in content.split('\\n'):\n",
    "            match = re.match(pattern, line.strip())\n",
    "            if match:\n",
    "                word = match.group(1).lower()  # Extract word and convert to lowercase\n",
    "                words.add(word)\n",
    "        \n",
    "        return words, len(words)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return set(), 0\n",
    "\n",
    "def process_unigram_files(directory, test_mode=True):\n",
    "    \"\"\"\n",
    "    Process unigram files and combine words into catalog_stopwords set\n",
    "    \"\"\"\n",
    "    # Identify files\n",
    "    unigram_files = identify_unigram_files(directory)\n",
    "    \n",
    "    if not unigram_files:\n",
    "        print(\"No unigram files found!\")\n",
    "        return set(), 0, 0\n",
    "    \n",
    "    print(f\"Found {len(unigram_files)} unigram files:\")\n",
    "    for file_path in unigram_files:\n",
    "        print(f\"  - {file_path.name}\")\n",
    "    \n",
    "    # Initialize combined set\n",
    "    catalog_stopwords = set()\n",
    "    total_words_processed = 0\n",
    "    files_processed = 0\n",
    "    \n",
    "    # Process files (test mode: only first file, otherwise all files)\n",
    "    files_to_process = [unigram_files[0]] if test_mode else unigram_files\n",
    "    \n",
    "    print(f\"\\nProcessing {'1 file (TEST MODE)' if test_mode else f'{len(files_to_process)} files'}:\")\n",
    "    \n",
    "    for file_path in files_to_process:\n",
    "        print(f\"\\nProcessing: {file_path.name}\")\n",
    "        \n",
    "        words, word_count = extract_unigrams_from_file(file_path)\n",
    "        \n",
    "        if words:\n",
    "            catalog_stopwords.update(words)\n",
    "            total_words_processed += word_count\n",
    "            files_processed += 1\n",
    "            \n",
    "            print(f\"  - Extracted {word_count} unique words\")\n",
    "            print(f\"  - Sample words: {list(words)[:5]}\")  # Show first 5 words as sample\n",
    "        else:\n",
    "            print(f\"  - No words extracted from {file_path.name}\")\n",
    "    \n",
    "    return catalog_stopwords, files_processed, total_words_processed\n",
    "\n",
    "# Run the processor\n",
    "print(\"=== NLTK Unigram Processor ===\")\n",
    "print(f\"Input directory: {unigram_input_dir}\")\n",
    "print(f\"Directory exists: {unigram_input_dir.exists()}\")\n",
    "\n",
    "# Process files (TEST MODE - only first file)\n",
    "catalog_stopwords, files_processed, total_words_processed = process_unigram_files(\n",
    "    unigram_input_dir, \n",
    "    test_mode=False  # Change to False to process all files\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "# Example output from the unigram script (what catalog_stopwords would contain)\n",
    "# Based on your file examples, it would extract words like:\n",
    "example_catalog_stopwords = {\n",
    "    'university', 'state', 'term', 'tuition', 'student', 'students', \n",
    "    'wgu', 'financial', 'aid', 'program', 'per', 'may', 'payment',\n",
    "    'course', 'degree', 'credit', 'academic', 'enrollment', 'requirement'\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_stopwords = pd.DataFrame(list(example_catalog_stopwords), columns=['word'])\n",
    "\n",
    "print(df_stopwords.head())\n",
    "print(f\"\\nDataFrame shape: {df_stopwords.shape}\")\n",
    "print(f\"\\n=== RESULTS ===\")\n",
    "print(f\"Files processed: {files_processed}\")\n",
    "print(f\"Total words processed: {total_words_processed}\")\n",
    "print(f\"Unique words in catalog_stopwords: {len(catalog_stopwords)}\")\n",
    "\n",
    "if catalog_stopwords:\n",
    "    print(f\"\\nFirst 10 words in catalog_stopwords:\")\n",
    "    sorted_words = sorted(catalog_stopwords)\n",
    "    for i, word in enumerate(sorted_words[:10]):\n",
    "        print(f\"  {i+1}. {word}\")\n",
    "    \n",
    "    print(f\"\\nLast 10 words in catalog_stopwords:\")\n",
    "    for i, word in enumerate(sorted_words[-10:]):\n",
    "        print(f\"  {len(sorted_words)-9+i}. {word}\")\n",
    "\n",
    "# Display catalog_stopwords set for verification\n",
    "print(f\"\\ncatalog_stopwords set contains: {len(catalog_stopwords)} unique words\")\n",
    "print(\"Set ready for use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "650f1bb0-d29d-4baf-b8db-8eafe5637b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram_processor.py\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def identify_unigram_files(directory):\n",
    "    return sorted([\n",
    "        f for f in directory.glob(\"*.txt\")\n",
    "        if 'unigram' in f.name.lower()\n",
    "    ])\n",
    "\n",
    "def extract_unigrams_from_file(file_path):\n",
    "    pattern = r'^\\d+\\.\\s+([a-zA-Z]+)\\s+\\(\\d+\\)$'\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.read().splitlines()\n",
    "    return {\n",
    "        match.group(1).lower()\n",
    "        for line in lines\n",
    "        if (match := re.match(pattern, line.strip()))\n",
    "    }\n",
    "\n",
    "def combine_unigrams(directory):\n",
    "    \"\"\"\n",
    "    Combine all unigram words from unigram .txt files in the directory\n",
    "    Returns a set of stopwords to be added to NLTK stopwords\n",
    "    \"\"\"\n",
    "    unigram_files = identify_unigram_files(directory)\n",
    "    stopwords_set = set()\n",
    "    for file_path in unigram_files:\n",
    "        stopwords_set.update(extract_unigrams_from_file(file_path))\n",
    "    return stopwords_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47d9f4da-98af-4eb3-b150-f18fcebdd999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_institutional_stopwords.py\n",
    "\n",
    "# Generate stopwords from unigrams\n",
    "institutional_stopwords = combine_unigrams(unigram_input_dir)\n",
    "\n",
    "# Save to file\n",
    "output_file = unigram_input_dir.parent / \"institutional_stopwords.txt\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for word in sorted(institutional_stopwords):\n",
    "        f.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cef72c-c6b3-4755-b7fd-e8a57b1d4afa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (WGU)",
   "language": "python",
   "name": "wgu-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
