{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b06cb2-dcca-4ebf-8a61-85359bec1fe8",
   "metadata": {},
   "source": [
    "### ðŸ“Œ Question Detection via Keyword + Pattern Mining\n",
    "\n",
    "**1. Load & Prep**\n",
    "\n",
    "- Load posts from DB\n",
    "- Combine & lowercase `title + selftext` â†’ `post_text`\n",
    "- Filter to posts with `?`, add `word_count`\n",
    "\n",
    "**2. Analyze Question Length**\n",
    "\n",
    "- Plot histogram of word count for `?` posts\n",
    "- Focus on short posts (< 40 words)\n",
    "\n",
    "**3. Highlight & Preview**\n",
    "\n",
    "- Highlight `?` in short posts\n",
    "- Display table to explore language manually\n",
    "\n",
    "**4. Extract Question Sentences**\n",
    "\n",
    "- Use NLTK `sent_tokenize()` on `post_text`\n",
    "- Collect sentences ending with `?`\n",
    "\n",
    "**5. Analyze Question Starters**\n",
    "\n",
    "- Extract first word of question sentences\n",
    "- Count frequency, display top starters\n",
    "\n",
    "**6. Keyword Match Evaluation**\n",
    "\n",
    "- Load manual labels (`help_truth`)\n",
    "- Test accuracy for each starter keyword (e.g. \"how\", \"what\", \"can\")\n",
    "\n",
    "**7. Discover Question Bigrams**\n",
    "\n",
    "- For each starter, extract top bigrams (`starter next_word`)\n",
    "- Display top 10 bigrams per starter\n",
    "\n",
    "**8. Compare Top Starters**\n",
    "\n",
    "- Compare bigram distributions for \"how\" vs \"what\"\n",
    "- Identify overlap in starter-next patterns\n",
    "\n",
    "**9. First Word â†’ Next Word Patterns**\n",
    "\n",
    "- For top 40 question starters:\n",
    "  - Show top 3 most frequent next words\n",
    "  - Reveal syntactic patterns in real posts\n",
    "\n",
    "**10. Rule Discovery**\n",
    "\n",
    "- Find most common starting bigrams in short `?` posts\n",
    "- Use for rule-based detection (e.g. wh-words, auxiliary verbs, \"if\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdc8090c-a172-4fda-871a-51115d05858c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>post_type</th>\n",
       "      <th>permalink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1k6jeqd</td>\n",
       "      <td>Examity</td>\n",
       "      <td>Iâ€™m curious as to how examity works. I read on...</td>\n",
       "      <td>text</td>\n",
       "      <td>/r/WGU/comments/1k6jeqd/examity/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1k6j88n</td>\n",
       "      <td>Any Canadians here pursuing software developme...</td>\n",
       "      <td>Iâ€™m considering getting a software development...</td>\n",
       "      <td>text</td>\n",
       "      <td>/r/WGU/comments/1k6j88n/any_canadians_here_pur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1k6iufu</td>\n",
       "      <td>ANYONE IN D277</td>\n",
       "      <td>Iâ€™m half way through Front End Web Development...</td>\n",
       "      <td>text</td>\n",
       "      <td>/r/WGU/comments/1k6iufu/anyone_in_d277/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                              title  \\\n",
       "0  1k6jeqd                                            Examity   \n",
       "1  1k6j88n  Any Canadians here pursuing software developme...   \n",
       "2  1k6iufu                                     ANYONE IN D277   \n",
       "\n",
       "                                            selftext post_type  \\\n",
       "0  Iâ€™m curious as to how examity works. I read on...      text   \n",
       "1  Iâ€™m considering getting a software development...      text   \n",
       "2  Iâ€™m half way through Front End Web Development...      text   \n",
       "\n",
       "                                           permalink  \n",
       "0                   /r/WGU/comments/1k6jeqd/examity/  \n",
       "1  /r/WGU/comments/1k6j88n/any_canadians_here_pur...  \n",
       "2            /r/WGU/comments/1k6iufu/anyone_in_d277/  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ready. Project root: /Users/buddy/Desktop/WGU-Reddit, Data folder: /Users/buddy/Desktop/WGU-Reddit/notebooks/data, Loaded 19001 rows.\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Setup + DB test (Jupyter)\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Add project root to sys.path\n",
    "project_root = Path().resolve().parent  # assumes notebook is in notebooks/\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from utils.db_connection import get_db_connection\n",
    "from utils.db_connection_new import load_posts_dataframe\n",
    "\n",
    "data_folder = project_root / \"notebooks\" / \"data\"\n",
    "\n",
    "# Load posts from DB\n",
    "df = load_posts_dataframe()\n",
    "display(df.head(3))  # optional\n",
    "\n",
    "print(f\"âœ… Ready. Project root: {project_root}, Data folder: {data_folder}, Loaded {len(df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09e7bd0-b931-415b-85d8-ff93f33c372e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b7d4a7-8cbf-4118-b458-cf3264740d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 Imports\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "\n",
    "\n",
    "# Set project root to one level above current notebook directory\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from utils.db_connection import get_db_connection\n",
    "\n",
    "# cell 2 Load data\n",
    "db = get_db_connection()\n",
    "df = pd.read_sql_query(\n",
    "    \"\"\"\n",
    "    SELECT p.post_id, p.subreddit_id, p.title, p.selftext, p.created_utc,\n",
    "           p.score, p.num_comments, p.permalink, s.name AS subreddit_name\n",
    "    FROM posts p\n",
    "    LEFT JOIN subreddits s ON p.subreddit_id = s.subreddit_id\n",
    "    \"\"\", db)\n",
    "db.close()\n",
    "\n",
    "\n",
    "# Summary metrics\n",
    "total_posts = len(df)\n",
    "unique_subs = df['subreddit_name'].nunique()\n",
    "\n",
    "\n",
    "print(f\"Loaded {total_posts} posts from {unique_subs} subreddits\")\n",
    "print(\"Total posts in df_clean:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1733ed33-5b5c-4402-8238-7d78c9f3b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_clean_text(df):\n",
    "    \"\"\"\n",
    "    Returns a cleaned text Series combining 'title' and 'selftext'.\n",
    "    \"\"\"\n",
    "    post_text = (df['title'].fillna('') + ' ' + df['selftext'].fillna('')).str.strip()\n",
    "    post_text = post_text.str.lower()\n",
    "    return post_text\n",
    "\n",
    "df_clean['post_text'] = combine_and_clean_text(df_clean).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a39af4-f3cf-42d7-b45e-d0b619360d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and clean with updated column name 'post_text'\n",
    "df_clean = df.copy()\n",
    "df_clean['post_text'] = combine_and_clean_text(df_clean)\n",
    "\n",
    "print(\"df columns:\", df.columns.tolist())\n",
    "print(\"df_clean columns:\", df_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401353a5-3eeb-4646-b343-e84bd007eeba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541e3e53-0fc0-40a9-86be-4a67e6b513a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb940b-49e9-4b41-bce3-427cdc29e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename: step4_question_post_stats.py\n",
    "\n",
    "# Add word count column\n",
    "df_q = df_clean[df_clean['post_text'].str.contains(r'\\?', na=False)].copy()\n",
    "df_q['word_count'] = df_questions['post_text'].str.split().str.len()\n",
    "\n",
    "# Describe stats\n",
    "stats = df_q['word_count'].describe()\n",
    "\n",
    "print(\"Post Length Stats (posts with '?'):\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bd9d36-7a48-437c-88c8-ea5cf592fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename: step4_question_post_hist.py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histogram of word counts\n",
    "plt.figure(figsize=(8, 5))\n",
    "df_questions['word_count'].plot.hist(bins=50, range=(0, 500))\n",
    "plt.title(\"Word Count Distribution (Posts with '?')\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()# filename: step4_question_post_bin_counts.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define bins: 0â€“10, 11â€“20, ..., 91â€“100\n",
    "bins = list(range(0, 110, 10))\n",
    "labels = [f\"{b+1}-{b+10}\" for b in bins[:-1]]\n",
    "\n",
    "# Bin the word counts\n",
    "df_q['bin'] = pd.cut(df_questions['word_count'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "# Count posts in each bin\n",
    "bin_counts = df_q['bin'].value_counts().sort_index()\n",
    "\n",
    "# Display\n",
    "print(\"Word Count Bin Frequencies (Posts with '?'):\")\n",
    "for label, count in bin_counts.items():\n",
    "    print(f\"{label:>8}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37767a5-b0c4-42f5-8386-14ac34f59806",
   "metadata": {},
   "source": [
    "**observation:** Most posts with a `?` are short. We will focus on short posts, <40 words to search for help-seeking patters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796086f7-cb48-4ff0-957b-3b4329d95875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe93ff-fd92-48b6-afc8-a54750cc8c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94e6125-2afc-4e89-85ed-71912d969baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fa0f32-0ebb-4219-bfaf-16c66aed14d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename: step4_preview_short_questions.py\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Filter short posts (< 40 words) with '?'\n",
    "df_q = df_q[df_q['word_count'] < 40].copy()\n",
    "\n",
    "# Highlight question marks\n",
    "def highlight_question(text):\n",
    "    return text.replace('?', '<mark>?</mark>') if isinstance(text, str) else text\n",
    "\n",
    "# Apply highlighting\n",
    "preview = df_q[['title', 'selftext']].copy()\n",
    "preview['title'] = preview['title'].str.slice(0, 100).apply(highlight_question)\n",
    "preview['selftext'] = preview['selftext'].str.slice(0, 200).apply(highlight_question)\n",
    "\n",
    "# Render scrollable table\n",
    "html = preview.to_html(index=False, escape=False)\n",
    "display(HTML(f\"\"\"\n",
    "<h4>Short Posts (â‰¤ 40 words, contains '?')</h4>\n",
    "<div style=\"max-height:500px; overflow:auto; border:1px solid #ccc; padding:10px; font-family:monospace; font-size:12px\">\n",
    "{html}\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab25ad0-67ef-42a9-b47d-1dc239c83584",
   "metadata": {},
   "source": [
    "**observation** many questions start with a who, what, when where why or how. Before doing NLTK NGrams, examine the first word. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd700e4-88a5-4120-8ee3-c6f66685ce34",
   "metadata": {},
   "source": [
    "### See if we can split sentences in order to find ones ending with `?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d723d60-5d52-4db6-9b9a-2527d304e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename: step4_question_sentences_from_df_q.py\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Collect question-ending sentences from df_q\n",
    "question_sentences = []\n",
    "\n",
    "for text in df_q['post_text'].dropna():\n",
    "    sentences = sent_tokenize(text)\n",
    "    for s in sentences:\n",
    "        if s.strip().endswith('?'):\n",
    "            question_sentences.append(s.strip())\n",
    "\n",
    "# Print stats and sample\n",
    "print(f\"Total sentences ending with '?': {len(question_sentences)}\\n\")\n",
    "\n",
    "print(\"Sample sentences ending with '?':\\n\")\n",
    "for s in question_sentences[:10]:\n",
    "    print(\"-\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b25770f-1da2-473c-a654-80c3f48d8fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect question-ending sentences from df_q\n",
    "question_sentences = []\n",
    "\n",
    "for text in df_q['post_text'].dropna():\n",
    "    sentences = sent_tokenize(text)\n",
    "    for s in sentences:\n",
    "        if s.strip().endswith('?'):\n",
    "            question_sentences.append(s.strip())\n",
    "\n",
    "# Print stats and sample\n",
    "print(f\"Total sentences ending with '?': {len(question_sentences)}\\n\")\n",
    "\n",
    "print(\"Sample sentences ending with '?':\\n\")\n",
    "for s in question_sentences[:10]:\n",
    "    print(\"-\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6ea2f-fa81-4260-a142-01a1a17a23a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename: step4_question_starters.py\n",
    "\n",
    "def display_list(items, cols=8):\n",
    "    rows = (len(items) + cols - 1) // cols\n",
    "    columns = [[] for _ in range(cols)]\n",
    "    for i, item in enumerate(items):\n",
    "        columns[i % cols].append(item)\n",
    "    max_len = max(len(col) for col in columns)\n",
    "    for col in columns:\n",
    "        col.extend([\"\"] * (max_len - len(col)))\n",
    "    df = pd.DataFrame({f'Col {i+1}': columns[i] for i in range(cols)})\n",
    "    display(df)\n",
    "\n",
    "# Collect first words of question-ending sentences from df_q\n",
    "first_words = []\n",
    "\n",
    "for text in df_q['post_text'].dropna():\n",
    "    for s in sent_tokenize(text):\n",
    "        s = s.strip()\n",
    "        if s.endswith('?'):\n",
    "            words = word_tokenize(s)\n",
    "            if words:\n",
    "                first_words.append(words[0].lower())\n",
    "\n",
    "# Frequency count\n",
    "counts = Counter(first_words)\n",
    "top_items = counts.most_common(100)\n",
    "formatted = [f\"{word}:{count}\" for word, count in top_items]\n",
    "\n",
    "# Display\n",
    "display_list(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094c1f8a-a9f5-4c99-ba42-55e1cd8f8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the test function\n",
    "def run_keyword_match_test(df_clean, truth_csv_path='data/manual_help_truth.csv', keywords=None):\n",
    "    if keywords is None:\n",
    "        keywords = ['?']\n",
    "\n",
    "    df_truth = pd.read_csv(truth_csv_path)[['post_id', 'help_truth']]\n",
    "    df_labeled = df_clean.merge(df_truth, on='post_id', how='left')\n",
    "\n",
    "    def detect_keywords(text):\n",
    "        matches = [kw for kw in keywords if kw in str(text).lower()]\n",
    "        return ' | '.join(matches), int(bool(matches))\n",
    "\n",
    "    df_labeled['keyword_match'], df_labeled['help_flag'] = zip(*df_labeled['post_text'].map(detect_keywords))\n",
    "\n",
    "    correct_matches = (df_labeled['help_flag'] == df_labeled['help_truth']).sum()\n",
    "    accuracy = correct_matches / len(df_labeled)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# List of keywords\n",
    "question_starters = [\n",
    "    \"what\", \"how\", \"is\", \"any\", \"anyone\", \"does\", \"i\", \"has\",\n",
    "    \"can\", \"do\", \"have\", \"if\", \"are\", \"which\", \"did\", \"or\",\n",
    "    \"for\", \"where\", \"will\", \"should\", \"also\", \"would\", \"anybody\",\n",
    "    \"who\", \"question\", \"am\", \"why\", \"when\", \"was\"\n",
    "]\n",
    "\n",
    "# Run test for each keyword\n",
    "def test_question_starters(df_clean, truth_csv_path='data/manual_help_truth.csv'):\n",
    "    results = {\n",
    "        word: run_keyword_match_test(df_clean, truth_csv_path, keywords=[word])\n",
    "        for word in question_starters\n",
    "    }\n",
    "\n",
    "    sorted_results = dict(sorted(results.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    for word, score in sorted_results.items():\n",
    "        print(f\"{word}: {score:.3f}\")\n",
    "\n",
    "    return sorted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87489648-fdb5-450f-88b0-4690294a9ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug cell\n",
    "\n",
    "# Confirm functions exist\n",
    "print(\"run_keyword_match_test defined:\", callable(run_keyword_match_test))\n",
    "print(\"test_question_starters defined:\", callable(test_question_starters))\n",
    "\n",
    "# Confirm df_clean has expected structure\n",
    "print(\"df_clean shape:\", df_clean.shape)\n",
    "print(\"df_clean columns:\", df_clean.columns.tolist())\n",
    "\n",
    "# Peek at data\n",
    "display(df_clean.head(1))\n",
    "\n",
    "# Run and show results\n",
    "results = test_question_starters(df_clean)\n",
    "print(\"\\nTop results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2db2a6a-b28b-401b-960f-159338929441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fcde73-a3d1-4cd2-a554-a7c27c20b879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f444ae3-e526-42eb-8307-81eb6e2a4865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af080480-fdba-4c25-8d26-82ddc702f287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cd2747-2463-46a9-bf7d-3c744c6e3186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ca7c9-81f1-439d-b93c-f77cfc780c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f952e5f-6d30-4d10-bff3-35a1a7cde4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ca6f1-a81a-4111-8f42-281d6158b1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac14c59-0f40-4380-b462-13c149e9bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename: step4_top_bigrams_per_starter.py\n",
    "\n",
    "starter_bigrams_map = {}\n",
    "\n",
    "for starter in question_starters:\n",
    "    bigram_counts = Counter()\n",
    "\n",
    "    for text in df_q['post_text'].dropna():\n",
    "        for s in sent_tokenize(text):\n",
    "            s = s.strip().lower()\n",
    "            if s.endswith('?'):\n",
    "                words = word_tokenize(s)\n",
    "                for i in range(len(words) - 1):\n",
    "                    if words[i] == starter:\n",
    "                        bigram = (words[i], words[i + 1])\n",
    "                        bigram_counts[bigram] += 1\n",
    "\n",
    "    # Store top bigrams for this starter\n",
    "    top = bigram_counts.most_common(10)\n",
    "    starter_bigrams_map[starter] = [f\"{w1} {w2}:{count}\" for (w1, w2), count in top]\n",
    "\n",
    "# Prepare display\n",
    "rows = []\n",
    "max_len = max(len(v) for v in starter_bigrams_map.values())\n",
    "for starter in question_starters:\n",
    "    row = [starter] + starter_bigrams_map.get(starter, [])\n",
    "    row += [''] * (max_len - len(row) + 1)\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows, columns=['Starter'] + [f\"Bigram {i+1}\" for i in range(1, max_len + 1)])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a9187-91fa-4df6-8941-8599c9f17fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename: step4_count_total_bigrams.py\n",
    "\n",
    "total_bigrams = 0\n",
    "\n",
    "for starter in question_starters:\n",
    "    bigram_counts = Counter()\n",
    "\n",
    "    for text in df_q['post_text'].dropna():\n",
    "        for s in sent_tokenize(text):\n",
    "            s = s.strip().lower()\n",
    "            if s.endswith('?'):\n",
    "                words = word_tokenize(s)\n",
    "                for i in range(len(words) - 1):\n",
    "                    if words[i] == starter:\n",
    "                        bigram_counts[(words[i], words[i + 1])] += 1\n",
    "\n",
    "    total_bigrams += sum(bigram_counts.values())\n",
    "\n",
    "print(f\"Total bigram instances (from question starters): {total_bigrams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f12d6b-82d1-4449-8814-cb467b737164",
   "metadata": {},
   "outputs": [],
   "source": [
    "## What and How, top 2 question starters:  bigram, vs, starter next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b76018-1fe7-40eb-96f8-c8f4cea34d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename: step4_compare_how_vs_what.py\n",
    "\n",
    "# Process and display separately for \"how\" and \"what\"\n",
    "for starter in [\"how\", \"what\"]:\n",
    "    # Collect bigrams and next words for current starter\n",
    "    next_words = []\n",
    "    bigram_counts = Counter()\n",
    "\n",
    "    for text in df_q['post_text'].dropna():\n",
    "        for s in sent_tokenize(text):\n",
    "            if s.endswith('?'):\n",
    "                words = word_tokenize(s.lower())\n",
    "                for i in range(len(words) - 1):\n",
    "                    if words[i] == starter:\n",
    "                        next_words.append(words[i+1])\n",
    "                        bigram_counts[(words[i], words[i+1])] += 1\n",
    "\n",
    "    # Format bigrams\n",
    "    top_bigrams = bigram_counts.most_common(30)\n",
    "    formatted_bigrams = [f\"{w1} {w2}:{count}\" for (w1, w2), count in top_bigrams]\n",
    "\n",
    "    # Format top next words\n",
    "    top_next = Counter(next_words).most_common(10)\n",
    "    formatted_next = [[starter] + [f\"{w}:{c}\" for w, c in top_next]]\n",
    "    max_len = len(formatted_next[0])\n",
    "    df_next = pd.DataFrame(formatted_next, columns=[\"Starter\"] + [f\"Next {i+1}\" for i in range(max_len - 1)])\n",
    "\n",
    "    # Display\n",
    "    print(f\"\\n--- {starter.upper()} ---\\n\")\n",
    "    print(\"Top Bigrams:\")\n",
    "    display_list(formatted_bigrams)\n",
    "    print(\"\\nTop Next Words:\")\n",
    "    display(df_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e548e-a8ad-44c1-be91-09d8d5148717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conclusion starter_next = bigram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6241c71c-329c-47ef-893d-d9cf74a2b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare top 20 bigrams from each\n",
    "how_bigrams = set([bg for (bg, _) in Counter([(w1, w2) for (w1, w2), _ in counts.items() if w1 == \"how\"]).most_common(20)])\n",
    "what_bigrams = set([bg for (bg, _) in Counter([(w1, w2) for (w1, w2), _ in counts.items() if w1 == \"what\"]).most_common(20)])\n",
    "overlap = how_bigrams & what_bigrams\n",
    "print(\"Overlapping bigrams:\", overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eafcebe-5ad1-4d84-ad92-72fb03be23a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe0b56b-c523-457a-b95a-dfdf79e1b6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079a7e6c-a345-4088-8c74-0ef1e3672fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84adefb9-680d-4c7f-ad3e-17bf2df69e6b",
   "metadata": {},
   "source": [
    "identify question-starting words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9cd6d5-dc36-484d-b12d-e8155b0849ef",
   "metadata": {},
   "source": [
    "**observation** wh- questions emerge, many are logical, but we want the data to tell the story. For each of these, show the next common word, and if the two start a question sentence we know it's not junk. We'll see if it matches our intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b4512-bb67-43a2-bfbd-8deffa48411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "question_starters = [\n",
    "    \"what\", \"how\", \"is\", \"any\",\n",
    "    \"anyone\", \"does\", \"i\", \"has\",\n",
    "    \"can\", \"do\", \"have\", \"if\",\n",
    "    \"are\", \"which\", \"did\", \"or\",\n",
    "    \"for\", \"where\", \"will\", \"should\",\n",
    "    \"also\", \"would\", \"anybody\", \"who\",\n",
    "    \"question\", \"am\", \"why\", \"when\",\n",
    "    \"was\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fa515f-65bf-455b-ab77-9a9fc4394ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_words = []\n",
    "\n",
    "for text in df_q['post_text'].dropna():\n",
    "    for s in sent_tokenize(text):\n",
    "        s = s.strip()\n",
    "        if s.endswith('?'):\n",
    "            words = word_tokenize(s)\n",
    "            if words:\n",
    "                word = words[0].lower()\n",
    "                if word not in remove_from_list:\n",
    "                    first_words.append(word)\n",
    "\n",
    "# Frequency count\n",
    "counts = Counter(first_words)\n",
    "top_items = counts.most_common(100)\n",
    "\n",
    "# Convert to multi-column format (e.g., 4 columns)\n",
    "cols = 4\n",
    "rows = (len(top_items) + cols - 1) // cols\n",
    "columns = [[] for _ in range(cols)]\n",
    "\n",
    "for i, (word, count) in enumerate(top_items):\n",
    "    col = i % cols\n",
    "    columns[col].append(f\"{word}:{count}\")\n",
    "\n",
    "# Pad shorter columns\n",
    "max_len = max(len(col) for col in columns)\n",
    "for col in columns:\n",
    "    col.extend([\"\"] * (max_len - len(col)))\n",
    "\n",
    "# Create and display DataFrame\n",
    "df = pd.DataFrame({f'Col {i+1}': columns[i] for i in range(cols)})\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08577254-586f-49b4-8767-3c2aff849bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784ccb41-09ad-4e4e-ad55-ed2739fef0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b13b6d-6a0a-4c78-b209-4d9b537c6643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect bigrams from sentences ending with '?'\n",
    "from collections import defaultdict\n",
    "\n",
    "followers = defaultdict(list)\n",
    "\n",
    "for text in df_q['post_text'].dropna():\n",
    "    for s in sent_tokenize(text):\n",
    "        s = s.strip()\n",
    "        if s.endswith('?'):\n",
    "            tokens = word_tokenize(s)\n",
    "            if len(tokens) >= 2:\n",
    "                first = tokens[0].lower()\n",
    "                second = tokens[1].lower()\n",
    "                followers[first].append(second)\n",
    "\n",
    "# Count and display top 3 followers for top 40 first words\n",
    "print(\"First word : count | top 3 followers\\n\")\n",
    "\n",
    "for word, count in counts.most_common(40):\n",
    "    next_words = Counter(followers[word])\n",
    "    top_next = \", \".join(w for w, _ in next_words.most_common(3))\n",
    "    print(f\"{word:>10}: {count:<4} | {top_next}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98135213-dcbd-440b-ac80-98e847fa7489",
   "metadata": {},
   "source": [
    "Rules start to emerge, if we can write rules for how people ask questions we can find questions without ?, increase our baseling detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeaac9c-170e-41f6-950f-9f0ee030be5b",
   "metadata": {},
   "source": [
    "question_starters = wh words + helping verbs, + if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9cd56c-098c-4d43-a5c9-bde32084b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if = conjunction, not coordinating conjunction, but suboordinate conjunction.  http://partofspeech.org/conjunction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c455225-be1a-4fe4-ab95-57ccaf64f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subordinate conjunction = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec1857c-0ea4-470c-a00e-f677c50bddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect question-start bigrams\n",
    "start_bigrams = []\n",
    "\n",
    "for text in df_q_short['post_text'].dropna():\n",
    "    sentences = sent_tokenize(text)\n",
    "    for s in sentences:\n",
    "        s = s.strip()\n",
    "        if s.endswith('?'):\n",
    "            tokens = word_tokenize(s)\n",
    "            if len(tokens) >= 2:\n",
    "                start_bigrams.append((tokens[0].lower(), tokens[1].lower()))\n",
    "\n",
    "# Frequency count\n",
    "bigram_counts = Counter(start_bigrams)\n",
    "\n",
    "# Display top 20\n",
    "print(\"Top bigrams starting question-ending sentences:\\n\")\n",
    "for pair, count in bigram_counts.most_common(20):\n",
    "    print(f\"{' '.join(pair):>15}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00161b9-a170-4ded-b3cf-de55e3f49915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e594803-2def-4615-a669-d1b6fa473819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5748b234-7bf1-439c-8beb-87777a984d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5a4cf8-1524-4b67-bbcb-6b2a54ee6e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6057cce5-b9a4-47a4-9fdb-240078623552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee772b17-891d-4bae-b273-b33896999533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (WGU)",
   "language": "python",
   "name": "wgu-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
