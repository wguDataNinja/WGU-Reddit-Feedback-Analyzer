# filename: scripts/batch_generate_pdfs.py

import csv, json, argparse, logging
from pathlib import Path
from datetime import datetime, timezone
from markdown import markdown
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

from utils.logger import setup_logger, get_timestamp_str

MIN_PAIN_POINTS = 0


def load_course_info(course_csv: Path) -> dict:
    with course_csv.open(newline='', encoding='utf-8') as f:
        return {
            row['CourseCode']: {
                'title': row['Title'],
                'college': row['Colleges']
            } for row in csv.DictReader(f)
        }


def load_pain_points(jsonl_path: Path) -> dict:
    pain_points = {}
    with jsonl_path.open(encoding='utf-8') as f:
        for line in f:
            entry = json.loads(line)
            pid = entry['pain_point_id']
            post_id = entry.get('post_id')
            pain_points[pid] = {
                'quote': entry.get('quoted_text', '').strip(),
                'url': f"https://reddit.com/comments/{post_id}" if post_id else ''
            }
    return pain_points


def generate_markdown(course_code: str, course_info: dict, clusters_file: Path, pain_points: dict, min_pain_points: int) -> str:
    info = course_info[course_code]
    data = json.loads(clusters_file.read_text(encoding='utf-8'))
    filtered = [c for c in data.get('clusters', []) if len(c.get('pain_point_ids', [])) >= min_pain_points]
    filtered.sort(key=lambda c: len(c.get('pain_point_ids', [])), reverse=True)

    timestamp = datetime.now(timezone.utc).strftime("%B %d, %Y at %I:%M %p UTC")
    lines = [
        "# WGU-Reddit Student Feedback",
        "Generated by the WGU-Reddit Feedback Analyzer...",
        f"**Generated:** {timestamp}  ",
        f"# {course_code}: {info['title']}",
        f"**College:** {info['college']}  ",
        f"**Topics:** {len(filtered)}  ",
        f"**Pain Points:** {sum(len(c['pain_point_ids']) for c in filtered)}  ",
        "---"
    ]

    if not filtered:
        lines.append("No feedback topics meet the selected threshold.")
    else:
        lines.append("## Feedback Topics")
        for idx, cl in enumerate(filtered, 1):
            lines.append(f"### {idx}. {cl.get('title', 'Untitled')} – [{len(cl.get('pain_point_ids', []))} posts]")
            if summary := cl.get('root_cause_summary', '').strip():
                lines.append(f"{summary}  ")
            for pid in cl.get('pain_point_ids', []):
                pp = pain_points.get(pid)
                if pp:
                    lines.append(f'> "{pp["quote"]}"  ')
                    if pp['url']:
                        lines.append(f"> [Reddit post]({pp['url']})  ")
            lines.append("---")

    return "\n".join(lines)


def wrap_feedback_topics(html_body: str) -> str:
    soup = BeautifulSoup(html_body, "html.parser")
    new_body = soup.new_tag("div")
    children = list(soup.children)
    i = 0
    while i < len(children):
        node = children[i]
        if node.name == "h3":
            wrapper = soup.new_tag("div", **{"class": "topic-block"})
            wrapper.append(node)
            i += 1
            while i < len(children) and children[i].name != "h3":
                wrapper.append(children[i])
                i += 1
            new_body.append(wrapper)
        else:
            new_body.append(node)
            i += 1
    return str(new_body)


def generate_pdf(md_content: str, pdf_path: Path):
    html_body = markdown(md_content, extensions=['extra'])
    html_body_wrapped = wrap_feedback_topics(html_body)
    date_str = datetime.now().strftime("%B %d, %Y")

    html = f"""<!DOCTYPE html>
<html><head><meta charset="utf-8"><style>
body {{ font-family: sans-serif; margin: 0; padding: 0.5in; }}
h1, h2, h3 {{ page-break-inside: avoid; margin-top: 1em; }}
blockquote {{ margin: 1em; padding-left: 1em; border-left: 3px solid #ccc; }}
.topic-block {{ page-break-inside: avoid; margin-bottom: 1.5em; }}
a {{ color: blue; text-decoration: none; }}
footer {{ position: fixed; bottom: 0.33in; left: 0; right: 0; font-size: 11px; color: #888; text-align: center; }}
</style></head><body>
{html_body_wrapped}
<footer>Generated {date_str} | Report v1.1 | AI-generated — may contain inaccuracies</footer>
</body></html>"""

    with sync_playwright() as p:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.set_content(html, wait_until="load")
        page.pdf(
            path=str(pdf_path),
            format="A4",
            margin={"top": "0.33in", "bottom": "0in", "left": "0.33in", "right": "0.33in"},
            print_background=True
        )
        browser.close()


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--output_dir", required=True, help="Base output dir: outputs/runs/YYYY-MM-DD/")
    parser.add_argument("--course_csv", required=True, help="Path to CSV with CourseCode, Title, Colleges")
    args = parser.parse_args()

    base = Path(args.output_dir)
    logs_dir = base / "logs"
    logs_dir.mkdir(parents=True, exist_ok=True)

    pdf_logger = setup_logger("pdf_generation", logs_dir / "pdf_generation.log")
    pipeline_logger = logging.getLogger("pipeline")

    clusters_dir = base / "stage2_output"
    pain_points_path = base / "stage1" / "pain_points_stage1.jsonl"
    pdf_output_dir = base / "pdfs"
    pdf_output_dir.mkdir(parents=True, exist_ok=True)

    course_info = load_course_info(Path(args.course_csv))
    pain_points = load_pain_points(pain_points_path)

    total, success, failed = 0, 0, 0

    for file in clusters_dir.glob("*_clusters.json"):
        total += 1
        course_code = file.stem.replace("_clusters", "")
        try:
            if course_code not in course_info:
                raise ValueError(f"Course {course_code} not found in CSV.")
            md = generate_markdown(course_code, course_info, file, pain_points, MIN_PAIN_POINTS)
            pdf_path = pdf_output_dir / f"{course_code}_Reddit_Feedback.pdf"
            generate_pdf(md, pdf_path)

            success += 1
            pdf_logger.info(json.dumps({
                "event": "pdf_course_success",
                "course": course_code,
                "pdf": pdf_path.name,
                "timestamp": get_timestamp_str()
            }))
        except Exception as e:
            failed += 1
            pdf_logger.error(json.dumps({
                "event": "pdf_course_failed",
                "course": course_code,
                "error": f"{type(e).__name__}: {e}",
                "timestamp": get_timestamp_str()
            }))

    pdf_logger.info(json.dumps({
        "event": "pdf_generation_complete",
        "total": total,
        "success": success,
        "failed": failed,
        "timestamp": get_timestamp_str()
    }))

    # Concise summary to pipeline.log
    pipeline_logger.info(
        f"PDF generation complete: {success}/{total} succeeded, {failed} failed (details in pdf_generation.log)"
    )


if __name__ == "__main__":
    main()