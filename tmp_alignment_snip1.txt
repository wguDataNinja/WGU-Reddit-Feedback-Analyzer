== model_client.py ==
# src/wgu_reddit_analyzer/benchmark/model_client.py
"""
Model client wrapper for Stage 1 benchmarking.

Responsibility:
    - Hide provider-specific details (OpenAI vs Ollama).
    - Use the model registry for provider and pricing metadata.
    - Call the underlying LLM (Chat Completions for OpenAI, HTTP for Ollama).
    - Run cost/latency estimation via cost_latency.estimate_cost.
    - Return a structured LlmCallResult object for downstream use.

This is the single entry point Stage 1 code should use:
    generate(model_name: str, prompt: str) -> LlmCallResult

Features:
    - Per-call timeout.
    - Simple retry with exponential backoff.
    - llm_failure / num_retries / error_message flags on LlmCallResult.

Decoding:
    - Stage-1 benchmarks are expected to use deterministic decoding
      (e.g., temperature=0, top_p=1, single candidate) at the provider layer.
"""

from __future__ import annotations

import time
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError
from typing import Any, Tuple

from wgu_reddit_analyzer.utils.config_loader import get_config
from wgu_reddit_analyzer.benchmark.model_registry import get_model_info
from wgu_reddit_analyzer.benchmark.cost_latency import estimate_cost
from wgu_reddit_analyzer.benchmark.stage1_types import LlmCallResult
from wgu_reddit_analyzer.benchmark.llm_connectivity_check import (
    _call_openai_responses,
    _call_ollama,
)
from wgu_reddit_analyzer.utils.logging_utils import get_logger

logger = get_logger("benchmark.model_client")

DEFAULT_TIMEOUT_SEC = 90.0
MAX_RETRIES = 2


def _call_model_once(
    model_name: str,
    prompt: str,
    provider: str,
    cfg: Any,
) -> str:
    """
    Single underlying model call.

    Provider-specific helpers are responsible for configuring decoding
    (e.g., deterministic settings for benchmarking).
    """
    if provider == "openai":
        return _call_openai_responses(model_name, prompt, cfg.openai_api_key or "")
    if provider == "ollama":
        return _call_ollama(model_name, prompt)
    raise RuntimeError(f"Unsupported provider for model '{model_name}': {provider}")


def _call_model_with_retry(
    model_name: str,
    prompt: str,
    provider: str,
    cfg: Any,
    timeout_sec: float = DEFAULT_TIMEOUT_SEC,
    max_retries: int = MAX_RETRIES,
) -> Tuple[str | None, bool, int, str | None]:
    """
    Run the underlying model call with a per-attempt timeout and simple retries.

    Returns:
        raw_text (str | None)
        llm_failure (bool)
        num_retries (int)  # how many retries were actually attempted
        error_message (str | None)
    """
    last_error: str | None = None

    for attempt in range(max_retries + 1):
        try:
            if attempt > 0:
                backoff = 2**attempt
                logger.warning(
                    "Retrying model call model=%s provider=%s attempt=%d backoff=%ds",
                    model_name,
                    provider,
                    attempt,
                    backoff,
                )
                time.sleep(backoff)

            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(
                    _call_model_once,
                    model_name,
                    prompt,
                    provider,
                    cfg,
                )
                raw_text: str = future.result(timeout=timeout_sec)

            if attempt > 0:
                logger.info(
                    "Model call succeeded after %d retries model=%s provider=%s",
                    attempt,
                    model_name,
                    provider,
                )
            return raw_text, False, attempt, None

        except FuturesTimeoutError:
            last_error = f"Timeout after {timeout_sec}s"
            logger.error(
                "Model call timeout model=%s provider=%s attempt=%d timeout_sec=%.1f",
                model_name,
                provider,
                attempt,
                timeout_sec,
            )
        except Exception as e:
            last_error = f"{type(e).__name__}: {e}"
            logger.error(
                "Model call failed model=%s provider=%s attempt=%d error=%s",
                model_name,
                provider,
                attempt,
                e,
            )

    logger.error(
        "Model call giving up after %d retries model=%s provider=%s last_error=%s",
        max_retries,
        model_name,
        provider,
        last_error,
    )
    return None, True, max_retries, last_error


def generate(model_name: str, prompt: str) -> LlmCallResult:
    """
    Provider-agnostic LLM invocation.

    Parameters
    ----------
    model_name : str
        Registry key for the model.
    prompt : str
        Fully rendered prompt text.

    Returns
    -------
    LlmCallResult
        Structured result including raw text, cost, latency, and failure flags.

== logging_utils.py ==
from __future__ import annotations
import logging
from pathlib import Path

REPO_ROOT = Path(__file__).resolve().parents[3]
LOG_DIR = REPO_ROOT / "logs"
LOG_DIR.mkdir(parents=True, exist_ok=True)


def get_logger(name: str) -> logging.Logger:
    """
    Canonical logger:
    - Writes to logs/<name>.log
    - Also logs to stderr
    - Idempotent (no duplicate handlers)
    """
    logger = logging.getLogger(name)
    if logger.handlers:
        return logger

    logger.setLevel(logging.INFO)

    fmt = logging.Formatter(
        "%(asctime)s [%(levelname)s] %(name)s: %(message)s"
    )

    fh = logging.FileHandler(LOG_DIR / f"{name}.log", encoding="utf-8")
    fh.setFormatter(fmt)
    logger.addHandler(fh)

    sh = logging.StreamHandler()
    sh.setFormatter(fmt)
    logger.addHandler(sh)

    logger.propagate = False
    return logger
== gold_labels.csv (first 6 lines) ==
post_id,split,course_code,contains_painpoint,root_cause_summary,ambiguity_flag,labeler_id,notes
1c0qqf5,DEV,AFT2,y,AFT2 Task1 requirements unclear,0,001,
y8ajxy,DEV,C100,n,,0,001,
1lhtc75,DEV,C165,n,,0,001,
1jzu8xc,DEV,C168,n,,0,001,
10xra7v,DEV,C182,n,,0,001,

== stage1 manifest: gpt-5-mini_s1_few_fulldev_20251121_045651 ==
{
  "model_name": "gpt-5-mini",
  "provider": "openai",
  "run_slug": "gpt-5-mini_s1_few_fulldev",
  "prompt_template_path": "prompts/s1_few.txt",
  "prompt_filename": "s1_few.txt",
  "prompt_copied_path": "artifacts/benchmark/stage1/runs/gpt-5-mini_s1_few_fulldev_20251121_045651/s1_few.txt",
  "split": "DEV",
  "gold_path": "artifacts/benchmark/gold/gold_labels.csv",
  "candidates_path": "artifacts/benchmark/DEV_candidates.jsonl",
  "num_examples": 135,
  "metrics_path": "artifacts/benchmark/stage1/runs/gpt-5-mini_s1_few_fulldev_20251121_045651/metrics_DEV.json",
  "predictions_path": "artifacts/benchmark/stage1/runs/gpt-5-mini_s1_few_fulldev_20251121_045651/predictions_DEV.csv",
  "raw_io_path": "artifacts/benchmark/stage1/runs/gpt-5-mini_s1_few_fulldev_20251121_045651/raw_io_DEV.jsonl",
  "run_dir": "artifacts/benchmark/stage1/runs/gpt-5-mini_s1_few_fulldev_20251121_045651",
  "started_at_epoch": 1763719011.3685,
  "finished_at_epoch": 1763720118.658719
}
== stage3 cluster_global_index.csv (gpt-5-mini_s3_global_gpt-5-mini_s2_cluster_4courses_20251126_192835, first 6 lines) ==
cluster_id,global_cluster_id,course_code,course_title,num_posts,provisional_label,normalized_issue_label
AFT2_1,G002,AFT2,Accreditation Audit,1,Ambiguous task instructions,unclear_or_ambiguous_instructions
C165_1,G001,C165,Integrated Physical Sciences,1,Assessment & practice misalignment,assessment_material_misalignment
C165_2,G005,C165,Integrated Physical Sciences,1,Low-quality or unclear course materials,missing_or_low_quality_materials
C180_1,G001,C180,Introduction to Psychology,1,Assessment & practice misalignment,assessment_material_misalignment
C190_1,G003,C190,Introduction to Biology,1,Instructor/support unresponsiveness,instructor_or_support_unresponsiveness

== stage3 manifest (gpt-5-mini_s3_global_gpt-5-mini_s2_cluster_4courses_20251126_192835) ==
{
  "stage3_run_dir": "artifacts/stage3/runs/gpt-5-mini_s3_global_gpt-5-mini_s2_cluster_4courses_20251126_192835",
  "stage3_run_slug": "gpt-5-mini_s3_global_gpt-5-mini_s2_cluster_4courses",
  "stage2_run_dir": "artifacts/stage2/runs/gpt-5-mini_s2_cluster_4courses_20251126_192631",
  "stage2_run_slug": "gpt-5-mini_s2_cluster_4courses",
  "stage2_manifest_path": "artifacts/stage2/runs/gpt-5-mini_s2_cluster_4courses_20251126_192631/manifest.json",
  "clusters_csv_path": "artifacts/stage3/preprocessed/gpt-5-mini_s2_cluster_4courses/clusters_llm.csv",
  "global_model_name": "gpt-5-mini",
  "global_prompt_path": "prompts/s3_normalize_clusters.txt",
  "num_input_clusters": 7,
  "num_input_courses": 4,
  "total_input_posts": 7,
  "num_batches": 1,
  "num_global_clusters": 6,
  "num_unassigned_clusters": 0,
  "total_assigned_posts": 7,
  "total_unassigned_posts": 0,
  "cluster_coverage_fraction": 1.0,
  "post_coverage_fraction": 1.0,
  "started_at_epoch": 1764203315.376203,
  "finished_at_epoch": 1764203338.200313,
  "wallclock_sec": 22.82411003112793,
  "total_cost_usd": 0.001234,
  "total_elapsed_sec_model_calls": 22.822
}